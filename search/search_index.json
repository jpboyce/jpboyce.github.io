{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:ansible","title":"ansible","text":"<ul> <li>            Installing Ansible AWX on CentOS 8          </li> </ul>"},{"location":"tags/#tag:api-management","title":"api management","text":"<ul> <li>            Managing Azure API Management with APIOPS          </li> </ul>"},{"location":"tags/#tag:automation","title":"automation","text":"<ul> <li>            Installing Ansible AWX on CentOS 8          </li> <li>            Thoughts on User Experience and Automation          </li> </ul>"},{"location":"tags/#tag:awx","title":"awx","text":"<ul> <li>            Installing Ansible AWX on CentOS 8          </li> </ul>"},{"location":"tags/#tag:azure-bicep","title":"azure bicep","text":"<ul> <li>            Developing a Bicep Validation Pipeline          </li> </ul>"},{"location":"tags/#tag:azure-devops","title":"azure devops","text":"<ul> <li>            Azure Defender for DevOps \u2013 First Impressions          </li> <li>            Developing a Bicep Validation Pipeline          </li> <li>            PowerShell Quality of Life Improvements \u2013 Automatic Versioning          </li> <li>            PowerShell Quality of Life Improvements \u2013 Code Signing          </li> <li>            PowerShell Quality of Life Improvements \u2013 Code Testing          </li> <li>            PowerShell Quality of Life Improvements \u2013 PS Repository          </li> <li>            PowerShell Quality of Life Improvements \u2013 Release          </li> <li>            Registering a VM with Multiple Azure DevOps Environments          </li> <li>            \u201cCould not create SSL/TLS secure channel\u201d error when using self-hosted Azure DevOps Agent          </li> </ul>"},{"location":"tags/#tag:azure-front-door","title":"azure front door","text":"<ul> <li>            Rendering Issues with Nodejs/NextJS and Azure Front Door          </li> </ul>"},{"location":"tags/#tag:bicep","title":"bicep","text":"<ul> <li>            Developing a Bicep Validation Pipeline          </li> </ul>"},{"location":"tags/#tag:blizzcon","title":"blizzcon","text":"<ul> <li>            Blizzard\u2019s IT Architecture and Testing at Blizzcon 2017          </li> </ul>"},{"location":"tags/#tag:conference","title":"conference","text":"<ul> <li>            CeBIT Australia 2015 Experience          </li> </ul>"},{"location":"tags/#tag:cve-2021-21972","title":"cve-2021-21972","text":"<ul> <li>            Remediating VMSA-2021-0002 \u2013 Potential Issues          </li> </ul>"},{"location":"tags/#tag:cve-2021-21973","title":"cve-2021-21973","text":"<ul> <li>            Remediating VMSA-2021-0002 \u2013 Potential Issues          </li> </ul>"},{"location":"tags/#tag:cve-2021-21974","title":"cve-2021-21974","text":"<ul> <li>            Remediating VMSA-2021-0002 \u2013 Potential Issues          </li> </ul>"},{"location":"tags/#tag:devops","title":"devops","text":"<ul> <li>            DORA 2024 State of DevOps Report          </li> </ul>"},{"location":"tags/#tag:elasticsearch","title":"elasticsearch","text":"<ul> <li>            Installing ElasticStack Beats on vCenter 6.7          </li> </ul>"},{"location":"tags/#tag:emc","title":"emc","text":"<ul> <li>            Want hands on with Virtual Volumes? EMC has a Virtual Appliance for that          </li> </ul>"},{"location":"tags/#tag:games","title":"games","text":"<ul> <li>            Blizzard\u2019s IT Architecture and Testing at Blizzcon 2017          </li> </ul>"},{"location":"tags/#tag:home-lab","title":"home lab","text":"<ul> <li>            Home lab expansion          </li> <li>            Updating to vRealize Automation 7.6 Hotfix 7          </li> </ul>"},{"location":"tags/#tag:horizon","title":"horizon","text":"<ul> <li>            VMware Horizon 7 \u2013 First Impressions          </li> </ul>"},{"location":"tags/#tag:imperva","title":"imperva","text":"<ul> <li>            Clearing Imperva Cloud WAF Cache using Powershell          </li> </ul>"},{"location":"tags/#tag:ipam","title":"ipam","text":"<ul> <li>            vRA 8 and phpIPAM          </li> </ul>"},{"location":"tags/#tag:it","title":"it","text":"<ul> <li>            Project Honolulu \u2013 First impressions          </li> </ul>"},{"location":"tags/#tag:it-industry","title":"it industry","text":"<ul> <li>            DORA 2024 State of DevOps Report          </li> </ul>"},{"location":"tags/#tag:lifecycle-manager","title":"lifecycle manager","text":"<ul> <li>            VMware vRealize Suite Lifecycle Manager 1.2 \u2013 First Impressions          </li> </ul>"},{"location":"tags/#tag:nexpose","title":"nexpose","text":"<ul> <li>            Adding extra service mappings to Nexpose          </li> </ul>"},{"location":"tags/#tag:nsx","title":"nsx","text":"<ul> <li>            VMware NSX \u2013 First Impressions          </li> </ul>"},{"location":"tags/#tag:nutanix","title":"nutanix","text":"<ul> <li>            Building Nutanix AHV Templates with Packer          </li> </ul>"},{"location":"tags/#tag:open-vm-tools","title":"open-vm-tools","text":"<ul> <li>            vRA 8 \u2013 GetDiskInfo: ERROR: Partition name buffer too small          </li> </ul>"},{"location":"tags/#tag:packer","title":"packer","text":"<ul> <li>            An Introduction to Packer          </li> <li>            Building Nutanix AHV Templates with Packer          </li> </ul>"},{"location":"tags/#tag:phpipam","title":"phpipam","text":"<ul> <li>            vRA 8 and phpIPAM          </li> </ul>"},{"location":"tags/#tag:powershell","title":"powershell","text":"<ul> <li>            Automating Vulnerability Scans \u2013 Nexpose/Powershell          </li> <li>            Bulk Add Flavor Mappings Using vRA 8 REST API          </li> <li>            Clearing Imperva Cloud WAF Cache using Powershell          </li> <li>            PowerShell 7.0 \u2013 Cmdlet Compatibility          </li> <li>            PowerShell 7.0 \u2013 Introduction, History, Installation          </li> <li>            PowerShell Quality of Life Improvements \u2013 Automatic Versioning          </li> <li>            PowerShell Quality of Life Improvements \u2013 Code Signing          </li> <li>            PowerShell Quality of Life Improvements \u2013 Code Testing          </li> <li>            PowerShell Quality of Life Improvements \u2013 PS Repository          </li> <li>            PowerShell Quality of Life Improvements \u2013 Release          </li> <li>            SQL As A Service Proof of Concept with SQL 2012 and vRealize Automation          </li> <li>            Security Agencies Issue PowerShell Guidance          </li> <li>            vRA/vRO 8.1 Powershell \u2013 Peaking Under The Hood          </li> <li>            vRealize Orchestrator \u2013 PowerShell Hosts          </li> </ul>"},{"location":"tags/#tag:powershell-security","title":"powershell security","text":"<ul> <li>            Security Agencies Issue PowerShell Guidance          </li> </ul>"},{"location":"tags/#tag:pricing-card","title":"pricing card","text":"<ul> <li>            vRealize Automation 8 Pricing Cards          </li> </ul>"},{"location":"tags/#tag:psrule","title":"psrule","text":"<ul> <li>            Developing a Bicep Validation Pipeline          </li> </ul>"},{"location":"tags/#tag:rest-api","title":"rest api","text":"<ul> <li>            Bulk Add Flavor Mappings Using vRA 8 REST API          </li> </ul>"},{"location":"tags/#tag:security","title":"security","text":"<ul> <li>            Azure Defender for DevOps \u2013 First Impressions          </li> <li>            Remediating VMSA-2021-0002 \u2013 Potential Issues          </li> <li>            VMSA-2021-0020 \u2013 19 vulnerabilities on vCenter          </li> </ul>"},{"location":"tags/#tag:sql-server","title":"sql server","text":"<ul> <li>            SQL As A Service Proof of Concept with SQL 2012 and vRealize Automation          </li> </ul>"},{"location":"tags/#tag:storage","title":"storage","text":"<ul> <li>            Want hands on with Virtual Volumes? EMC has a Virtual Appliance for that          </li> </ul>"},{"location":"tags/#tag:system-center-orchestrator","title":"system center orchestrator","text":"<ul> <li>            Calling System Center Orchestrator Runbooks from vRealize Orchestrator          </li> </ul>"},{"location":"tags/#tag:telstra","title":"telstra","text":"<ul> <li>            Results from Telstra\u2019s Free Data Day          </li> </ul>"},{"location":"tags/#tag:vcenter","title":"vcenter","text":"<ul> <li>            Installing ElasticStack Beats on vCenter 6.7          </li> <li>            Remediating VMSA-2021-0002 \u2013 Potential Issues          </li> <li>            VCSA 6.5 Root Account Password \u2013 Reset and Cause Investigation          </li> <li>            VMSA-2021-0020 \u2013 19 vulnerabilities on vCenter          </li> <li>            vCenter 6.7 Update 2 \u2013 Code Capture          </li> </ul>"},{"location":"tags/#tag:vdi","title":"vdi","text":"<ul> <li>            VMware Horizon 8/2012 \u2013 Install Walkthrough &amp; First Impressions          </li> </ul>"},{"location":"tags/#tag:vexpert","title":"vexpert","text":"<ul> <li>            I\u2019m a vExpert!          </li> <li>            vExpert 2022          </li> </ul>"},{"location":"tags/#tag:vmsa","title":"vmsa","text":"<ul> <li>            VMSA-2022-0007 \u2013 VMware Tools vulnerability          </li> </ul>"},{"location":"tags/#tag:vmsa-2021-0002","title":"vmsa-2021-0002","text":"<ul> <li>            Remediating VMSA-2021-0002 \u2013 Potential Issues          </li> </ul>"},{"location":"tags/#tag:vmsa-2021-0020","title":"vmsa-2021-0020","text":"<ul> <li>            VMSA-2021-0020 \u2013 19 vulnerabilities on vCenter          </li> </ul>"},{"location":"tags/#tag:vmware","title":"vmware","text":"<ul> <li>            I\u2019m a vExpert!          </li> <li>            vCloud Air Test Experience          </li> </ul>"},{"location":"tags/#tag:vmware-horizon","title":"vmware horizon","text":"<ul> <li>            VMware Horizon 8/2012 \u2013 Install Walkthrough &amp; First Impressions          </li> </ul>"},{"location":"tags/#tag:vmware-horizon-2021","title":"vmware horizon 2021","text":"<ul> <li>            VMware Horizon 8/2012 \u2013 Install Walkthrough &amp; First Impressions          </li> </ul>"},{"location":"tags/#tag:vmware-horizon-8","title":"vmware horizon 8","text":"<ul> <li>            VMware Horizon 8/2012 \u2013 Install Walkthrough &amp; First Impressions          </li> </ul>"},{"location":"tags/#tag:vmware-tools","title":"vmware tools","text":"<ul> <li>            VMSA-2022-0007 \u2013 VMware Tools vulnerability          </li> </ul>"},{"location":"tags/#tag:vmware-vcenter","title":"vmware vcenter","text":"<ul> <li>            Remediating VMSA-2021-0002 \u2013 Potential Issues          </li> </ul>"},{"location":"tags/#tag:vra8","title":"vra8","text":"<ul> <li>            vRealize Automation 8.2 Released          </li> <li>            vRealize Automation 8.4          </li> <li>            vRealize Automation 8.5          </li> <li>            vRealize Automation 8.6.2          </li> </ul>"},{"location":"tags/#tag:vrealize-automation","title":"vrealize automation","text":"<ul> <li>            Bulk Add Flavor Mappings Using vRA 8 REST API          </li> <li>            Cumulative Update for vRealize Automation 7.6          </li> <li>            Managing Local Admins via vRealize Automation          </li> <li>            SQL As A Service Proof of Concept with SQL 2012 and vRealize Automation          </li> <li>            Updating to vRealize Automation 7.6 Hotfix 7          </li> <li>            VMUG vRealize Suite 2019 and vRA 8          </li> <li>            VMware vRealize Automation 7.6 \u2013 What\u2019s New          </li> <li>            VMware vRealize Suite Lifecycle Manager 1.2 \u2013 First Impressions          </li> <li>            vRA 8 and phpIPAM          </li> <li>            vRA 8 \u2013 GetDiskInfo: ERROR: Partition name buffer too small          </li> <li>            vRA/vRO 8.1 Powershell \u2013 Peaking Under The Hood          </li> <li>            vRealize Automation 7.0 \u2013 First Impressions          </li> <li>            vRealize Automation 8 First Impressions \u2013 Cloud Assembly          </li> <li>            vRealize Automation 8 First Impressions \u2013 Getting Started          </li> <li>            vRealize Automation 8 First Impressions \u2013 Installation          </li> <li>            vRealize Automation 8 First Impressions \u2013 Lifecycle Manager          </li> <li>            vRealize Automation 8 First Impressions \u2013 Service Broker          </li> <li>            vRealize Automation 8 Pricing Cards          </li> <li>            vRealize Automation 8 \u2013 cloudConfig          </li> <li>            vRealize Automation 8.0.1 Update Walkthrough          </li> <li>            vRealize Automation 8.1 New Features Walkthrough          </li> <li>            vRealize Automation 8.2 Released          </li> <li>            vRealize Automation \u2013 Dealing with a disconnected/orphaned IAAS Server          </li> </ul>"},{"location":"tags/#tag:vrealize-automation-8","title":"vrealize automation 8","text":"<ul> <li>            vRA 8 and phpIPAM          </li> <li>            vRealize Automation 8 Pricing Cards          </li> <li>            vRealize Automation 8.2 Released          </li> <li>            vRealize Automation 8.4          </li> <li>            vRealize Automation 8.5          </li> <li>            vRealize Automation 8.6.2          </li> </ul>"},{"location":"tags/#tag:vrealize-orchestration","title":"vrealize orchestration","text":"<ul> <li>            Creating Service Accounts with vRealize Orchestrator          </li> <li>            Improving the vRA Admin Experience \u2013 Reservation Alerts to Slack          </li> <li>            Improving the vRA Customer Experience \u2013 Send Chef errors to Slack          </li> <li>            Managing F5 Load Balancers with vRealize Orchestrator          </li> <li>            vRA/vRO 8.1 Powershell \u2013 Peaking Under The Hood          </li> <li>            vRealize Orchestrator \u2013 PowerShell Hosts          </li> </ul>"},{"location":"tags/#tag:vrealize-orchestrator","title":"vrealize orchestrator","text":"<ul> <li>            Calling System Center Orchestrator Runbooks from vRealize Orchestrator          </li> <li>            Managing Local Admins via vRealize Automation          </li> <li>            vRA 8 and phpIPAM          </li> </ul>"},{"location":"2015/10/13/cebit-australia-2015-experience/","title":"CeBIT Australia 2015 Experience","text":"<p>This year was the first time I had attended CeBIT, with a primary motivator being the Cloud Conference component, with that component being of interest personally and professionally, as well as the fact that events of this size are rarely held in Perth.  The speakers for the Cloud Conference covered private enterprise and government, giving a broad view of how cloud was making IT work better.</p> <p>The first speaker was Chris C Kemp, former CTO at NASA and co-founder of OpenStack.  He spoke about the concepts of anti-scarcity, where a thing can be made more valuable by making it more freely available and accessible because more parties are involved and invested in the item.  This concept applies strongly with open source software and OpenStack in particular, which because as an internal project at NASA.  By allowing it to be available to all, OpenStack now has support and investment from large IT vendors such as HP, IBM and Cisco.</p> <p>David Boyle from NAB opened his presentation by stating he wouldn\u2019t use the \u201cc word\u201d (cloud).  He managed to stick with his promise, and talked about the concepts of traditional \u201chorse and cart\u201d IT, the model we\u2019ve used in the past of physical infrastructure, lengthy release cycles and waterfall development.  This was contrasted with \u201cFerrari\u201d IT, which uses virtualisation, frequent release models, continuous deployment, automation and dev ops.  A key concept he outlined was \u201cfail fast\u201d \u2013 having a deployment framework that can be run rapidly so success or failure can be determined quickly and subsequent deployments attempted once problems have been fixed.</p> <p>Dez Blanchfield outlined a somewhat subversive approach to cloud adoption, via the use of \u201cmicro clouds\u201d.  Dez has managed to produce a cloud that can fit on a USB stick, using a number of free and open source tools.  It can run on as little as 8GB of memory and allows people to reuse old hardware as a platform to dive into cloud computing without making the capex committment typically associated with getting a cloud up and running.</p> <p>Robin Phua from the State Library of NSW and Pedro Harris from NSW Department of Finance each detailed how cloud computing had changed IT service delivery in their respective areas.  The State Library was able to use cloud based storage to contain large amounts of digital data in a cost effective way.  Pedro detailed the GovDC marketplace that the NSW government had setup, allowing a shorter procurement cycle.</p> <p>Johnathan Sharratt from ING Direct presented about ING\u2019s \u201cBank in a box\u201d concept.  This was a project I\u2019d read about previously and was very interested in seeing more detail about how they actually utilised it.  From Johnathan\u2019s presentation, the technology had progressed from being used to deploy test or development environments to being used for production releases, where an entire copy of the bank\u2019s application stack (over 300 applications) would be deployed and then switched over to be the live production system.  The entire process is highly automated and includes garbage collection to prevent sprawl.  This allows the bank to run on a release cycle of 2 per week, which allows ING to rapidly deploy updates and new features to meet the needs of their business and their customers.</p> <p>With regard to the general exhibition area, a few booths stood out:</p> <ul> <li>Orion VM \u2013 Orion provides a whole sale model for cloud, allowing you to buy their product and rebrand it for your customers, including customised rate cards and billing.  The guys from Orion were very knowledgable and passionate about their product and from what I\u2019ve seen it offers some new and interesting things in the way IAAS is built and served.</li> <li>ControlNow \u2013 What got my attention about ControlNow is their product that presents a \u201csingle pane of glass\u201d that summarises security, updates and monitoring of your infrastructure, providing a overall view of the health of your systems.</li> <li>Helpmaster \u2013 Helpmaster\u2019s service desk software included a sophisticated workflow engine, which could allow workflows to be kicked off based on key words or phrases from monitored mailboxes.  These workflows can run Powershell scripts, which allows an incredible amount of flexibility.</li> <li>Airwatch \u2013 Airwatch was recently acquired by VMware as part of their push into the mobile space.  Airwatch is a mobility management suite and supports a very broad range of devices operating systems, including Android, iOS, Blackberry, MacOS and Windows, which would mean cover devices such as laptops, tablets and smartphones.  Device management included application deployment and security of the device.</li> </ul> <p>Overall, the conference was good and it was interesting to see how other state governments were doing their IT, particularly NSW which seems to have taken a stand to set themselves as the ICT leader in Australia (as displayed in their stand in the exhibit hall).</p>","tags":["conference"]},{"location":"2015/10/13/vcloud-air-test-experience/","title":"vCloud Air Test Experience","text":"<p>vCloud Air is VMware\u2019s public cloud offering, similar to Amazon\u2019s AWS or Microsoft\u2019s Azure. The key distincion between vCloud Air and these other offerings is that vCloud Air uses VMware\u2019s products such as vSphere.</p> <p>The VMWare User Group (VMUG) recently added free credits on vCloud Air OnDemand as part of their EVALExperience program. As the name suggests, vCloud Air OnDemand is a pay-as-you-go service. I looked at this service offering as a server engineer with a reasonable background in VMware, considering aspects such as the ease of basic tasks, general administration, technical considerations for the business (good and bad) and how it compares to other offerings.</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#signup-process","title":"Signup Process","text":"<p>The signup process was very easy and allows you to attach the service to an existing VMware account. Once provisioned, it will appear under your MyVMware Subscription Services. This area allows you to view current or past usage, as well as details relating to the service such as payment options.</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#region-selection","title":"Region Selection","text":"<p>Like AWS and Azure, the OnDemand service has regions so you\u2019re prompted to select a geographical region to store your Virtual Machines when you initiate it. At the time of writing, the options are:</p> <ul> <li>Australia</li> <li>Germany</li> <li>Japan</li> <li>UK</li> <li>3 US locations \u2013 California, Texas and Virginia</li> </ul> <p>This is a reasonably good geographical dispersion for the first iteration of this product. The Australia region is located in Sydney so latency should be tolerable for even West Coast administrators.</p> <p>After you select your location, your service will be provisioned, taking a couple of minutes. Once it\u2019s ready, it will present an option to create your first virtual machine.</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#creating-virtual-machines","title":"Creating Virtual Machines","text":"<p>The New Virtual Machine wizard presents VMware\u2019s catalog of templates, as well as allowing you to utilise your own or create a Virtual Machine from scratch. It\u2019s worth noting that the Windows-based templates attract a licensing fee in addition to their general running costs (at the time of writing, this is costed at 9.6 cents per vCPU per hour).</p> <p></p> <p>For my first Virtual Machine, I used the Windows 2012 R2 template. After selecting the template, settings such as RAM, CPU, storage and the name can be tweaked. A cost per hour is displayed, giving an idea of the running cost (this can be changed to a per month display as well). An interesting setting is by default, the CPU and RAM amounts are linked, creating a recommended range of RAM depending on the vCPUs. A 1 vCPU setting will have a 2-16GB of RAM range while for 2 vCPUs the range is 4 to 32GB of RAM.</p> <p>A random password is set when the Virtual Machine is provisioned, which is hidden in the Virtual Machine\u2019s details under the Guest OS heading. The Virtual Machine\u2019s console which doesn\u2019t appear to require any plugins, which is a plus.</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#managing-virtual-machines","title":"Managing Virtual Machines","text":"<p>The Actions menu for Virtual Machines contains the usual options (Power On/Off, Reset) as well as Change Owner, Edit Resources and Connect to Internet.</p> <p>The Edit Resources is an interesting option because it allows an administrator to adjust a running virtual machine\u2019s resources upwards on the fly. As shown below, I changed the memory from 4GB to 6GB without needing to restart the virtual machine. When I dug into the settings for my Virtual Machine in the vCloud Director admin interface, I could see that the Virtual Machine was configured to allow vCPU and RAM hot add.</p> <p></p> <p>By default, Virtual Machines in vCloud Air are on a private IP range and aren\u2019t directly connected to the Internet. This is a distinct difference from AWS or Azure. The Connect to Internet action allows you to expose your virtual machine to the internet, but requires a public IP be assigned to your gateway, which you are charged for. This default setup and approach to getting a virtual machine on the internet may seem counter-intuitive to those more familiar with AWS or Azure but I think it fits in line with the customers VMware is targeting (which I\u2019ll go into more detail later).</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#digging-deeper-into-administration","title":"Digging Deeper Into Administration","text":"<p>vCloud Air has what could be called two administrative interfaces \u2013 there is one for basic admin tasks, but from time to time you will see a \u201cManage in vCloud Director\u201d link when looking at an object. Clicking on this will launch a vCloud Director instance for managing your settings. In this interface you can get into really advanced options for managing Virtual Machines, Networks and so on. For anyone who\u2019s used vSphere 5.5 or 6.0\u2019s web interface, this will probably be a lot more familiar than the default administrative UI.</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#vcloud-connector","title":"vCloud Connector","text":"<p>Earlier I mentioned what I think VMware\u2019s target audience is for this product. Taking a page out of Microsoft\u2019s book in relation to tight administrative integration, VMware have produced vCloud Connector, which allows you to connect your private clouds and public clouds to allow central administration.</p> <p></p> <p>This means in effect your organisation uses the same tool set for managing resources in private and public clouds, use a common set of templates across these clouds and remove the need for specialist knowledge in your IT staff. To get the vCloud Connector system working, you need to setup two Virtual Appliances, the vCloud Connector Server (vCCS) and the vCloud Connector Node (vCCN).</p> <p>Once these are setup and configured to link to your clouds and your vCenter instance, you can manage private and public clouds from a single unified console.</p> <p></p> <p>This setup also allows two new features \u2013 Stretched Deploy and Offline Data Transfer. A Stretched Deploy is essentially moving a Virtual Machine from your in-house cloud to your public cloud. The process creates a network tunnel between the clouds so the Virtual Machine can still communicate with your in-house environment. Offline Data Transfer allows you to transfer groups of powered-off Virtual Machines or Templates to vCloud Air.</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#api-support","title":"API Support","text":"<p>Another point of interest for me was vCloud Air\u2019s API support, specifically Powershell. The API support from a programming point of view seems reasonably developed, with support for manipulating users, creating virtual machines and so on. At the time of writing, the latest update I could find in relation to Powershell support was that the vCloud Air PowerCLI module was still in evaluation and doesn\u2019t appear to have support for vCloud Air OnDemand yet.</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#access-management","title":"Access Management","text":"<p>At the time of writing, vCloud Air by itself doesn\u2019t support integration with Active Directory which means adding access on the web interface is a manual process. I have seen some information that suggests if you use vRealize Automation (vRA), you can use vRA\u2019s Active Directory integration, as well as future plans to include Active Directory integration with vCloud Air.</p>","tags":["vmware"]},{"location":"2015/10/13/vcloud-air-test-experience/#final-thoughts","title":"Final Thoughts","text":"<p>From what I\u2019ve seen so far, it seems clear that VMware is going for a particular vision and selling point with their vCloud Air product. By using the same virtualisation technology that most organisations already use in-house, they have removed a major roadblock for customers to move into a cloud environment. No time consuming messy conversion process required, just transfer over. By positioning the vCloud Connector and vRA as the fulcrum between in-house clouds and vCloud Air, have reduced the requirement for much, if any, staff training. The Powershell support needs another iteration or two to get to a decent mature stage, but it\u2019s an interesting product for a first release.</p>","tags":["vmware"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/","title":"vRealize Automation 7.0 \u2013 First Impressions","text":"<p>vRealize Automation is, as VMware puts it, cloud automation software. It\u2019s the black box where the magic that happens between a customer or consumer of your cloud services and the infrastructure the cloud sits on, providing the services we would normally associate with a cloud service such as self-service, elasticity and multi-tenant support.</p> <p></p> <p>In the past, this product was known as vCloud Automation Center, or vCAC. It was rebranded along with a number of other VMware products under the vRealize banner. However, the newly branded vRealize Automation product still retains some references to vCAC.</p> <p>The previous version of vRealize Automation did a good job at providing the \u201cmagic\u201d for simple IAAS cloud. It required a lot of effort to set up, including a mix of virtual appliances and Windows servers (as shown in the architecture comparison below). There was also a lot of up front effort in setting it up and having to define what would be for many organisations, unknown processes and workflows. Application Services added complexity that required extra agents to work and didn\u2019t seem very stable. Older versions (especially 5.8) gave the impression that the vCAC product was a number of individual products roughly bolted together, with some integration issues. With the combined version 7 product I was hoping some of these issues would be resolved.</p> <p> vRealize 6.x Architecture</p> <p> vRealize 7.0 Architecture</p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#setup","title":"Setup","text":"<p>Upon deploying the virtual appliance and accessing its web interface, the installation wizard automatically initiates. It presents two deployment types \u2013 the minimal deployment has two virtual machines \u2013 the appliance and a Windows machine running the Infrastructure as a service (IAAS) function. This means you\u2019re still stuck with deploying and supporting additional Windows servers. The other deployment type is Enterprise, which adds N+1 redundancy to all the components. Assuming full separation of all the roles, you could end up with as many as 8 virtual machines, plus load balancers. For my first impression I selected the minimal deployment, which the wizard said is suited for development or proof of concept environments.</p> <p>The IAAS server needs a management agent installed, which links it to the vRealize appliance early in the installation wizard. This allows the pre-requisite checker to run on all the Windows machines involved. In the event that you missed anything, a detailed report lists all the items that aren\u2019t compliant and how to fix them. The rest of the wizard features the option to validate a step before proceeding as well as a final overall validation towards the end of the wizard. I found this validation process very good in the detail it provided for fixing items that were not compliant.</p> <p></p> <p>Following the validation, the wizard suggests taking snapshots of the vRealize appliance and any related servers before the installation starts. This is an interesting feature, because it suggests the installation can fail in a way where you can\u2019t simply try again on the same systems.</p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#further-configuration","title":"Further Configuration","text":"<p>The configuration takes place within the vRealize administration interface and very similar to how version 6.x was, with steps such as configuring the system (creating additional tenants and other system-wide settings), configuring tenant settings (branding, notifications, user/group roles) and configuring resources (adding endpoints such as your vSphere environment or supported public cloud, and creating reservations on those endpoints).</p> <p>One of the areas that has received a lot of development since version 6 is the options for authentication. Active Directory authentication is more robust and you are able to synchronise users and groups either from a single domain or from a multi-domain environment. There is also now support for two-form authentication such as RSA SecurID and smart card support. Given the scope of access that an administrator may have on a vRealize Automation system, these extra levels of authentication are a good addition.</p> <p>Security can be further tightened with access policies which can allow you to define network ranges and then what allowed authentication depending on the device. An example of this is you may define a more rigid requirement for access outside your corporate network. In this scenario, the access policy workflow would be something like this:</p> <ul> <li>Define your corporate LAN as a network range, and then set the authentication method to just password if the device used is a web browser</li> <li>Define an \u201coutside\u201d network range and require another authentication method is required such as SecurID.</li> </ul> <p>It\u2019s also worth mentioning just how broad the endpoint support is in this version of vRealize Automation. The full list of end points now includes AWS, OpenStack, vCloud Air, vCloud Director, Hyper-V, KVM, vSphere and NetApp ONTAP.</p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#blueprints","title":"Blueprints","text":"<p>Blueprints are now designed and authored in a new graphical interface. The details to configure the virtual machine for a blueprint are very similar to those which existed in the 6.x interface, with options such as selecting the machine prefix, the type of blueprint and the action performed (ie. Clone an existing virtual machine or create a new one), what resources to assign the machine and so on.</p> <p></p> <p>The new Blueprint Design unifies the design of pure Infrastructure as a Service and the application delivery-based designs in vRealize Automation Application Services (vRAAS) into a single interface. A positive aspect of the separation in the older versions was that vRAAS had a pre-defined set of components such as MySQL and .NET installers, and example deployments. These don\u2019t seem to be present in the new merged vRA 7, but there is an example blueprint that can be imported. Unfortunately, the documentation to do this refers to using wget to download the blueprint onto the appliance, yet wget isn\u2019t installed by default.</p> <p>Another section that works hand in hand with the blueprints is Software Components. The functionality of this section is stuff that used to exist in vRAAS, and it\u2019s where you can define all your various software components such as databases, web applications and scripts. Properties can be defined for the component, which generally take the form of information you would supply during the installation. In the MySQL component that is populated by the Bank example blueprint, the properties include items such as the database root username, password and port.</p> <p></p> <p>These properties can then be referenced in the Actions for the component. The four available actions are Install, Configure, Start and Uninstall, and can be defined as Bash, cmd or Powershell scripts. There\u2019s a reasonable amount of flexibility in how these scripts can run. Take the example of how to get source or install files. The MySQL component from the bank demo will use an install script that will use yum or apt-get to install MySQL, essentially installing it from the internet. In contrast, most of the other components used by the bank demo (such as the JBoss installer and database scripts) will download from the vRealize virtual appliance.</p> <p>The last area relating to blueprints is \u201cXAAS Blueprints\u201d. These blueprints are workflows that can be created and published to the service catalog. These blueprints are for generic activities, such as creating a user, which can be automated and easily applied to a range of scenarios. A form designer allows you to create custom forms to take input that the XAAS blueprint requires. Once defined, they can be published to the service catalog for use.</p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#service-catalog","title":"Service Catalog","text":"<p>The Service Catalog works very similar to vRA 6.x, with the defining of Services (groupings of catalog items), catalog items and entitlements (who can access an item). Requesting an item works a little differently now with some input required, depending on how the blueprint is structured. For a single VM, this takes the form of submitting a reason for the deployment and being able to tweak settings of the VM. For more complicated deployments such as the bank demo, there is more to confirm.</p> <p>Once the request has been submitted, the progress can be followed. The progress is shown in a table view, which isn\u2019t as visually impressive as the flowchart view that vRAAS used.</p> <p></p> <p>During certain steps, you can see the resulting log output. This is very useful in being able to troubleshoot what went wrong without having to trawl through logs on the virtual machines directly.</p> <p></p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#cost-management","title":"Cost Management","text":"<p>One disappointing change in vRealize Automation 7 is that the basic costings system has been removed. In 6.x, there was a basic costing system for certain cloud environments. In version 7, this functionality has been fully removed, requiring the use of vRealize Business for any sort of cost modelling.</p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#identityuser-management","title":"Identity/User Management","text":"<p>While the security options in vRealize are quite broad, identity management is tied solely to Active Directory. There are two options presented for this \u2013 for single domain environments, AD over LDAP can be used. For more complex Active Directory environment, integrated authentication is used. The process for setting up Active Directory authentication is more complicated than previous versions, and in my case, the initial step of creating the link to Active Directory seemed to take a very long time. The steps in all are:</p> <ul> <li>Create a link to Active Directory \u2013 this is where you can opt to use LDAP or Integrated Authentication. If LDAP is used, you need information such as the Distinguished Name of the account used to connect and the location of Active Directory to search for user objects. If using Integrated, you just need to supply a username and password.</li> <li>Confirm the domains you wish to be associated with the Active Directory connection. If you used Integrated Authentication and have a multi-domain environment, you might get more than one listed for this.</li> <li>Confirm the attribute name mappings.</li> <li>Select the groups you want to synchronise (and any other individual users).</li> </ul> <p>In some respects, this approach is better than version 6.x, which worked on specifying a location in Active Directory to synchronise user and group objects from. For many organisations, this might mean having to synchronise from the root of their Active Directory to get adequate coverage, but also result in a very wasteful exercise. The 7.0 model is much more targeted. However, it\u2019s worth noting that because the Distinguished Name of groups and users is used in 7.0, it would mean that moving these objects in Active Directory would almost certainly break things.</p> <p>One thing I also noticed during my testing was any operation relating to Active Directory seemed to be incredibly slow. I couldn\u2019t see any performance conditions such as high CPU utilisation on the vRealize appliance or my domain controller, so it may have been a quirk or something with my configuration.</p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#extending-vrealize-automation","title":"Extending vRealize Automation","text":"<p>vRealize Automation\u2019s functionality can be extended and broadened via the use of vRealize Orchestrator, which is VMware\u2019s task automation product. Fortunately, an instance of Orchestrator is included with the vRealize Appliance. Using this embedded installation seems to remove a lot of the configuration required, as it defaults a lot of settings to use the vRealize Automation settings.</p> <p>Orchestrator has two separate interfaces \u2013 a slick HTML 5 administrative back end for a lot of the nuts and bolts settings that you might have to configure in a stand-alone installation. The majority of activity happens in the Orchestrator Client, a java app.</p> <p>There is a large number of included items in the Workflow Library, including manipulating computer accounts in Active Directory, performing database operations and administrating vRealize Automation itself. Workflows have version history so they can be easily iterated upon. Lastly there are some sample Workflows for HTTP-REST operations.</p> <p>This kind of functionality suggests that the functionality of vRealize Automation could be extended to a very high degree internally (adding new tasks) and externally (interface with other tools or exposing features to other tools via API).</p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#beyond-iaas","title":"Beyond IAAS","text":"<p>The out of the box experience with the previous versions of vRealize Automation was just provisioning infrastructure (ie. Servers). The addition of Application Services on top enabled a broader range of deployment options, such as being able to give your developers a full configured and usable 3-tier application stack.</p> <p>While the administrative functionality of Application Services, as I\u2019ve previously mentioned, has been rolled into the core administrative interfaces of vRealize Automation 7 as Software Components, the process of using it in practice is still as painful as prior versions. There is still a need to install a guest agent and a software bootstrap agent to leveraging Software Components. Both agents require installation where there is no wizard, the correct options must be specified as part of the command line.</p> <p>Given how much of an improvement there was in the initial deployment and setup of vRealize 7, I had hoped there would be improvement in this area as well, perhaps to the point of having a single unified and easy to install agent.</p>","tags":["vrealize automation"]},{"location":"2016/01/29/vrealize-automation-70--first-impressions/#final-thoughts","title":"Final Thoughts","text":"<p>The new version 7 of the product is a good improvement on the previous versions, with all the pieces feeling more of a cohesive system. However once you start getting into depth with it, some of the bug bears from previous versions are still there.</p> <p>Good * Broad endpoint support * Application Services now built in * Simpler architecture</p> <p>Bad * Software provisioning still needs 2 separate agent installs * Basic cost modelling has been removed</p>","tags":["vrealize automation"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/","title":"Want hands on with Virtual Volumes? EMC has a Virtual Appliance for that","text":"<p>Virtual Volumes (VVols) is a new method of managing storage introduced in VMware vSphere 6.0.  Unlike many of the new features in vSphere 6.0, VVols requires not just vSphere 6.0 to work, but a storage device that supports the technology.  Fortunately, EMC have produced a virtual appliance that emulates a storage device with VVol support, so you can get some practical exposure to Virtual Volumes without needing a shiny new storage array.  Download and documetation can be found at http://www.emc.com/products-solutions/trial-software-download/vvols.htm  The process for getting Virtual Volumes completely working is rather long, as the flow chart from EMC\u2019s documentation below shows:</p> <p></p> <p>I\u2019ll run through the steps in getting the Appliance and Virtual Volumes working on a vSphere installation.</p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#provisioning-the-appliance","title":"Provisioning the Appliance","text":"<p>As a Virtual Appliance, it\u2019s easy to get up and running on your vSphere environment.  One cavet worth mentioning is the Appliance has a 12GB memory reservation.  It will not work correctly if you try to change this reservation.  This means your environment must be able to support such a reservation.  In my case, this reservation setting prevented the Appliance from starting on a HP Gen8 Microserver with 16GB of RAM.  So this reservation might be a problem for home lab environment or smaller test labs at work.</p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#adding-storage","title":"Adding Storage","text":"<p>The virtual disks provisioned with the appliance are used only by the system, so to create Storage Pools and other storage related operations later on, you will need to add extra virtual disks.  The disks must be at least 10GB and can be added via the vSphere interface.</p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#accessing-unisphere","title":"Accessing UniSphere","text":"<p>Once the Appliance is powered on, you can access Unisphere using the management IP you configured.  The Configuration Wizard will appear, as it\u2019s the first time you\u2019ve logged in.</p> <p></p> <p>Run through the wizard.  During the wizard you can install a license file.  This can be acquired from the appliance page I linked at the start and only requires an EMC account (don\u2019t need a customer support agreement or the like).</p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#creating-a-storage-pool","title":"Creating a Storage Pool","text":"<p>The wizard used to create a Storage Pool during the Configuration Wizard and after are the same, so the process is easy.  The Storage Pool Wizard will detect any unassigned storage you\u2019ve assigned to the Appliance and list it.  One of the nice things about the storage it lists is it shows the SCSI ID of the disks, which matches the SCSI ID of the virtual disks you added.  This can be helpful if you added a number of disks of the same size.</p> <p></p> <p>There are three default tiers \u2013 Extreme Performance, Performance and Capacity.  To properly use VVols, a Capability Profile has to be created during the Storage Pool creation.</p> <p></p> <p>If you did create a Capability Profile, you then specify constraints.  In the case of my first pool using capacity storage, a number of default settings were included such as Bronze Service Level.  You can also add your own extra tags (maybe things like the media type).  The constraints and tags associated with your Capability Profile are leveraged later on when you create Storage Policies on the VMware side.</p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#creating-virtual-volumes","title":"Creating Virtual Volumes","text":"<p>After the Storage Pools have been configured and defined, you can move on to creating Virtual Volumes.  Under Storage -&gt; VMware, click on the Add icon to start the wizard.  The first page of the wizard asks for the Datastore type to create.  Since you want to create a Virtual Volume, select the third option (VVOL).  The wizard is very short, with just a name and the capability profile to be used.</p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#creating-a-protocol-endpoint","title":"Creating a Protocol Endpoint","text":"<p>The Protocol Endpoint is the middle man between the Virtual Volumes and the ESXi hosts.  At least one needs to be created.  To create the endpoint requires a chain of actions \u2013 create/configure a NAS Server, configure hosts and then finally the endpoint.  To configure the NAS Server, go to Storage &gt; File &gt; NAS Servers and click the Add icon.  The wizard will start.  Under Storage &gt; VMware &gt; Protocol Endpoints, click on the Add icon.  Run through the wizard.  For protocol, select NFS.</p> <p>Next go to Access &gt; VMware &gt; vCenters and click Add.  Enter the details of your vCenter server and click Find.  If successful, it will list all the hosts managed by your vCenter and if they can be imported or not.</p> <p></p> <p>Select the hosts you want to import and click Next.  Finish the wizard.</p> <p>Lastly, go to Storage &gt; VMware &gt; Protocol Endpoints.  Click the Add icon.  You will be able to select the NAS server you just created as well as the hosts that were added.</p> <p>At this point, the Unisphere part of the Workflow is complete.  The next steps involve vSphere itself.</p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#adding-a-storage-provider","title":"Adding a Storage Provider","text":"<p>In the vSphere web interface, select the vCenter server and go to Manage &gt; Storage Providers.  Click the Add icon.  This is where the details of the Appliance can be added.  For the URL, enter https://:8443/vasa/version.xml where  is the IP address or hostname of the management IP on the appliance.  Assuming all goes well, you will be prompted about the certificate on the appliance (since it\u2019s a self-signed one).  Click Yes to trust the host. <p></p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#adding-the-vvol-datastore","title":"Adding the VVOL Datastore","text":"<p>Start the New Datastore Wizard.  On the Type page, select the third option, VVOL, and click Next.  Because the Storage Provider was added, all the Virtual Volumes provisioned inside Unisphere are visible (listed under Backing Storage Container).  Select the item to use and enter a Datastore Name, and click Next.</p> <p></p> <p>Finish off the Wizard as you normally would when adding a datastore.  If all goes well, you will see a new VVOL datastore, as shown below.</p> <p></p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#configuring-storage-policies","title":"Configuring Storage Policies","text":"<p>In my demo with the appliance, I created 3 Storage Pools which were each linked to the three default Capability Profiles \u2013 SP01 as capacity, SP02 as Performance and SP03 as Extreme Performance.  SP01 now exists as the VVOL-based Datastore dsvvol01.  To fully utilise the Capability Profile abilities, Storage Policies are needed.</p> <p>Navigate to the VM Storage Policies area in vSphere.  Click the Create New VM Storage Policy button.  The Wizard will start.  Enter a name for the Policy and click Next.  A short explanation of Rule Sets is shown, click Next.  Because the EMC Appliance has already been added as a Storage Provider, it will be present in the drop-down.  Select EMC.VNX.VVOL.  There are three general types of rules available:</p> <ul> <li>serviceLevel \u2013 Lists a simple service level system of Platinum, Gold, Silver, Bronze and Basic.</li> <li>usageTag \u2013 Appears based on media type (SATA, SSD, SAS, Optimized)</li> <li>storageProprerties \u2013 The most complicated of all, as it contains 4 sub-categories: driveType (the various Tiers), fastCache, raidType, and tieringPolicy.</li> </ul> <p>With the policy I created for the screenshot below, the policy was set to only allow Capacity Tier storage.  As such, only dsvvol01 (which resides on SP01, a Capacity Tier Storage Pool) is listed as compatible.</p> <p></p>","tags":["emc","storage"]},{"location":"2016/02/12/want-hands-on-with-virtual-volumes-emc-has-a-virtual-appliance-for-that/#further-work-and-why-bother-with-virtual-volumes","title":"Further Work and Why Bother With Virtual Volumes","text":"<p>In the first pass I did, I created three Storage Pools (SP01-03), which each had their own Datastore on the EMC Appliance side (vvol01-03), resulting in three individual Virtual Volume-based datastores in vSphere (dsvvol01-03).  This one to one model is very similar to how we do things today, where as VMware administrators we might request a LUN of fast, medium and slow storage and present this as individual datastores.</p> <p>One of the main benefits of Virtual Volumes is to move away from this model to a model based around what capabilities you require.  As such, its possible to provision the same three storage pools (SP11-13) and have them all be in the same VVOL Datastore on the Appliace and present as one Datastore in vCenter.</p> <p>This allows the storage administrator to provision storage based on capability requirements (either singular or multiple).  As an example, the storage administrator might define a standardised capability set for developers that includes settings for database servers to reside on \u201cSilver\u201d grade storage (for better performance) and \u201cBronze\u201d for web servers.</p> <p>This also allows the consumer of the storage to apply the capabilities they need.  Taking the example above, the developers might need better performance out of their web server.  They would simply apply a policy for higher performing storage to their web server and it would be migrated behind the scenes.</p> <p>The Virtual Volume model offloads a lot of operations to the storage device.  In the past, moving a Virtual Machine from one storage area to another or other storage related operations involved spraying traffic over network links.  With Virtual Volumes, vSphere issues high level commands and it\u2019s up to the storage device to manage it.</p> <p>The last benefit is that the storage management can be even more granular than per-Virtual Machine.  The new model restructures a Virtual Machine\u2019s components into a number of separate volumes which can each be managed individually.  These include one for configuration and logs, one for each disk the Virtual Machine has, one for swap and so on.  This is very beneficial for Virtual Machines where you may want better performance for selected disks.</p> <p>In all, Virtual Volumes allows the VMware administrator to focus more on the core aspects of their role, while allowing the same for the storage administrator.</p>","tags":["emc","storage"]},{"location":"2016/03/05/sql-as-a-service-proof-of-concept-with-sql-2012-and-vrealize-automation/","title":"SQL As A Service Proof of Concept with SQL 2012 and vRealize Automation","text":"<p>Standing up a redundant/highly available database infrastructure can be one of the more complicated pieces of work.  Doing it by hand is a long process with any points where errors could happen.  It was with this in mind that I decided to use this as my first \u201cproject\u201d with vRealize Automation.</p>","tags":["vrealize automation","sql server","powershell"]},{"location":"2016/03/05/sql-as-a-service-proof-of-concept-with-sql-2012-and-vrealize-automation/#a-brief-history-of-sql-server-high-availability","title":"A Brief History of SQL Server High Availability","text":"<p>When discussing redundancy or high availability (HA) for databases, there\u2019s two distinct outcomes \u2013 firstly to ensure the continued delivery of the service in the event of infrastructure failure (the actual HA part) and secondly to ensure the data is kept in an orderly fashion (data integrity, no loss of data, etc).  Where these two activities happen depend on the technology used.</p> <p>In older versions of SQL Server, these outcomes were achieved using SQL Clustering.  In SQL Clustering, the HA function was achieved at the server level by having 2 or more servers, while data integrity was maintained by the database residing on shared storage.</p> <p>The use of shared storage on some virtualization platforms presents issues with this approach, with a number of limitations to restrict the benefits of SQL Clustering.</p> <p>By comparison, newer HA technologies from Microsoft that do not have to use shared storage have almost none of these issues.  With AlwaysOn Availability Groups, each SQL server has a \u201clocal\u201d copy of the database.</p>","tags":["vrealize automation","sql server","powershell"]},{"location":"2016/03/05/sql-as-a-service-proof-of-concept-with-sql-2012-and-vrealize-automation/#boundaries-and-details-of-the-design","title":"Boundaries and Details of the Design","text":"<p>I set a number of boundaries on the design of this project.</p> <ul> <li>Use a basic Windows 2012 template \u2013 while it is possible to create a template with a stub install of SQL on it, that would mean more inventory to manage.</li> <li>Activities of the design should be confined to standard blueprints and software components in vRealize Automation (thus excluding the use of vRealize Orchestrator).</li> <li>Two SQL server nodes, one File Share Witness Node</li> <li>Basic storage design (OS disk and SQL disk) for SQL nodes</li> <li>The database and agent services would run under their own service accounts.</li> </ul>","tags":["vrealize automation","sql server","powershell"]},{"location":"2016/03/05/sql-as-a-service-proof-of-concept-with-sql-2012-and-vrealize-automation/#workflow-of-the-deployment","title":"Workflow of the Deployment","text":"<p>The workflow is as shown in the diagram below.</p> <p></p> <p>In the vRealize Automation Blueprint Designer, it looks like this:</p> <p></p>","tags":["vrealize automation","sql server","powershell"]},{"location":"2016/03/05/sql-as-a-service-proof-of-concept-with-sql-2012-and-vrealize-automation/#challenges","title":"Challenges","text":"<p>The largest challenge in this activity was that of security scopes.  vRealize Automation executes workflows on systems by the use of an software agent.  On Windows systems, this agent runs under the context of a local user account that has full local administrative rights on that individual system.  This works well until you need to perform activities outside the scope of that individual system, such as actions on Active Directory.  To get around this I used Powershell Remoting, allowing commands to be run under the context of an account with the required access.</p> <p>The creation of  an AlwaysOn Availability Group also requires two IP addresses \u2013 one for the Windows Failover Cluster object and one for the Listener Object.  There are probably a dozen different ways of achieving this \u2013 in my case I used Windows 2012\u2019s IP Address Management (IPAM) to query and reserve IP addresses via Powershell.</p>","tags":["vrealize automation","sql server","powershell"]},{"location":"2016/03/05/sql-as-a-service-proof-of-concept-with-sql-2012-and-vrealize-automation/#does-it-actually-work","title":"Does It Actually Work?","text":"<p>In testing I performed, the AlwaysOn Availability Group works as expected.  When a client connects to Group Listener, the connection can be seen on the primary node of the Group.  If that node is taken offline, the client is disconnected \u2013 this is expected behaviour.  However, the client can initiate a reconnection to the Group Listener immediately, with this new connection appearing on what Secondary Node (which has now been promoted to Primary as part of the failover process).</p> <p>The Failover Cluster Manager console and the Availability Groups Dashboard both respond to these events as expected.  Once the offline node is brought back online, it re-establishes connections and the Cluster is back in a healthy state, with any required sychronisation between nodes happening automatically.</p>","tags":["vrealize automation","sql server","powershell"]},{"location":"2016/04/04/results-from-telstras-free-data-day/","title":"Results from Telstra\u2019s Free Data Day","text":"<p>Telstra had another free data day on Sunday 3rd April and this time I decided to take advantage of it since my phone has been capable of giving back speed test results of 90Mbits/sec compared to 12 on my ADSL.</p>","tags":["telstra"]},{"location":"2016/04/04/results-from-telstras-free-data-day/#prep-setup","title":"Prep &amp; Setup","text":"<p>Unfortunately I made the assumption that my wifi adapter would work again since the Windows 10 upgrade I performed on my PC.  I went to bed early and set the alarm so I would wake up around 3am.  Due to issues with getting the adapter working, I lost about an hour and started around 4:30am.  The primary aim was to download a number of games on Steam that I hadn\u2019t downloaded yet.</p>","tags":["telstra"]},{"location":"2016/04/04/results-from-telstras-free-data-day/#in-action","title":"In Action","text":"<p>At the start, I was getting very good transfer rates, around 70-80Mbits/sec.  Around 9am, the speed started to drop off.  I\u2019m assuming this is due to people waking up and taking advantage of the free data too.</p> <p>At midday it had slowed to a crawl and was actually slower than my ADSL.  By the afternoon, I had switched back to my ADSL, but kept running regular speed tests on my phone to see if it improved.</p> <p>Later about 10pm, the speeds had improved so I switched back over and kept downloading until midnight.</p>","tags":["telstra"]},{"location":"2016/04/04/results-from-telstras-free-data-day/#results","title":"Results","text":"<p>Data Usage from Windows is as below:</p> <p></p> <p>Details are below.</p> <p></p> <p>And results from the phone:</p> <p></p>","tags":["telstra"]},{"location":"2016/04/06/vmware-horizon-7--first-impressions/","title":"VMware Horizon 7 \u2013 First Impressions","text":"<p>VMware Horizon is a Virtual Desktop Infrastructure (VDI) product which initially allowed provisioning of Virtual Desktops off a base image in an easily to manage fashion.  Over time, VMware have added extra functionally, such as the ability to add Remote Desktop Services (RDS) servers.</p> <p>Version 7 has added a number of interesting features and improvements.  The one mentioned first in the release notes is Instant Clones.  This is a technology I\u2019ve been following for a while, ever since I read about it.  Originally known as VMFork, it\u2019s a technology to allow very rapid, almost instant, provisioning of Virtual Machines.  Duncan Epping wrote a good overview of VMFork/Instant Clone back in 2014.  Support for Virtual Volumes and Linux desktops are some of the other features that have been added.</p> <p>My background in the context of this first impression is I have used 5.3 of Horizon, mostly in the provisioning of standard Windows 7 desktops in providing resources for training and testing.  With that in mind, I\u2019ll be looking primarily at the installation/configuration process for version 7, getting a couple of pools running and how Instant Clone works compared to Linked-Clone.</p>","tags":["horizon"]},{"location":"2016/04/06/vmware-horizon-7--first-impressions/#installation","title":"Installation","text":"<p>The installation process for View 7 is largely similar to earlier versions.  The Connection Server installation has some new options, such as IPv6 support for some roles and an enrolment server role which is used for \u201cTrue SSO\u201d.  The Composer component installs with minimal configuration</p>","tags":["horizon"]},{"location":"2016/04/06/vmware-horizon-7--first-impressions/#first-time-configuration","title":"First Time Configuration","text":"<p>The First Time Configuration workflow is very similar to previous versions \u2013 enter a valid license key, add the vCenter server and so on.  When adding the vCenter server, there is an additional option for the number of concurrent Instant Clone operations.</p>","tags":["horizon"]},{"location":"2016/04/06/vmware-horizon-7--first-impressions/#preparing-gold-images","title":"Preparing Gold Images","text":"<p>The process for setting up gold images for pools is very familiar.  There\u2019s a few new options when installing the Horizon Agent.  The most important one is the option to support Instant Clone.  This is performed by selecting the Instant Clone Agent, however this means you can\u2019t use the same image for a regular pool.</p>","tags":["horizon"]},{"location":"2016/04/06/vmware-horizon-7--first-impressions/#creating-pools","title":"Creating Pools","text":"<p>When creating a new Automated Desktop Floating Pool, the option appears to utilise Instant Clones.  When configuring a pool using standard linked clones, there are a few extra options such as selecting VMware Blast as the default display protocol.  There is also some support for using Virtual Volumes (VVOL) but there\u2019s a number of caveats that restrict its usefulness.  Lastly, there\u2019s an interesting option to over-ride the Transparent Page Sharing scope.  This was a feature in earlier versions of ESXi which allowed more efficient use of memory by sharing pages between virtual machines, but at a point in time VMware deemed it a security risk and disabled it.</p>","tags":["horizon"]},{"location":"2016/04/06/vmware-horizon-7--first-impressions/#working-with-instant-clone-pools","title":"Working with Instant Clone Pools","text":"<p>As mentioned above, you cannot have the regular Horizon Agent and Instant Clone Agent installed at the same time, which means an Instant Clone Pool needs its own base image.  Part if the process in creating the pool is the creation of a number of folders and virtual machines, as shown below.</p> <p></p> <p>This is part of the internal workings of Instant Clone and \u201cnormal\u201d.  Instant Clone Pools have a number of restrictions such as no 3D rendering, lack of sysprep support and only floating user assignment.  These restrictions perhaps make them unsuitable for specific user scenarios, but their rapid provisioning makes them well suited for rapid ramp up in demand on a similar user base, such as user training.</p>","tags":["horizon"]},{"location":"2016/04/06/vmware-horizon-7--first-impressions/#linked-clone-pool-vs-instant-clone-pool-performance","title":"Linked-Clone Pool vs Instant Clone Pool Performance","text":"<p>In my testing I was somewhat limited by my infrastructure, so I tested by expanding each pool by 10 systems (from 1 to 11).  The graph below shows the difference between the two provisioning times:</p> <p></p> <p>In the case of my test, the provisioning was 7 times faster with Instant Clone.  This is a significant difference.</p>","tags":["horizon"]},{"location":"2016/04/06/vmware-horizon-7--first-impressions/#final-thoughts","title":"Final Thoughts","text":"<p>Like a lot of the recent versions of products I\u2019ve been looking at lately, this is more of an evolution rather than revolution.  In its own way, this is a good thing because it means the product is in a stable and mature form.</p> <p>The performance of Instant Clones was pleasantly surprising given the infrastructure I was testing on isn\u2019t anything particularly great (a Synology NAS running NAS disks).</p> <p>The setup and administration is still very consistent with the past versions I\u2019ve used with only a bit of extra awareness and learning that an administrator will have to take on board.  Overall, a pretty good update for this product.</p>","tags":["horizon"]},{"location":"2016/06/29/vmware-nsx--first-impressions/","title":"VMware NSX \u2013 First Impressions","text":"<p>One of the first \u201ckiller applications\u201d on the PC platform was Lotus 1-2-3, a spread sheeting program that greatly improved the productivity of the people using it and making a clear case for buying PCs.  More recently, we\u2019ve seen this sort of thing happening in IT infrastructure, with virtualisation, automation, cloud and \u201cas a service\u201d.  VMware\u2019s NSX product is the latest in a line of products from VMware in this sort of area.</p> <p>If we go back to the \u201cgood old days\u201d of getting a server up and running, it could take weeks.  The diagram below shows the amount of effort involved.</p> <p></p> <p>While some of these numbers may have been more or less depending on circumstances, in many cases it could\u2019ve taken over 150 business hours to get a server ready for use.  Or almost a full month.</p> <p>With the introduction of virtualisation, a number of these tasks were removed or diminished due to the hardware provisioning process being decoupled from the server provisioning process.  With this, plus better automation tools that have appeared in recent years, the common timeline today may look like this:</p> <p></p> <p>The only element of this which hasn\u2019t changed is configuring the network because, in many organisations, the processes and technologies used to configure and maintain a network hasn\u2019t changed much and in many cases, extra complexity has been introduced which increases the burden of administration.</p> <p>NSX tries to address this issue \u2013 of providing network provisioning that can keep up with where compute provisioning is today.  The second issue NSX tries to address is around security.  In many organisations, the emphasis is on securing the edge of the network.  This leaves a corporate network with a robust hard shell but a soft inside.  In a large environment, effectively securing each item can become extremely expensive and/or technically challenging.  Because of the nature of virtualisation, NSX is able to have a good chance at addressing this issue.</p>","tags":["nsx"]},{"location":"2016/06/29/vmware-nsx--first-impressions/#installation-configuration","title":"Installation &amp; Configuration","text":"<p>Running through the first few stages of configuring NSX is fairly basic, although to do it properly would require some amount of planning.  The first hint of the underlying complexity of NSX comes when creating a Logical Router.  Up until this point, all the entities configured exist inside the NSX controller appliance or in vSphere/vCenter configurations.  The Logical Router is its own virtual appliance.  The wizard to add the Router has a lot of settings that are very similar to setting up a virtual appliance via OVA/OVF import, such as passwords and where it lives in the environment.</p> <p></p> <p>By combining Logical Routers, Switches and Edge Gateways in NSX, you can create a structure very similar to a physical network.</p>","tags":["nsx"]},{"location":"2016/06/29/vmware-nsx--first-impressions/#security","title":"Security","text":"<p>The real power in NSX comes from its security system.  You can define Security Polices which apply to a Security Group.  The power in this arrangement comes from the fact that the membership of a Security Group is defined dynamically, using a set of criteria.  Most examples of using NSX out on the web use the criteria of Virtual Machine name (ie. All Virtual Machines names containing \u201cweb\u201d).  Other options include the name of the Operating System, the Computer Name, Security Tag or Entity.  You can also define objects to always include or exclude.</p> <p>This model allows relative ease in creating polices and applying them.  One broad reaching scenario outlined in the NSX documentation is creating a policy that locks down any machine with a detected virus on it (by the anti-virus software applying an \u201cinfected\u201d security tag, which tend causes the network quarantine policy to apply).</p> <p>Once you\u2019ve defined Security Groups and Policies and related them, the groups can be viewed in the Canvas, which gives a high level visual view of how many Policies, rules and other settings are applied and to which Virtual machines.</p> <p></p> <p>In the screenshot above, I\u2019ve defined 2 firewall rules in a General Web Servers policy which allows HTTP and HTTPS traffic.</p> <p>The architecture of NSX is such that the Firewall component actually lives on the ESXi hosts, meaning traffic is inspected on egress and ingress on the ESXi host.  Other functionality includes the ability to prevent network traffic spoofing via IP and MAC address validation.</p>","tags":["nsx"]},{"location":"2016/06/29/vmware-nsx--first-impressions/#integration","title":"Integration","text":"<p>VMware\u2019s promotional material make a lot of noise about integration with NSX.  Firstly there seems to be decent integration with their vRealize Automation product.  There is also a REST API and Powershell commands available.  Third party support comes in a range of vendors, including Symantec and Palo Alto.  This level of integration and support is good because it allows scenarios like the one described earlier with an anti-virus outbreak, where the anti-virus management system is able to flag infected virtual machines.</p>","tags":["nsx"]},{"location":"2016/06/29/vmware-nsx--first-impressions/#final-thoughts","title":"Final Thoughts","text":"<p>I think NSX is at a point where it\u2019s a pretty mature product.  As I wrote earlier in this piece, getting it setup is pretty easy, it just requires good planning beforehand to get the most out of it.  Some of the more exotic features like fenced networking allow some really interesting design options.  Any organisation with a network of reasonable complexity should be looking at this product or something similar as one of their next options for improving IT outcomes.</p>","tags":["nsx"]},{"location":"2017/07/10/adding-extra-service-mappings-to-nexpose/","title":"Adding extra service mappings to Nexpose","text":"<p>Nexpose does have good coverage of services in the \u201cwell known\u201d range of ports (0-1024).  An environment with a lot of propriety systems will cause Nexpose to some services as unknown or even misidentifying them.  The screenshot below is a good example of this.</p> <p></p> <p>The example is from a Domain Controller.  Nexpose identifies the port 3389 service correctly as RDP.  Ports 3269/32769 are used by the Global Catalog service, so labelling them as LDAP/LDAPS isn\u2019t strictly accurate.  For port 3260 and 5666 it gives up.  Depending on your needs, you may want to get these labels a bit more accurate.  This can be achieved by using a custom service names file (you can alter the default one, but it\u2019s probably best to leave that in its default state).</p> <p>The default file, default-services.properties, is located in the <code>&lt;install location&gt;/plugins/java/1/NetworkScanners/1</code> folder.  The format is basic, with each line as <code>&lt;port #&gt;/&lt;tcp or udp&gt;=&lt;Service Name&gt;</code>.  Some of the custom ports I added are shown below:</p> <p></p> <p>Once the properties file is in a state you\u2019re happy with, place it in the same folder as the default one and either create a new or edit an existing scan template and put the file name into the field on the Service Discovery section.  Load up the page of an asset to test and queue a scan on it with the scan template.  The reported services should update with the new values.</p>","tags":["nexpose"]},{"location":"2017/08/17/vcsa-65-root-account-password--reset-and-cause-investigation/","title":"VCSA 6.5 Root Account Password \u2013 Reset and Cause Investigation","text":"<p>One of the more frustrating experiences one can experience with VMware\u2019s vCenter Server Appliance (VCSA) is having the root account locked out or forgetting the password for it.  I recently experienced this after I rebuilt the VCSA in my home lab from scratch.</p> <p>How to Reset the VCSA Root Password VMware have a short process on how to reset the password for the root account, detailed in KB2147144.  THe process is:</p> <ol> <li>Backup the VCSA (via snapshot or other means)</li> <li>Reboot the VCSA</li> <li>During the boot process, when the photon splash screen appears press the e key to get into the boot menu</li> <li>In the text box that appears, go to the line starting with \u201clinux\u201d.  Go to the end of the line (which is right after the text \u201cconsoleblank=0\u201d) and enter the text <code>rw init=/bin/bash</code>  This will cause the boot process to jump right into the bash shell without needing credentials</li> <li>Press F10 to continue booting</li> <li>At the command prompt, run the <code>passwd</code> command to reset the password</li> <li>Unmount the file system by running <code>umount /</code></li> <li>Reboot the VCSA using <code>reboot -f</code></li> <li>Following reboot, confirm the new password works</li> <li>If you took a snapshot in step 1 remove it</li> </ol>","tags":["vcenter"]},{"location":"2017/08/17/vcsa-65-root-account-password--reset-and-cause-investigation/#cause-investigation","title":"Cause Investigation","text":"<p>After resetting the password and restarting, I still couldn\u2019t login.  One thing I noticed was there had already been 20 login failures.  In my situation, the VCSA was working one evening and the following morning the login issues happened.  Something had to be causing these issues.  Using the password reset process to get to the bash shell again, I looked around in some logs.  First I tried checking the /var/log/messages log.</p> <p></p> <p>Unfortunately, using \u2018FAILED\u2019 only showed 2 login attempts on the console which were caused by myself after the lockout happened.  Failed SSH login attempts are logged under a text string that uses \u2018Failed\u201d.  The second search attempt used \u2018Failed\u2019 and yielded better results.</p> <p></p> <p>The log had numerous entries for 192.168.1.55 trying to login using root and other accounts.  The system on 192.168.1.55 was the trial of Nexpose.  Even though I didn\u2019t have credentials set in Nexpose to logon to the VCSA, it still was trying to logon using root and was causing the failures.  This hadn\u2019t been an issue prior to the VCSA rebuild.</p> <p>I excluded the VCSA from the scanning that Nexpose performs and did the password reset process again.  I was now able to login successfully.</p>","tags":["vcenter"]},{"location":"2017/10/11/project-honolulu--first-impressions/","title":"Project Honolulu \u2013 First impressions","text":"<p>Project Honolulu is Microsoft\u2019s attempt at revamping the server administration experience.  Historically the Windows server toolkit has been built around using numerous MMC (Microsoft Management Console) plugins \u2013 things like Event Viewer, AD Users and Computers and DNS Management are all built on MMC.  We\u2019ve seen a couple of attempts at revamping this in the past, there was Server Manager in 2008 and a refreshed form in 2012.</p> <p>I suspect one of the driving forces behind Honolulu is the shift from RPC-based connectivity to WinRM for remote administration of servers.  Honolulu seems to represent an alignment with this since it supports only Server 2012 onwards as nodes to manage, and its gateway component installs on Windows 10 or 2016.  The documentation claims the management functions are performed using remote powershell or WMI over WinRM.</p>","tags":["it"]},{"location":"2017/10/11/project-honolulu--first-impressions/#installation","title":"Installation","text":"<p>The installer for Project Honolulu is only about 30MB.  While it supports installation on a system in a workgroup configuration, the TrustedHosts value for WS-Man needs to include the target nodes to be managed.  The installer can do this for you or you can manually perform the commands to do it.  On a domain joined system this isn\u2019t required.</p>","tags":["it"]},{"location":"2017/10/11/project-honolulu--first-impressions/#starting-up-project-honolulu","title":"Starting Up Project Honolulu","text":"<p>Following install, the web interface will load with a tour splash screen.  The main landing page is bare to start with.</p> <p>Clicking on the Add button presents three possible options \u2013 a regular server connection, a fail-over cluster or a hyper-converged cluster.  The first 2 options then allow the adding of single items or importing in bulk from a test file.  The hyper-converged cluster option allows only single items.  When adding in the server name or IP, Honolulu will appear to perform ongoing checking of the name to ensure it meets correct requirements and if it can connect with current credentials.</p> <p>Single sign-on authentication is supported and is the default option.  In the case of my test scenario, the system running Honolulu was in a workgroup and attempting to connect to domain based systems.  For this, I manually specified credentials.  One thing to note is it appears that even if a server is added using IP, the name will be resolved (perhaps by the initial connection) and subsequent connection attempts will fail if this name can\u2019t be resolved by DNS.  For domain-joined installations this should be  a very rare case, in workgroup configurations it could happen.  Honolulu will initiate its connections to the target node on the target\u2019s port 5985.  Assuming initial connection and authentication is successful, you should see a HTTP/1.1 Status 200 in a network capture.  If everything is good, the status will have a tick and say \u201cOnline\u201d.  From there you can click on the server\u2019s name, or select it and click Connect to drill down further.</p>","tags":["it"]},{"location":"2017/10/11/project-honolulu--first-impressions/#overview-page","title":"Overview Page","text":"<p>The first page shown is an overview of the server and includes some metric graphs like CPU and RAM usage and the ability to shutdown or reboot the server remotely.</p> <p>On the left is a collapsible menu of the other sections such as events, firewall, registry, processes, etc.</p>","tags":["it"]},{"location":"2017/10/11/project-honolulu--first-impressions/#tools","title":"Tools","text":"<p>Some of the tools seem quite functional.  The Events one allows filtering of the event log with a reasonable number of criteria and allows exporting.  Files allows browsing of the target node\u2019s file system with the ability to create folders, rename/delete and so on.  The Process section looks to have all the main functions you would typically use in Task Manager, allowing remote termination of processes.</p>","tags":["it"]},{"location":"2017/10/11/project-honolulu--first-impressions/#closing-thoughts","title":"Closing Thoughts","text":"<p>Project Honolulu is an interesting tool from Microsoft.  It seems capable of replacing the traditional Server Manager app that most Windows sytem administrators are familar with.  I\u2019ll be most interested in seeing how it develops in terms of extensions.</p>","tags":["it"]},{"location":"2018/06/19/blizzards-it-architecture-and-testing-at-blizzcon-2017/","title":"Blizzard\u2019s IT Architecture and Testing at Blizzcon 2017","text":"<p>Last November I was able to attend Blizzcon in Anaheim.  Blizzcon is the annual convention hosted by Blizzard Entertainment (creators of Overwatch, Diablo, Starcraft, World of Warcraft, etc).  In the past the focus has been solely on the games and the game developers.  In the last 2-3 years there have been more panels that give more of a look \u201cbehind the curtain\u201d.  These panels have more information about design processes and engineering practices at Blizzard.  There were 2 panels I went to which highlighted this \u2013 one was engineering and the other was about level design.  Some points that jumped out were:</p>","tags":["games","blizzcon"]},{"location":"2018/06/19/blizzards-it-architecture-and-testing-at-blizzcon-2017/#blizzards-overarching-architectural-philosophy","title":"Blizzard\u2019s Overarching Architectural Philosophy","text":"<p>During the Q&amp;A for the engineering panel, the engineers were asked about whether there was any sort of mandated technologies that have to be used across the business or in particular areas.  The response?  They used whatever technology or tools that made sense for that area of the business and its needs.  The team that handles the websites end up using technologies that make sense in that area.  This led into a discussion about the Blizzard\u2019s use of APIs as the means to allow these different technology islands to talk to each other.  This approach allows the best tools for the job in an area, but creates a reliance on ensuring any API changes to don\u2019t have downstream effects.  Which leads into the next topic\u2026</p>","tags":["games","blizzcon"]},{"location":"2018/06/19/blizzards-it-architecture-and-testing-at-blizzcon-2017/#testing-and-documentation","title":"Testing and Documentation","text":"<p>There was an interesting reference to how Blizzard deal with keeping documentation up to date.  With their reliance on APIs, there would most likely be a process where changes have to be tested.  Part of their test model involves taking sample data and assets from documentation and run tests with it.  If the documentation\u2019s samples haven\u2019t been updated reflect changes in functionality, the test should fail and be flagged.  This approach isn\u2019t completely foolproof, but it was an interesting approach to the issue of documentation in IT.</p>","tags":["games","blizzcon"]},{"location":"2018/06/19/blizzards-it-architecture-and-testing-at-blizzcon-2017/#giving-people-space-to-be-creative","title":"Giving people space to be creative","text":"<p>The level design panel blew away one major assumption I had about Blizzard\u2019s level design process for World of Warcraft.  My assumption was that the game designers would detail the game world to a fine degree.  The level design people would build that without much scope for changing things.  The reality was that the game designers would only outline what a particular zone or area would need (mostly in terms of quest flow or general look and feel).  It was the level designers who would flesh out the world.  Many of those pieces of \u201ccharacter\u201d or \u201cflavour\u201d in the game world were due to the level designers filling those gaps with their own stories.</p> <p>I\u2019m hoping in the future, they\u2019ll keep doing these sort of panels.  One with a bit more focus on the infrastructure side of things would be cool to see.</p>","tags":["games","blizzcon"]},{"location":"2018/06/19/vmware-vrealize-suite-lifecycle-manager-12--first-impressions/","title":"VMware vRealize Suite Lifecycle Manager 1.2 \u2013 First Impressions","text":"<p>When VMware created the vRealize brand, they grouped together some of their most complex products under one banner. vRealize Automation (vRA) required the deployment and configuration of two components \u2013 a virtual appliance and a Windows server. The Windows server had a long list of prerequisites. In terms of operational management, using products like vRA meant ongoing work on scripts, workflows and other artifacts. The logical response to this is to create a non-production instance to protect your production instance. Moving updates to production could be achieved manually or via VMware\u2019s Codestream product, but both approaches left a lot to be desired. vRealize Suite Life Cycle Manager (vRSLCM or just LCM) is a new approach to this set of problems.</p>","tags":["vrealize automation","lifecycle manager"]},{"location":"2018/06/19/vmware-vrealize-suite-lifecycle-manager-12--first-impressions/#getting-lcm-running","title":"Getting LCM Running","text":"<p>LCM comes supplied as a \u201cVirtual Application\u201d where a few configuration options are required to provision it. One of the LCM-specific settings is whether you want to enable the vaguely named \u201cContent Management\u201d. Enabling this will cause the appliance to use 4 processors instead of 2. Once the appliance is deployed and started, the rest of the configuration happens via the web interface. </p>","tags":["vrealize automation","lifecycle manager"]},{"location":"2018/06/19/vmware-vrealize-suite-lifecycle-manager-12--first-impressions/#configuration-options","title":"Configuration Options","text":"<p>Many of the technical settings are hidden in the System Settings area. One important item in this area is adding product binaries, as one of the main functions of LCM is deploying other vRealize products. Another important area is the \u201cMy VMware\u201d section where you can put in your VMware credentials to integrate licensing and product downloads.</p> <p></p> <p>In terms of getting things to a point of deploying products, VMware requires you to setup some structure first. LCM uses a \u201cData Center\u201d as it\u2019s highest level container, which you can then associate vCenter servers with and Environments. Adding a Data Center requires just a name and location, while adding a vCenter Server is a matter of selecting the Data Center you want to associate with it and entering the vCenter Server\u2019s details.</p> <p></p> <p>Creating an Environment is where settings become more detailed, including specifying an Environment Type.</p> <p></p>","tags":["vrealize automation","lifecycle manager"]},{"location":"2018/06/19/vmware-vrealize-suite-lifecycle-manager-12--first-impressions/#deploying-products","title":"Deploying Products","text":"<p>Once an Environment is created, the interfaces for deploying Products (individual vRealize products) or Solutions (combinations of products) becomes available. Each Product option can have its version and deployment type specified.</p> <p></p> <p>Picking a product to deploy initiates a long, multi-step wizard. It\u2019s during this wizard you\u2019re most likely to hit the first snag in getting a product deployed, as the wizards have validation checks that rely on you performing prerequisite work first. This can be especially annoying when attempting to deploy vRA, as LCM assumes the Windows-based IAAS server component is already staged and ready. One nice feature is at the end of the wizard you can download a JSON-based configuration file, as the deployment process supports using a preexisting configuration file. Once the wizard is finished and the deployment starts, it appears under the Request section is an updating progression graphic.</p> <p></p> <p>When the Request is finished, the product can be managed, including getting logs, exporting the configuration or even performing upgrades to newer versions.</p>","tags":["vrealize automation","lifecycle manager"]},{"location":"2018/06/19/vmware-vrealize-suite-lifecycle-manager-12--first-impressions/#marketplace","title":"Marketplace","text":"<p>This is a section in LCM that has a range of virtual appliances that you can easily deploy to extend the functionality of VMware\u2019s products. Items can be filtered by target product, the vendor or what technology it relates to.</p> <p></p> <p>Where the Marketplace becomes particularly useful is if your environment has a production and non-production vRA instances. One of the Marketplace items is a Gitlab appliance, which LCM can use for migrating changes between vRA instances.</p>","tags":["vrealize automation","lifecycle manager"]},{"location":"2018/06/19/vmware-vrealize-suite-lifecycle-manager-12--first-impressions/#content-management","title":"Content Management","text":"<p>This is the area where LCM gets interesting if, like me, you\u2019re having to deal with transferring artifacts between a non-production vRA instance and a production one. The Content Management function of LCM is essentially a more mature and capable form of Codesteam and uses Gitlab. What this helps achieve is more visibility on what was changed over time, avoid the \u201cstepping on each others toes\u201d problem of trying to edit the same thing at the same time, as well as allowing easy rollback.</p>","tags":["vrealize automation","lifecycle manager"]},{"location":"2018/06/19/vmware-vrealize-suite-lifecycle-manager-12--first-impressions/#rest-api","title":"REST API","text":"<p>Like many of VMware\u2019s products, LCM has a REST API that allows operations to be performed via automation tools. This functionality can be leveraged as part of a larger workflow relating to deploying one of the products that LCM supports. For example, the larger workflow could include earlier staging steps required before LCM\u2019s activities or actions after LCM\u2019s part to further configure what was deployed.</p>","tags":["vrealize automation","lifecycle manager"]},{"location":"2018/06/19/vmware-vrealize-suite-lifecycle-manager-12--first-impressions/#final-thoughts","title":"Final Thoughts","text":"<p>Life cycle Manager certain seems to fill a gap if the environment you\u2019re working in has many of the VMware products that it supports and you need a more robust and mature way to manage them. I\u2019ll be looking at trying to prototype a concept deployment via LCM in the near future.</p> <p></p>","tags":["vrealize automation","lifecycle manager"]},{"location":"2018/09/12/installing-elasticstack-beats-on-vcenter-67/","title":"Installing ElasticStack Beats on vCenter 6.7","text":"<p>I recently deployed a vCenter appliance to 6.7 after a power outage corrupted the 6.5 instance.  A followup task for the virtual appliance was getting the ElasticStack Beats (MetricBeat, Filebeat) installed again.  In this post, I will go through the process of installing the Beats and some of the minor issues I ran into.</p> <p>vCenter 6.7 Package Management VMware\u2019s vCenter 6.7 virtual appliance is based upon the Photon OS.  Photon uses a custom package manager called tdnf (Tiny Dandified Yum).  Like Yum, it has repositories.</p> <p></p> <p>tdnf appears to use the same format for repositories as yum in terms of location and content.  Based on this, I attempted to install Filebeat first using the repo-based approach as I found it did a better job of handling file permissions.</p>","tags":["elasticsearch","vcenter"]},{"location":"2018/09/12/installing-elasticstack-beats-on-vcenter-67/#adding-the-elasticstack-repository","title":"Adding the ElasticStack Repository","text":"<p>Adding the repository requires a few deviations from what\u2019s in ElasticStack\u2019s documentation.  Firstly, it appears tdnf doesn\u2019t support a HTTP URL for the gpgkey value.  The default repositories use file URLs.  So the first variation is downloading the gpgkey file.</p> <p>When creating the .repo file, the gpgkey path needs to reflect where the file was downloaded to.  For the sake of consistency, I put it in the same directory as the other repositories. <pre><code>[elastic-6.x]\nname=Elastic repository for 6.x packages\nbaseurl=https://artifacts.elastic.co/packages/6.x/yum\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/GPG-KEY-elasticsearch\nenabled=1\nautorefresh=1\ntype=rpm-md\n</code></pre></p> <p>After doing these steps, the tdnf repolist command output shows a new entry.</p> <p></p> <p>Just to confirm that tdnf would use the new repository correctly, I ran commands that would return information about the FileBeat package.</p> <p></p> <p>With the correct gpgkey value, the install can proceed.  An issue with this that I ran into was just specifying the beat package name will cause the 32-bit version to be installed.  This version will fail to run.  This can be avoided by explicitly installing the 64-bit version, using the command: <code>tdnf install metricbeat.x86_64</code> (Thanks to Paul in the comments suggesting this).</p>","tags":["elasticsearch","vcenter"]},{"location":"2018/09/12/installing-elasticstack-beats-on-vcenter-67/#configuring-the-beats","title":"Configuring The Beats","text":"<p>Following the installation, the configuration files can be updated for each beat.  In my case, I already had a configuration file for MetricBeat.  As a sanity check, I do a test on the configuration and an initial run via command line, outputting to the console</p> <p></p> <p>Upon viewing data in Kibana, it\u2019s possible to confirm that the MetricBeat information is flowing through correctly.  Since the Beat is working, the last step can be performed, which is to configure it to run automatically.</p> <p></p> <p>The same steps can be repeated for FileBeat.  In my case I was primarily using just MetricBeat to collect statistical data from the vCenter appliance.</p>","tags":["elasticsearch","vcenter"]},{"location":"2018/09/19/improving-the-vra-customer-experience--send-chef-errors-to-slack/","title":"Improving the vRA Customer Experience \u2013 Send Chef errors to Slack","text":"<p>One of the issues that can be amplified by automation is logging.  Some logs have an ephemeral nature, having a short lifespan due to various factors.  This can be especially painful if the logs relate to failures and contain information that could assist in fixing the problem.</p> <p>This was the issue I was seeing when vRealize Automation (vRA) requests would fail when Chef attempted to apply settings.  If Chef failed critically, vRA would be made aware of it and fail the entire request.  Of course, vRA would then delete the virtual machine and the local Chef logs.  In many cases, there was a gap of only a minute or two between the Chef failure and the vRA cleanup tasks.</p> <p>The approach I wanted to take was somehow getting those logs or at least the error message off the affected virtual machine and put that data somewhere else.  It could be an email, it could be a shared folder.  I eventually decided upon sending it to a Slack channel by triggering a vRealize Orchestrator (vRO) workflow.  Some of the reasoning behind this included:</p> <ul> <li>There was already a precedent in the organisation for having critical errors be fed into Slack</li> <li>There was already existing generic workflows in vRealize Orchestrator (vRO) for creating Slack messages</li> <li>vRO is an internal system, accessible by the network zones likely to have deployed virtual machines.  This meant being able to reach the vRO REST API from these virtual machines had a high chance of working.  Going directly to Slack would require a lot of firewall rule pain</li> </ul> <p>The existing workflows were generic enough that all I needed to specify was the target (a Slack Channel or user) and the message.  The final piece was how to trigger all this on the virtual machine experiencing the failure.</p>","tags":["vrealize orchestration"]},{"location":"2018/09/19/improving-the-vra-customer-experience--send-chef-errors-to-slack/#chef-handlers","title":"Chef Handlers","text":"<p>Chef allows \u201cHandlers\u201d to be executed when the Chef Client experiences a certain scenario.  One of these scenarios is when an exception happens.  At a high level, I wanted the Handler to be triggered on an exception, collect the exception details and pass those details onto the vRO workflow via REST API.</p>","tags":["vrealize orchestration"]},{"location":"2018/09/19/improving-the-vra-customer-experience--send-chef-errors-to-slack/#handler-code-detail","title":"Handler Code Detail","text":"<p>Credit has to go to Eike Waldt (AKA yeoldegrove) and his HTTPAPI Handler code.  The main changes I had to do from his base was some formatting hoop jumping because of how vRO likes to receive JSON.  First I define the error message to sent: <pre><code>@api_error_message = \"Something went horribly wrong when converging `#{node.name}` :scream:  Some details are below, maybe they can help... :thinking_face: \\\\n*Run List:*\\\\n`#{formatted_run_list}`\\\\n*Exception:*\\\\n`#{run_status.exception}`\"\n</code></pre> Eike\u2019s original code also included the backtrace but I found this makes the slack message almost unreadable and too long.  In a divergence, I use the handler by calling it directly in a recipe using the chef_handler resource. <pre><code>chef_handler 'ErrorsToSlackModule::ErrorsToSlack' do\n  source \"#{node['chef_handler']['handler_path']}/errors-to-slack.rb\"\n  supports start: false, report: false, exception: true\n  action :enable\nend\n</code></pre></p> <p>When the cookbook hits the failure, it will hit the vRO REST API to execute the workflow.  The default is a slack message with some relevant info about the error.</p> <p></p>","tags":["vrealize orchestration"]},{"location":"2018/09/19/improving-the-vra-customer-experience--send-chef-errors-to-slack/#effort-vs-reward","title":"Effort vs Reward","text":"<p>When I did my first pass at this code, I spent 1 or 2 days on it.  I figured that if I couldn\u2019t get it working after that point, I\u2019d shelve it.  However if I managed to get it working it would have some good benefit in relation to the time taken.  Given the scenario I think it turned out well.</p>","tags":["vrealize orchestration"]},{"location":"2018/09/28/improving-the-vra-admin-experience--reservation-alerts-to-slack/","title":"Improving the vRA Admin Experience \u2013 Reservation Alerts to Slack","text":"<p>The Reservation system in vRealize Automation (vRA) provides a bucket of resources to a team or business unit via Business Group.  A risk with Reservations comes about with how I think VMware intended them to be used vs how some organisations may use them.  I suspect VMware\u2019s intention was that Reservations should be self-managed by the Business Group associated with it.  This makes sense if each individual team has a Business Group as the scope of what\u2019s in the Reservation is \u201ctheir stuff\u201d.  It would mean if a Reservation reached capacity, it would be up to that team to manage the situation.</p> <p>What if the Business Group was being used differently, where it covers multiple teams?  In the event of the Reservation becoming full, the scope is larger than one team.  In this situation, it might be good to get a heads up on when Reservations are running low on resources.  Email alerts can be setup and yes, sent through to Slack, the formatting in Slack is less than desirable.  So I decided to look at a way of doing it better.</p>","tags":["vrealize orchestration"]},{"location":"2018/09/28/improving-the-vra-admin-experience--reservation-alerts-to-slack/#re-using-the-wheel","title":"Re-using The Wheel","text":"<p>In the entry about sending Chef errors to Slack, I had written a generic vRealize Orchestrator (vRO) workflow that can send a message to Slack.  Since that part of things exist, all I need to do is get the Reservation information out of vRA.</p>","tags":["vrealize orchestration"]},{"location":"2018/09/28/improving-the-vra-admin-experience--reservation-alerts-to-slack/#getting-reservation-data-approach-options","title":"Getting Reservation Data \u2013 Approach Options","text":"<p>There\u2019s a few ways to possibly extract the information about Reservations in vRA.  When looking at the API Explorer inside vRO, there is references to Reservation objects.  When I looked at using this approach, I found that it wasn\u2019t exposing the information in a easy-to-use fashion.  There was hopping from one object type to another, with the actual information stored in a format that required a little too much effort to deconstruct.  The vRA API exposes Reservation actions directly.  Specifically, by using the /api/reservations/info action, it will return summary information about Reservations (using /api/reservations returns a lot more detailed information than I needed).</p>","tags":["vrealize orchestration"]},{"location":"2018/09/28/improving-the-vra-admin-experience--reservation-alerts-to-slack/#introducing-the-vcac-rest-client","title":"Introducing the \u201cvCAC\u201d REST Client","text":"<p>Using vRA\u2019s REST API would imply all the hoop-jumping required, like getting your bearer token constructed and so on.  Fortunately, vRO can leverage an internal REST client of sorts.  By using the vCACCAFEHost object of your vRA server, it\u2019s possible to leverage a createRestClient method, which allows a REST client to be easily created. <pre><code>// Find the VCACHost by its ID\nvar vcacCafeHost = Server.findForType(\"vCACCAFE:VCACHost\",vcacHostId);\n// Define the endpoint\nvar endpoint = 'com.vmware.vcac.core.cafe.reservation.api';\n// Create a REST client with this endpoint\nvar restClient = vcacCafeHost.createRestClient(endpoint);\n</code></pre></p> <p>Once this is done, it\u2019s just a matter of feeding in the sort of things we normally would for a REST call.  This process is easier than a regular REST API call as we don\u2019t have to worry about JSON payload formatting or similar issues. <pre><code>// Define the resource URL\nvar resItemsUrl = \"reservations/info\";\n// Perform a GET request using the resource URL\nvar resItems = restClient.get(resItemsUrl);\n// resItems is a vCACCAFEServiceResponse object type, with a .getBodyAsString method that gets the response body as a string response\nvar response = resItems.getBodyAsString();\n// Parse the response into a JSON object\nvar json = JSON.parse(response);\n</code></pre> Once the response has been returned, it\u2019s a matter of iterating through it and checking values against a threshold.  If the threshold is exceeded, an alert message text is constructed and sent to Slack via the Slack Message workflow. <pre><code>// High memory condition\nSystem.log(\"Too much memory allocated, sending slack message!\");\n// Setup Slack items\nvar slackwfMemAlloc = Server.getWorkflowWithId(slackWorkflowId);\nvar slackwfMemAllocProperties = new Properties();\nvar slackMessageMemAlloc = \":rotating_light: WARNING: *\" + tmpResMemAlloc + \"%* memory allocation on reservation `\" + x.name + \"`! :rotating_light:\";\nslackwfMemAllocProperties.put(\"slackMessage\",slackMessageMemAlloc);\nvar slackwfMemAllocToken = slackwfMemAlloc.execute(slackwfMemAllocProperties);\n</code></pre> The resulting output is shown in the screenshot below.</p> <p></p> <p>Since the Slack message format allows a reasonable amount of formatting in a syntax that doesn\u2019t break vRO\u2019s javascript, you can customise the message output to your requirements without worrying about things breaking.</p>","tags":["vrealize orchestration"]},{"location":"2018/12/11/creating-service-accounts-with-vrealize-orchestrator/","title":"Creating Service Accounts with vRealize Orchestrator","text":"<p>vRealize Orchestrator (vRO) has a lot of plugins that allow it to integrate with other systems and services.  One of such plugin is for Active Directory.  This plugin allows you to perform a number of standard AD activities, like creating users.  vRO already has built in workflows to create and manipulate users.  In this post, I\u2019m going to run through what you might end up implementing if you wanted to be able to create Service Accounts via vRO.</p>","tags":["vrealize orchestration"]},{"location":"2018/12/11/creating-service-accounts-with-vrealize-orchestrator/#the-scenario","title":"The Scenario","text":"<p>Some questions may come up as to why you would need to create a workflow to do this.  One question might be why we would want to create Service Accounts via vRO.  There may be times when you need a Service Account for a new system or service (such as SQL Server) being deployed via vRO.  Rather than use the same shared Service Account, your organisation may prefer to use individual ones.</p> <p>The second question that may come up is why not use the existing Workflows.  These Workflows may achieve the core goal of creating the Service Account in Active Directory but they don\u2019t achieve the nuance of what might be required without some extra code.  Using some of the organisations I\u2019ve worked at as an example, some of these extra bits may be:</p> <ul> <li>The password needs to be significantly longer</li> <li>The password needs to be stored in some sort of password or secret management solution so other administrators can reference it</li> <li>Service Accounts have to be created in certain Organisational Units</li> </ul> <p>Based on these sort of factors, the out-of-the-box workflows can\u2019t fully achieve what\u2019s required.  Also, you may want to take some control or work out of the process (like removing the choice of target Organisational Unit or the generation of the password).  Lastly, you may want to put some extra protections or controls in place, like checking if the requested Service Account name exists already.</p> <p>Based on these factors, it would be quite reasonable to have a request or user story that outlines the following:</p> <ul> <li>The Workflow allows the Service Account Name to be specified</li> <li>The Workflow checks to ensure the specified name isn\u2019t already in use</li> <li>The password for the Service Account is randomly generated and meets organisational requirements</li> <li>The password for the Service Account is saved to our password/secret management system</li> <li>The account is created in a predetermined Organisational Unit</li> </ul>","tags":["vrealize orchestration"]},{"location":"2018/12/11/creating-service-accounts-with-vrealize-orchestrator/#design-and-building-blocks","title":"Design and Building Blocks","text":"<p>Even if the existing Workflows aren\u2019t suitable for our purpose, there are pieces we can take from them.  The Workflow \u201cCreate a user with a password in an organizational unit\u201d is very close to the functionality we want and the core of this Workflow is the Action called \u201ccreateUserWithPassword\u201c.  The parameters for it appear to cover the bare essentials for what would be required.</p> <p></p> <p>When viewing the script that makes up the Action, we can see it uses a .createUserWithPassword method.  There\u2019s a comparable method called createUserWithDetails that adds parameters for First Name and Last Name.  With this as the core of the Workflow, we can start thinking about how the overall Workflow might look.</p> <p></p> <p>With a flowchart like this, we can see there\u2019s two main pieces that need building.  Generating the random password and adding the password to the password/secret management system.</p>","tags":["vrealize orchestration"]},{"location":"2018/12/11/creating-service-accounts-with-vrealize-orchestrator/#generating-a-random-password","title":"Generating A Random Password","text":"<p>Since vRealize Orchestrator uses JavaScript for its internal scripting, an easy approach for this piece could be to take an existing JavaScript password generator and adapt it.  I\u2019ve done this and wrapped it up as an Action, with passLength as an input. <pre><code>// Generates a random password of a specified length and returns it\n// Heavily inspired by the password generator at Sololearn.com by Marcel Feenstra (https://code.sololearn.com/WMLm7mRuVroc)\n\n// charSet is the characters to be used in generating the password.  If these characters aren't suitable for your needs, change it\nvar charSet = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\";\nvar passWord = \"\";\nvar charPos = \"\";\n\nfor (i = 0; i &lt; passLength; i++) {\n    charPos = Math.floor(Math.random() * charSet.length);\n    var passChar = charSet.charAt(charPos);\n    passWord += passChar;\n}\n\nreturn passWord;\n// END\n</code></pre></p>","tags":["vrealize orchestration"]},{"location":"2018/12/11/creating-service-accounts-with-vrealize-orchestrator/#adding-to-passwordsecrets-management-system","title":"Adding to Password/Secrets Management System","text":"<p>A lot of password management systems have API support these days and this allows the Workflow to pass the generated password and other details over to the management system to store.  In the case of my sample workflow, I\u2019m sending it to Password Manager Pro as it\u2019s a system I\u2019ve used in the past.  The only real difference between one system to the next will most likely be the exact format you need to send the data in the API call.</p>","tags":["vrealize orchestration"]},{"location":"2018/12/11/creating-service-accounts-with-vrealize-orchestrator/#adding-some-safety-features","title":"Adding Some Safety Features","text":"<p>One of the issues that comes into play when publishing a workflow like this is it will be consumed by a broader audience than the group of people who were performing this task manually previously (this is really part of the original reason for making this workflow in the first place).  A consequence of this is the need for \u201csafety\u201d \u2013 putting safeguards in place to ensure the workflow doesn\u2019t break anything.  One such safeguard we might put in is to check that the service account specified doesn\u2019t already exist.</p> <p>To achieve this, we might consider a simple bit of code that returns a true/false on the search and act accordingly.  Fortunately, vRO already has the core bit of code for this via the Action \u201cgetUserFromContainer\u201c.  The Action will either return the user as an AD:User object or a null.  Assuming all your service accounts are living in the same Organisational Unit structure, this Action is a good option for performing this check.  An example of how this could behave if implemented is shown below.</p> <p></p> <p>Another safety check to include may relate to the service account name itself.  Your organisation may have a particular format for the account names (using a prefix like svc is common).  This could either be performed as a regular expression check on the workflow\u2019s presentation elements or as part of the workflow.</p>","tags":["vrealize orchestration"]},{"location":"2018/12/11/creating-service-accounts-with-vrealize-orchestrator/#bringing-it-all-together-user-experience","title":"Bringing It All Together &amp; User Experience","text":"<p>With all these building blocks, we can now put them together into a Workflow.  The example Workflow I created looks like this:</p> <p></p> <p>When executed, the requester is prompted for the username and a display name.  Below is the user experience in the vRealize Automation (vRA) Catalog.</p> <p></p> <p>Directly exposing a vRO workflow as a vRA Catalog item doesn\u2019t always carry across Presentation behaviours exactly.  For example, the user name checking I created and associated in the vRO Workflow doesn\u2019t work in \u201creal time\u201d with the vRA Catalog Item.  However, it will execute when the request is submitted and behave correctly (it will block the request if the user exists) as shown below.</p> <p></p> <p>The end result of the Service Account object in Active Directory and the entry in Password Manager Pro can be seen below:</p> <p></p> <p>By using this sort of approach, it\u2019s possible to delegate the ability to perform a task like creating Service Accounts, while also maintaining some degree of control and create an audit trail of the requests.</p>","tags":["vrealize orchestration"]},{"location":"2019/01/02/vrealize-orchestrator--powershell-hosts/","title":"vRealize Orchestrator \u2013 PowerShell Hosts","text":"<p>PowerShell Hosts are one of the types of endpoint available in vRealize Orchestrator\u2019s Inventory. By having a PowerShell Host, you can leverage the breadth of PowerShell functionality from within your vRealize Orchestrator workflows. In this article, I\u2019ll run through adding a PowerShell Host as well as some considerations from a technical and security point of view.</p> <p>Adding A PowerShell Host vRealize Orchestrator has a built-in Workflow for adding a Host under Library &gt; PowerShell &gt; Configuration. Run the \u201cAdd a PowerShell host\u201d Workflow to start it. The opening interface is below:</p> <p></p> <p>Before getting int running the Workflow itself, there are some items you may need to organise in relation to the Host. These items include:</p> <ul> <li>On the PowerShell Host, WinRM needs to be configured and enabled</li> <li>The path between the vRealize Orchestrator appliance and the PowerShell Host and relevant ports must be clear. You can use either HTTP on port 5985 or HTTP on port 5986</li> <li>A username and password with appropriate rights on the target system to execute the PowerShell commands</li> <li>Which form of Authentication you want to use \u2013 Basic or Kerberos. Which you pick may be influenced by your environment and thus, influences how WinRM has to be configured. It should be noted that using Kerberos will require some additional configuration of the krb5.conf file on the vRealize Orchestrator appliance to support this.</li> </ul> <p>Once these items are completed, you should be able to execute the Workflow to completion without any issues. The first screen, as shown above doesn\u2019t require too much in terms of information \u2013 just a name, the Host\u2019s hostname or IP and the port you\u2019ll be using. On the next page of the Workflow, called Host Type, you\u2019ll be presented with some options that may need changing depending on what you defined in the points I mentioned above. In my case, I want to use a relatively secure setup, so I\u2019ve set the Workflow to use HTTPS as the protocol and Kerberos authentication.</p> <p></p> <p>The third page of the Workflow is about the User Credentials. It\u2019s worth noting that if you select Kerberos Authentication, the Username has to be entered in UPN format (user@domain.com), not in the older style (DOMAIN\\user). The last page, Advanced Options, has just one option that can be set. This option is the Shell Code Page. Once you\u2019re happy with everything, you can hit Submit.</p> <p>Once the PowerShell host has been added, you can view it in the Inventory. One nice feature is it will enumerate the SnapIns available, and then the commands available.</p> <p></p>","tags":["vrealize orchestration","powershell"]},{"location":"2019/01/02/vrealize-orchestrator--powershell-hosts/#security-considerations","title":"Security Considerations","text":"<p>When setting up a PowerShelll Host for production use, or if to just perform a \u201cbetter\u201d setup in a lab environment, some security considerations come into play. One of the first is that of allowed Logon Type. The Logon Type used for the PowerShell Host activities are type 3 or Network. Some organisations block this kind of Logon Type for accounts that are local accounts.</p> <p>The next consideration is the level of access on the PowerShell Host itself. When I was adding the Host in vRealize Orchestrator, it would continually fail until I added the account I was using as a local administrator. This is an undesirable outcome especially when there appears to be security groups that should grant the required access. It appears that the account used has to be in the Local Administrators group when adding the host and during subsequent script executions.</p> <p>While PowerShell does have the ability to limit what cmdlets can be run, this is implemented based on a profile specified during a PowerShell Remoting session (based on the information I had at hand) and can\u2019t be enforced on a particular user. So based on this, it seems this sort of enforcement can be done.</p> <p>As a tangent from Security, one thing that should be considered is turning on PowerShell transcript logging. Doing this creates log files whenever a PowerShell script is run and can supply useful information about that particular transaction. An example of a transcript is below: <pre><code>**********************\nWindows PowerShell transcript start\nStart time: 20181218211509\nUsername: BOYCE\\svc_vROPOSH\nRunAs User: BOYCE\\svc_vROPOSH\nMachine: BOYCESVR16 (Microsoft Windows NT 6.3.9600.0)\nHost Application: powershell -File -\nProcess ID: 5312\nPSVersion: 5.1.14409.1018\nPSEdition: Desktop\nPSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.14409.1018\nBuildVersion: 10.0.14409.1018\nCLRVersion: 4.0.30319.36470\nWSManStackVersion: 3.0\nPSRemotingProtocolVersion: 2.3\nSerializationVersion: 1.1.0.1\n**********************\n</code></pre></p>","tags":["vrealize orchestration","powershell"]},{"location":"2019/01/02/vrealize-orchestrator--powershell-hosts/#final-thoughts","title":"Final Thoughts","text":"<p>From what you\u2019ve seen, it\u2019s relatively easy to add a PowerShell Host for vRealize Orchestrator to use. Once it\u2019s in place, it can be leveraged to run PowerShell scripts for a wide range of activities, with inputs and outputs feeding into other workflow items.</p>","tags":["vrealize orchestration","powershell"]},{"location":"2019/03/22/building-nutanix-ahv-templates-with-packer/","title":"Building Nutanix AHV Templates with Packer","text":"<p>Packer is a tool that many IT Infrastructure professions would be familiar with. Packer allows the creation of \u201cmachine images\u201d (or base templates) in a way that\u2019s consistent and highly repeatable. The result is machine images that can be used on a variety of platforms such as cloud providers like AWS or Azure or on-prem infrastructure like VMware, all configured to your organisation\u2019s needs.</p> <p>Nutanix has its own Virtual Machine format called AHV, which runs on Nutanix\u2019s hypervisor Acropolis. Since this hypervisor is based off the KVM hypervisor, and Packer has support for KVM, Packer can be used to build templates for a Nutanix target platform. This post will detail the process I went through to create a Windows 2016 template for Nutanix.</p>","tags":["packer","nutanix"]},{"location":"2019/03/22/building-nutanix-ahv-templates-with-packer/#creating-the-builder","title":"Creating The Builder","text":"<p>Packer has the concept of a Builder \u2013 the platform where your template is created. One of these Builders is QEMU (Quck Emulator), which open source emulator or virtualisation platform. QEMU can also leverage so-called \u201caccelerators\u201d like KVM. With this information in mind, you can look at creating a Builder platform. In my case, I used a Linux-based VM running on VMware ESXi.</p> <p>When creating the Virtual Machine, ensure that Hardware Virtualisation is enabled in the VM\u2019s settings (as shown below). This ensures that virtualisation features are available to QEMU/KVM.</p> <p></p> <p>Provision the Virtual Machine with a Linux operating system. I originally used CentOS as this is the distribution I\u2019m most comfortable with, but there\u2019s a bug that prevents the use of SCSI-based controllers. So I ended up using Debian. When the OS has been installed, confirm that the virtualisation features are available with the following command:</p> <p><code>lscpu | grep Virtualization</code></p> <p>At this point, you can install QEMU and Packer: <pre><code># Do an update\napt update\n\n# Install QEMU/KVM\napt install qemu qemu-kvm\n\n# Install wget\napt install wget\n\n# Download Packer\nwget https://releases.hashicorp.com/packer/1.3.5/packer_1.3.5_linux_amd64.zip\n\n# Unzip packer\nunzip packer_1.3.5_linux_amd64.zip\n</code></pre></p> <p>If you plan to build the templates with headerless mode set to false (which is helpful for troubleshooting), then you\u2019ll need a GUI of some sort. I installed GNOME to handle this: <pre><code># Install GNOME Desktop\napt groups install \"GNOME Desktop\"\n</code></pre></p> <p>At this point you can move onto transferring the artifacts required to perform the build</p>","tags":["packer","nutanix"]},{"location":"2019/03/22/building-nutanix-ahv-templates-with-packer/#building-a-template","title":"Building A Template","text":"<p>Transfer the artifacts to build your template to the Builder VM, such as installation media, the packer json file, scripts, etc. Once everything is staged, you can run packer:</p> <p><code>./packer build qemu-centOS-7-base.json</code></p> <p>If successful, you should see some output similar to what\u2019s shown below:</p> <p></p> <p>To build a Windows-based template, so additional work is required. The default SCSI controller that Nutanix uses is the \u201cVirtIO SCSI pass-through controller\u201d, which like VMware\u2019s Paravirtualised SCSI controller, doesn\u2019t have native driver support on Windows. The same is true for the network card (VirtIO Ethernet Adapter). Without additional work to support these, the OS installation grinds to a halt very early on, as the installer can\u2019t find storage to install onto.</p> <p>To provide the drivers required, I downloaded the virtIO drivers from the Fedora Project site and added them to the artifacts. These files were then added to the list of those presented to the virtual machine being built and thus are automatically picked up by Windows. <pre><code>\"floppy_files\": [\n        \"answer_files/2016/autounattend.xml\",\n        \"scripts/enable-winrm.ps1\",\n        \"drivers/NetKVM/2k16/amd64/*.cat\",\n        \"drivers/NetKVM/2k16/amd64/*.inf\",\n        \"drivers/NetKVM/2k16/amd64/*.sys\",\n        \"drivers/vioscsi/2k16/amd64/*.cat\",\n        \"drivers/vioscsi/2k16/amd64/*.inf\",\n        \"drivers/vioscsi/2k16/amd64/*.sys\",\n        \"drivers/viostor/2k16/amd64/*.cat\",\n        \"drivers/viostor/2k16/amd64/*.inf\",\n        \"drivers/viostor/2k16/amd64/*.sys\"\n      ]\n</code></pre></p> <p>In the example code above, I\u2019ve included the drivers for the network card and SCSI controllers. The only other specific setting I made was to set the disk controller to virtio-scsi. There were also some settings specific to building a Windows-based template.</p>","tags":["packer","nutanix"]},{"location":"2019/03/22/building-nutanix-ahv-templates-with-packer/#presenting-the-template","title":"Presenting The Template","text":"<p>Once the template has successfully built, its time to get the output artifact into Nutanix. Nutanix has an \u201cImage Service\u201d where you can upload ISOs and templates for use. The process to add the template is quite simple. Login to Prism, go to Virtual Infrastructure &gt; Images. Click on the Add Image button and fill in the details. An example of adding a Windows 2016 image is show below:</p> <p></p> <p>Once the upload is finished, the image will appear in the list of those available. At this point, you can create a virtual machine based off this new image. To do so, navigate to Virtual Infrastructure &gt; VMs. Click on the Create VM button. When you go to add a disk, you can specify the image by changing the Operation from \u201cAllocate on Storage Container\u201d to \u201cClone from Image Service\u201d. The UI will change slightly to allow you to select the image you want to use. An example of how this might look is shown below:</p> <p></p> <p>Finish configuring the VM as you normally would. Once the Virtual Machine has been created, you can power it on and observe the bootup process. It should boot up in a fully functional state due to the additional drivers added during the packer build.</p>","tags":["packer","nutanix"]},{"location":"2019/03/22/building-nutanix-ahv-templates-with-packer/#extending-past-mvp","title":"Extending Past MVP","text":"<p>At this point, the process is really a Minimum Viable Product (MVP). Ideally, a more refined product would be part of an automated framework. At the moment, the workflow looks like this:</p> <p></p> <p>Under this workflow, the initiation is manual and the output artifact still needs to be transferred to the Nutanix infrastructure. A more refined form may look like this:</p> <p></p> <p>This sort of workflow is very similar to what\u2019s been used in past workplaces and could be extended further to include testing and other steps.</p>","tags":["packer","nutanix"]},{"location":"2019/03/22/building-nutanix-ahv-templates-with-packer/#lessons-learned","title":"Lessons Learned","text":"<p>During the process of trying to create templates for Nutanix, I did run into a few issues. Some of these included:</p> <ul> <li>I initially tried using VirtualBox but it appeared that the Virtualisation features weren\u2019t fully exposed</li> <li>There was a note in the packer documentation about use of the scsi disk interface with Red Hat-based distributions. I ignored that error since it was working at the start and switched to the virtio-scsi interface later on (which apparently doesn\u2019t work)</li> <li>The issue above wasn\u2019t surfaced in packer\u2019s default output. This led to me spending time trying to figure out what happened</li> <li>Due to the scsi disk controller issue, I switched to using Debian and used the stable release. The version of QEMU associated with this release was slightly older and also caused issues. I ended up switching to Debian unstable</li> <li>I haven\u2019t fully confirmed this, but I think I got errors building Windows VMs via a terminal, even with headerless mode turned on.</li> </ul>","tags":["packer","nutanix"]},{"location":"2019/03/22/building-nutanix-ahv-templates-with-packer/#acknowledgements-resources","title":"Acknowledgements &amp; Resources","text":"<p>When I first started working on this piece of work, I had trouble finding any articles or writings that others had done on it. The one I did find was \u201cUsing Packer to Build Images for the Acropolis Hypervisor\u201d by Andrew Nelson. This particular article was a major help in me getting this piece of work started and finished. Resources that others might find of use if they\u2019re looking at doing a similar piece of work:</p> <ul> <li>Using Packer to Build Images for the Acropolis Hypervisor</li> <li>Andrew Nelson\u2019s Acropolis Packer Examples Github Repo</li> <li>Packer QEMU Build documentation</li> <li>Creating Windows virtual machines using virtIO drivers</li> <li>CentOS 7 Packer QEMU Template</li> <li>Windows 2016 Packer QEMU Template</li> </ul>","tags":["packer","nutanix"]},{"location":"2019/03/28/managing-f5-load-balancers-with-vrealize-orchestrator/","title":"Managing F5 Load Balancers with vRealize Orchestrator","text":"<p>F5 Load Balancers (LB) have been a common feature across a number of environments I\u2019ve worked at. While administration of these devices is generally performed via the web interface, F5s also have a REST API that allows the same management tasks to be performed. This opens the possibility of using VMware\u2019s vRealize Orchestrator (vRO) to manage F5 Load Balancers via the same REST API.</p> <p>Getting Started The first thing to undertake is getting the pieces required to use the API in place. The first step in this would be to create an account on the F5 appliance that has appropriate rights for the tasks to be performed. The specifics of this will vary from environment to environment (such as partition configuration), but to manage pools and pool members, a combination of the Application Editor and Manager role permissions are required. In the screenshot below, I\u2019m adding a user called vrorest with the Manager role in the lb-prod partition.</p> <p></p> <p>After this, a REST Host object needs to be created in vRealize Orchestrator. This can be done using the \u201cAdd a REST Host\u201d workflow.</p> <p></p> <p>For Host Authentication, set the Type to Basic and then enter the details of the user account and complete the rest of the wizard. If all goes well, a new item will appear in the Inventory.</p> <p></p> <p>I prefer to reference REST Operations in vRO, so next I would add some common operations to this host. These include getting load balancer pools and pool members, as well as setting pool member details. The table below has some of these.</p> Name Method URL Template Notes Get Pool GET /mgmt/tm/ltm/pool/{poolName} Can be used to return all pools or a specific pool Get Node GET /mgmt/tm/ltm/node/{nodeName} Can be used to return all nodes or a specific node Get Pool Members GET /mgmt/tm/ltm/pool/~{partitionName}~{poolName}/members Set Pool Member PUT /mgmt/tm/ltm/pool/~{partitionName}~{poolName}/members/~{partitionName}~{memberName}:{memberPort} <p>Most of these \u201cGet\u201d items can be used to populate drop-downs and other user interface elements in Workflows via the use of Actions.</p>","tags":["vrealize orchestration"]},{"location":"2019/03/28/managing-f5-load-balancers-with-vrealize-orchestrator/#example-action-get-pool-names","title":"Example Action \u2013 Get Pool Names","text":"<p>Actions can be used to bind values to a user interface element. In this example, I want to be able to get just the names of the F5 pools so that a drop-down can be populated. To do this, I would use some code like that below:</p> <p><pre><code>/*\nThis script assumes an Input called restOperation of the type REST:RESTOperation and has a return type of Array/String \n*/\n\n// Init variables\nvar inputs = [\"\"];\nvar content = \"\";\nvar poolList = new Array;\n\n// Create request\nvar request = restOperation.createRequest(inputs,content);\n\n// Get response\nvar response = request.execute();\n\n// Parse JSON\nvar jsonObject = JSON.parse(response.contentAsString);\nvar jsonItems = jsonObject.items\n\n// Iterate - extracting the name value and pushing it into the poolList array\nfor each (var x in jsonItems) {\n    System.log(x.name);\n    poolList.push(x.name);\n}\n\n// return populated poolList array\nreturn poolList;\n</code></pre> To attach the outputted list of Pool Names to an input on a Workflow, I added the Property of \u201cPredefined list of elements\u201d and called the Action, as shown below:</p> <p></p> <p>When the Workflow is executed, the Action is called to populate the values, as shown in the image below.</p> <p></p>","tags":["vrealize orchestration"]},{"location":"2019/03/28/managing-f5-load-balancers-with-vrealize-orchestrator/#example-workflow-set-pool-member-state","title":"Example Workflow \u2013 Set Pool Member State","text":"<p>The code below is a snippet of a workflow that sets the state of a Pool Member based on inputs from the Workflow form. The first part of the snippet constructs the payload required to set the state based on what the user selected. Following that, the appropriate REST Operation is executed and the response returned.</p> <pre><code>/*\nThis snippet expects the following inputs:\n- memberName (string)\n- memberPort (number)\n- poolName (string)\n- restOperation (REST:RESTOperation)\n- state (string)\nrestOperation will be a RESTOperation object that uses the URL for setting a Pool Member's state\n*/\n\n// Construct payload based on the state value\nswitch(state) {\n    case \"enable\":\n        //\n        payload = '{\"state\": \"user-up\", \"session\": \"user-enabled\"}';\n        break;\n    case \"disable\":\n        //\n        payload = '{\"state\": \"user-up\", \"session\": \"user-disabled\"}';\n        break;\n    case \"forceoffline\":\n        //\n        payload = '{\"state\": \"user-down\", \"session\": \"user-disabled\"}';\n        break;\n    default:\n        // throw an error\n        throw \"An invalid value (\" + state + \") was submitted for the input parameter state\";\n    }\nSystem.log(\"Payload value has been set to: \" + payload);\n\n// Set inputs for the REST call\nvar inputs = [poolName,memberName,memberPort];\n\n// Create request\nvar request = restOperation.createRequest(inputs,payload);\n\n// Execute request\nvar response = request.execute();\n\n// Parse and output response\nvar jsonObject = JSON.parse(response.contentAsString);\nSystem.log(response.contentAsString);\n</code></pre>","tags":["vrealize orchestration"]},{"location":"2019/03/28/managing-f5-load-balancers-with-vrealize-orchestrator/#further-reading-resources","title":"Further Reading &amp; Resources","text":"<p>The two resources below were my starting point for doing these workflows. The material on the F5 Automation Labs has a bunch of Postman collections that cover a range of operations on F5 devices, allowing you to experiment with them.</p> <ul> <li>F5 Automation Labs \u2013 https://github.com/f5devcentral/f5-automation-labs</li> <li>iControl REST User Guide \u2013 https://devcentral.f5.com/d/icontrol-rest-user-guide</li> </ul>","tags":["vrealize orchestration"]},{"location":"2019/04/17/vrealize-automation--dealing-with-a-disconnectedorphaned-iaas-server/","title":"vRealize Automation \u2013 Dealing with a disconnected/orphaned IAAS Server","text":"<p>Following the installation of a replacement UPS I made a horrible discovery \u2013 there was problems with my vRealize Automation setup. The first sign of what was to come was when I reviewed the console of the vRealize Automation (vRA) Virtual Applications.</p> <p> Anyone who has had to setup vRealize Automation would probably know the sinking feeling I started feeling. Having to setup vRA again from scratch was not something I really wanted to do\u2026.</p>","tags":["vrealize automation"]},{"location":"2019/04/17/vrealize-automation--dealing-with-a-disconnectedorphaned-iaas-server/#start-the-troubleshooting","title":"Start the troubleshooting","text":"<p>I started having a look at what might be wrong. The Virtual Appliance administrative interface pointed out a very glaring issue \u2013 my IAAS server hadn\u2019t been connected for 84 days.</p> <p></p> <p>Given that I\u2019ve only been working in Orchestrator on my home lab lately, it seems easy to have missed this kind of thing happening. When viewing the services on the IAAS server, some of the vRA related ones were started. As I tried to start them, one of them started to throw up an error.</p> <p>The \u201cVMware vCloud Automation Center Service\u201d service wouldn\u2019t start, complaining about an issue with its side-by-side configuration (error code 14001). This is an issue that seems somewhat common where the Config file for the service gets chewed up for one reason or another. There is even a VMware Knowledge Base article (54736) that references this issue and how to fix it.</p> <p>After applying the steps in the article, the next issue appeared, where the service would stop with a very generic error. The log files for the service itself didn\u2019t seem to have anyhting. Fortunately the Windows Event Log did have something useful.</p> <pre><code>Log Name:      Application \nSource:        ManagementAgentService \nDate:          6/04/2019 6:51:22 PM \nEvent ID:      0 \nTask Category: None \nLevel:         Error \nKeywords:      Classic \nUser:          N/A \nComputer:      vraiaas.boyce.local \nDescription: Service cannot be started. System.Configuration.ConfigurationErrorsException: Required attribute 'id' not found. (C:\\Program Files (x86)\\VMware\\vCAC\\Management Agent\\VMware.IaaS.Management.Agent.exe.Config line 6)\nOn this error, I found another VMware Knowledge Base article \u2013 IaaS Management Agent does not start in vRA 7.1 ([2146550](https://kb.vmware.com/articleview?docid=2146550)). While doing the steps in the first article did create a fresh Config file for the service, it appears it was missing some key settings, such as the id attribute. I added this and tried again.\n</code></pre> <p>Again the service failed to start, but the error message had changed slightly, now complaining about a missing \u201cmanagementEndpoints\u201d attriubte. In a VMware forum thread where someone was experiencing the id attribute issue, they had helpfully posted their Config file which included the managementEndpoints section. So I attempted to add the same block into my own Config file. I took the certificate thumbprint from the vRA web interface, hoping it would be the correct one. After some formatting silliness, the service finally started.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;configuration&gt;\n    &lt;configSections&gt;\n        &lt;section name=\"agentConfiguration\" type=\"VMware.IaaS.Management.Agent.Configuration.ManagementAgentConfigurationSection, VMware.IaaS.Management.Agent, Version=7.5.0.0, Culture=neutral, PublicKeyToken=b6e51ea3eb1deebc\" allowLocation=\"true\" allowDefinition=\"Everywhere\" allowExeDefinition=\"MachineToApplication\" overrideModeDefault=\"Allow\" restartOnExternalChanges=\"true\" requirePermission=\"true\" /&gt;\n    &lt;/configSections&gt;\n    &lt;agentConfiguration machineUuid=\"&lt;MACHINE UUID&gt;\" id=\"&lt;ID FROM KB 2146550 COMMAND&gt;\"\n        pollingInterval=\"00:00:05\"&gt;\n        &lt;managementEndpoints&gt;\n            &lt;endpoint address=\"https://vravra.boyce.local:5480/\" thumbprint=\"\u200e&lt;CERTICATE THUMBPRINT&gt;\"/&gt;\n        &lt;/managementEndpoints&gt;\n&lt;/agentConfiguration&gt;\n&lt;/configuration&gt;\n</code></pre> <p>The last issue was logging \u2013 the Config file was missing the settings for logging, causing it to spray errors into the Windows event log. There\u2019s two items that cause this. It seems that again, when the Config file was recreated, it was missing the pieces needed to enable logging \u2013 firstly a block in  defining a section for the logging configuration and the actual logging configuration settings. <p>Now the service was starting and logging to log files correctly. Unfortunately, those logs were full of errors.</p> <pre><code>System.Runtime.Remoting.RemotingException: Soap Parse error, xsd:type 'xsd:hexBinary' invalid \u200e2221495FF9177CAB46D1209F5D2BE2B4D5E8AC3C\n</code></pre> <p>These errors seemed to happy whenever the service was trying to execute a number of routine tasks. Some of these involve accessing some REST API endpoints on the main Appliance. I was able to manually verify these were responding using Postman. At this point I was stuck for further options, so I exported my vRO workflows and decided to simply rebuild my vRA/vRO Setup</p>","tags":["vrealize automation"]},{"location":"2019/04/17/vrealize-automation--dealing-with-a-disconnectedorphaned-iaas-server/#rebuilding-and-new-problems","title":"Rebuilding and New Problems","text":"<p>I spun up a new Windows 2012 R2 Virtual Machine to act as my IAAS server and installed all the prerequisite items. Everything seemed to be going ok until I got to the prerequisite checker in the vRealize Automation\u2019s Virtual Appliance setup, where it refused to validate that Java was there correctly. As a note, I had installed the latest Java at the time, which was Java Version 8 Update 211. After a bit of digging around, I found a post on Reddit where someone experienced issues with VMware\u2019s Lifecycle Manager refusing to upgrade their vRA appliance when update 201 of Java was installed. When they dropped it down to 191, it worked. Using this premise, I managed to get 191 installed using Chocolatey and the prerequisite check passed.</p> <p>I did a bit of digging around on the vRA appliance after all this to try to figure out what exactly is done to perform this check and why it might fail. Unfortunately some of these things seem to be obscured by VMware and the best I could do is find some PowerShell snippets in a DLL. Based on what those snippets do, the script should execute without errors even with an update 200 or higher version of Java. Interestingly, the same script snippet includes code that attempts to remediate by installing Java (presumably using source files from the vRA appliance itself)</p> <p>A final issue I ran into after rebuilding was the same \u201cDEPLOYMENT HAS FAILED\u201d error appearing again, even though everything seemed to go okay and the system appeared to be working correctly. Upon reviewing the log file referenced in the error, I found an entry about ElasticSearch failing to start, although when I manually checked its status, it was working. After a bit of digging around, I found a KB article that seems to cover this scenario \u2013 Initial OVF deployment of vRA 7.5 fails with error: Deployment failed, you will need to redeploy (59333) There was also a forum thread that suggested this issue will be resolved in 7.6</p>","tags":["vrealize automation"]},{"location":"2019/04/17/vrealize-automation--dealing-with-a-disconnectedorphaned-iaas-server/#lessons-learned","title":"Lessons Learned","text":"<p>Out of all these issues come a few things that I\u2019ll probably look at implementing in my lab setup (and would almost certain implement in any production instance I manager in future):</p> <ul> <li>Services on the IAAS System \u2013 Since the operation of these is key and them being in a failed fashion is a good sign that things are broken, I\u2019ll be looking at adding monitoring items for them in my monitoring system</li> <li>VMware.IaaS.Management.Agent.exe.Config File \u2013 This is at least the second time I\u2019ve seen this file get mangled for some reason. So at a minimum, I would look at making regular backups of it and potentially have more scrutiny of it like checking file size/content</li> <li>vRA Node Status \u2013 The vRA Node Status display in the vRA Appliance is a good avenue for seeing if there\u2019s been any drift between the main appliance and the IAAS nodes. It may be possible to leverage the Health Check feature via REST API for this</li> <li>IAAS Event Logs \u2013 There\u2019s a number of items that the IAAS server logs in the Event Log and this can be clues to the positive or negative state of the system</li> <li>Java Versions \u2013 As shown in my experience and that of the person trying to do the vRA Upgrade via Lifecycle Manager, there seems to be an issue with how the version of Java is detected if its over update 200. As a point of reference, there appears to be an update 181 zip file of Java on the vRA appliance itself, so it might be worth while grabbing that and putting it somewhere safe</li> </ul>","tags":["vrealize automation"]},{"location":"2019/04/26/vmware-vrealize-automation-76--whats-new/","title":"VMware vRealize Automation 7.6 \u2013 What\u2019s New","text":"<p>VMware released an update for vRealize Automation (vRA) at the start of this month. This was a slight increment from 7.5 to 7.6. At a high level, two key areas of change are NSX integration and the vRealize Orchestrator (vRO) user experience. The Release Notes goes into a bit more detail and I\u2019ll be using those more detailed items as a guide for the content in this post. I will be skipping over the new NSX pieces as I don\u2019t have those available to me.</p>","tags":["vrealize automation"]},{"location":"2019/04/26/vmware-vrealize-automation-76--whats-new/#virtual-appliance-changes","title":"Virtual Appliance Changes","text":"<p>The first change that you will most likely see is the new Summary page. The page has a high level view of vRealize Automation\u2019s status, plus a graph of memory consumption and CPU load.</p> <p></p> <p>Clicking in the From and To boxes allows you to set the time range for the graph. The Virtual Appliance also now has a SNMP service, which can be leveraged to perform monitoring. At the time of writing, I haven\u2019t been able to find any documentation on enabling it or how to use it.</p>","tags":["vrealize automation"]},{"location":"2019/04/26/vmware-vrealize-automation-76--whats-new/#custom-forms-enhancements","title":"Custom Forms Enhancements","text":"<p>Custom Forms were VMware\u2019s attempt to bring a richer user experience to Blueprints. In vRA 7.6, some new UI elements have been added \u2013 Dual List, Multi Value Picker and Link. The Dual List element is one that a lot of people will likely get value from.</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/04/26/vmware-vrealize-automation-76--whats-new/#vrealize-orchestrator","title":"vRealize Orchestrator","text":"<p>Historically, people developing on vRealize Orchestrator have used a Java-based client. This presented a range of issues. In 7.5, there was a hint of what was to come with VMware starting to apply their Clarity UI to parts of vRA. In 7.6, we\u2019re seeing this extend further into Orchestrator itself. One nice addition is a System dashboard that presents a lot of useful metric data.</p> <p></p> <p>Perhaps the biggest real change is the ability to now edit Workflows and Actions directly within this web interface. This removes the need to use the Java client. The view when editing a Workflow is very similar to that of the Java client.</p> <p></p> <p>The Input Form tab takes the role of the Presentation tab that was present in the Java client. It has a look and feel that\u2019s very similar to the Custom Forms in vRA with the same UI Elements available. It is interesting to note that the Release Notes mention that input parameter constraints created via this new UI won\u2019t automatically transfer through to vRA when published as an XAAS blueprint. This was a behaviour that previously happened with the Java client (and still continues to work for workflows created/edited in the Java client).</p> <p>The Version History tab presents a \u201cdiff\u201d interface very similar to Github and similar systems, where you can see what code changes happened between versions. Lastly, the Audit tab gives a bit more governance oversight into the things happening to Orchestrator items, with a history of edits, executions and other activities. I\u2019ll have to spend a bit of time digging more into the new web UI for Orchestrator, but so far it seems at feature parity with the Java client and is a great move forward. Lastly, when editing a script, we now benefit from autocomplete.</p>","tags":["vrealize automation"]},{"location":"2019/04/26/vmware-vrealize-automation-76--whats-new/#api-changes","title":"API Changes","text":"<p>There doesn\u2019t seem to be a lot of change in this area. In the 7.6 API list, there is an item to interact with Orchestrator, however I can\u2019t remember if this is simply a documentation change or whether that API has been added.</p>","tags":["vrealize automation"]},{"location":"2019/04/26/vmware-vrealize-automation-76--whats-new/#summing-up","title":"Summing Up","text":"<p>On the vRA side of things, there\u2019s been a few tweaks. On the Orchestrator side, it\u2019s a radical change with the new HTML5 user interface, one that I\u2019m sure a lot of vRO developers should be pleased with.</p>","tags":["vrealize automation"]},{"location":"2019/05/26/vcenter-67-update-2--code-capture/","title":"vCenter 6.7 Update 2 \u2013 Code Capture","text":"<p>One of the very cool new features that came with the latest vCenter update is Code Capture. This feature allows you to \u201crecord\u201d actions in the HTML 5 web client. When the \u201crecording\u201d is ended, Code Capture will generate PowerCLI.NET code. For those who used Exchange 2007, you may remember a similar feature in the GUI management console. At the end of each wizard, there was a summary of PowerShell code that would perform the same task you just finished.</p> <p>Code Capture has been around for a while as part of the HTML 5 Client Web Fling. However, at the time, it was not appropriate for production use. With the feature being included in 6.7 Update 2, it now is appropriate for use.</p>","tags":["vcenter"]},{"location":"2019/05/26/vcenter-67-update-2--code-capture/#getting-started","title":"Getting Started","text":"<p>The Code Capture feature is under the new Developer Center menu option that\u2019s presented on the left-side menu on the Home screen.</p> <p></p> <p>In the Developer Center is three tabs: Overview, API Explore and Code Capture. In the Code Capture tab is a simple on/off switch and a little info bubble</p> <p></p> <p>Once you turn on Code capture, some new UI elements will appear under the on/off switch to do things like start recording. A record button also appears on the top menu bar between the Help icon and the logged in user\u2019s name.</p> <p></p> <p>The placement of this button means you can easily start and stop recording activities no matter what part of the vCenter client you are.</p>","tags":["vcenter"]},{"location":"2019/05/26/vcenter-67-update-2--code-capture/#example-1-create-a-vm-from-template","title":"Example 1 \u2013 Create a VM from Template","text":"<p>One of the more common activities performed by VMware administrators is creating a new Virtual Machine. Usually this is performed by cloning off an existing template. To \u201cCode Capture\u201d this actiivty, we would perform it exactly as we could but with the use of the new recording icon to bookend it. When I finish performing a clone like this with Code Capture on, I get a bunch of generated code. In this case, it generates about 1,200 lines of code. Due to its length, I\u2019ve put it into a gist available at https://gist.github.com/jpboyce/fca8aaacf248034510f11fdc3fee117a</p>","tags":["vcenter"]},{"location":"2019/05/26/vcenter-67-update-2--code-capture/#example-2-creating-a-new-user","title":"Example 2 \u2013 Creating a New User","text":"<p>Sometimes we need to add a new user account to vCenter. This is an scenario that shows what Code Capture will not touch. When I created a new user and added the account to a group, Code Capture recorded no useful code. It would seem Code Capture considers this sort of activity part of the \u201csensitive data\u201d mentioned in the info box earlier.</p>","tags":["vcenter"]},{"location":"2019/05/26/vcenter-67-update-2--code-capture/#example-3-editing-the-vm","title":"Example 3 \u2013 Editing the VM","text":"<p>Since manipulating accounts yields no results, I decided to perform a common modification task to the VM created in Example 1. I changed the number of CPUs from 2 to 4. Like the output from Example 1, the captured code is verbose, with a line count of 106. <pre><code>#----------------- Start of code capture -----------------\n\n#---------------EnvironmentBrowser---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.EnvironmentBrowser\n\n#---------------Config---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.Config\n\n#---------------QueryConfigOptionEx---------------\n$spec = New-Object VMware.Vim.EnvironmentBrowserConfigOptionQuerySpec\n$spec.GuestId = New-Object String[] (1)\n$spec.GuestId[0] = 'windows9Server64Guest'\n$_this = Get-View -Id 'EnvironmentBrowser-envbrowser-121'\n$_this.QueryConfigOptionEx($spec)\n\n#---------------DatastoreBrowser---------------\n$_this = Get-View -Id 'EnvironmentBrowser-envbrowser-121'\n$_this.DatastoreBrowser\n\n#---------------QueryTargetCapabilities---------------\n$_this = Get-View -Id 'EnvironmentBrowser-envbrowser-121'\n$_this.QueryTargetCapabilities($null)\n\n#---------------QueryConfigOptionDescriptor---------------\n$_this = Get-View -Id 'EnvironmentBrowser-envbrowser-121'\n$_this.QueryConfigOptionDescriptor()\n\n#---------------ListKmipServers---------------\n$_this = Get-View -Id 'CryptoManagerKmip-CryptoManager'\n$_this.ListKmipServers($null)\n\n#---------------ReconfigVM_Task---------------\n$spec = New-Object VMware.Vim.VirtualMachineConfigSpec\n$spec.CpuAllocation = New-Object VMware.Vim.ResourceAllocationInfo\n$spec.CpuAllocation.Shares = New-Object VMware.Vim.SharesInfo\n$spec.CpuAllocation.Shares.Shares = 4000\n$spec.CpuAllocation.Shares.Level = 'normal'\n$spec.NumCPUs = 4\n$spec.DeviceChange = New-Object VMware.Vim.VirtualDeviceConfigSpec[] (0)\n$spec.CpuFeatureMask = New-Object VMware.Vim.VirtualMachineCpuIdInfoSpec[] (0)\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.ReconfigVM_Task($spec)\n\n#---------------EnvironmentBrowser---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.EnvironmentBrowser\n\n#---------------EnvironmentBrowser---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.EnvironmentBrowser\n\n#---------------Config---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.Config\n\n#---------------QueryConfigOptionEx---------------\n$spec = New-Object VMware.Vim.EnvironmentBrowserConfigOptionQuerySpec\n$spec.GuestId = New-Object String[] (1)\n$spec.GuestId[0] = 'windows9Server64Guest'\n$_this = Get-View -Id 'EnvironmentBrowser-envbrowser-121'\n$_this.QueryConfigOptionEx($spec)\n\n#---------------DatastoreBrowser---------------\n$_this = Get-View -Id 'EnvironmentBrowser-envbrowser-121'\n$_this.DatastoreBrowser\n\n#---------------QueryTargetCapabilities---------------\n$_this = Get-View -Id 'EnvironmentBrowser-envbrowser-121'\n$_this.QueryTargetCapabilities($null)\n\n#---------------QueryConfigOptionDescriptor---------------\n$_this = Get-View -Id 'EnvironmentBrowser-envbrowser-121'\n$_this.QueryConfigOptionDescriptor()\n\n#---------------ListKmipServers---------------\n$_this = Get-View -Id 'CryptoManagerKmip-CryptoManager'\n$_this.ListKmipServers($null)\n\n#---------------ReconfigVM_Task---------------\n$spec = New-Object VMware.Vim.VirtualMachineConfigSpec\n$spec.CpuAllocation = New-Object VMware.Vim.ResourceAllocationInfo\n$spec.CpuAllocation.Shares = New-Object VMware.Vim.SharesInfo\n$spec.CpuAllocation.Shares.Shares = 4000\n$spec.CpuAllocation.Shares.Level = 'normal'\n$spec.NumCPUs = 4\n$spec.DeviceChange = New-Object VMware.Vim.VirtualDeviceConfigSpec[] (0)\n$spec.CpuFeatureMask = New-Object VMware.Vim.VirtualMachineCpuIdInfoSpec[] (0)\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.ReconfigVM_Task($spec)\n\n#---------------EnvironmentBrowser---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.EnvironmentBrowser\n\n#---------------Config---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.Config\n\n#---------------Config---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.Config\n\n\n#----------------- End of code capture -----------------\n</code></pre></p>","tags":["vcenter"]},{"location":"2019/05/26/vcenter-67-update-2--code-capture/#example-4-deleting-the-vm","title":"Example 4 \u2013 Deleting the VM","text":"<p>Deleting the VM created in Example one generates some very brief code: <pre><code>#----------------- Start of code capture -----------------\n#---------------Destroy_Task---------------\n$_this = Get-View -Id 'VirtualMachine-vm-121'\n$_this.Destroy_Task()\n#----------------- End of code capture -----------------\n</code></pre></p>","tags":["vcenter"]},{"location":"2019/05/26/vcenter-67-update-2--code-capture/#closing-thoughts","title":"Closing Thoughts","text":"<p>Code Capture is an interest concept. The implementation of it doesn\u2019t exactly match what I had in mind (which was for something very much like the code generation in Exchange 2007). As noted in the examples, the verboseness of the generated code can vary by a large degree. However, the structure of the code is in such a way where the references to Virtual Machines and other objects is abstracted by the use of their IDs, meaning code should be more transferable.</p> <p>It\u2019s also worth noting that the HTML 5 Fling that spawned Code Capture is still being developed, with features being ported over to the main vCenter release. One newer feature of note is the ability to general scripts in other languages, specifically Python and vRealize Orchestator\u2019s JavaScript. Over time I\u2019m sure it\u2019ll gain more features and maturity.</p>","tags":["vcenter"]},{"location":"2019/06/20/thoughts-on-user-experience-and-automation/","title":"Thoughts on User Experience and Automation","text":"<p>The title suggests an odd combination of topics. I believe there still is an aspect of User Experience in IT Automation. With Automation, a task that was previously performed by a person is being done with a script or the like. This is really one half of the scenario, as we are also replacing the interface our customer was using and putting a new one in place. In some cases, this new interface can be worse than what was in place before.</p> <p>I\u2019ve collected a number of observations and thoughts about this User Experience aspect and I\u2019ll go through some of them in this article. The examples I present will be relating to PowerShell and vRealize Orchestrator since those are tools I use frequently. But they could probably apply to any language/technology scenario involving automation.</p>","tags":["automation"]},{"location":"2019/06/20/thoughts-on-user-experience-and-automation/#using-the-appropriate-input-channel","title":"Using The Appropriate Input Channel","text":"<p>This is a \u201csin\u201d I see often with some people when they first get into PowerShell. I suspect this sin occurs because they were thinking in the context of their script being executed interactively by the user. This leads them to use the cmdlet Read-Host to get input from the user. In a way, Read-Host is seductive in its use. It allows you to assign the user\u2019s input to a variable. The prompt the user sees can also be customised.</p> <p></p> <p>The problem with this approach is the script will wait for the user\u2019s input. This makes it unsuitable for scenarios where the script would be programmatically executed. In the case of PowerShell specifically, using Read-Host instead of Parameters seems counter-intuitive to the Pipeline model that PowerShell uses, where objects can be easily passed from one command to another. Lastly, Read-Host has no validation of any sort, so performing validation of the input would require extra code. Which leads into the next item\u2026</p>","tags":["automation"]},{"location":"2019/06/20/thoughts-on-user-experience-and-automation/#using-input-validation","title":"Using Input Validation","text":"<p>Input Validation can help serve two goals. Firstly providing immediate feedback to the consumer of your automation that something bad was entered. Secondly protecting your automation against undesirable inputs. A bad example the first goal: a coworker who was trying to update details on some Virtual Machines, which involved inputting a project code. He had entered a value that the system didn\u2019t like, but only informed him after he had submitted the request. This created a poor user experience because he didn\u2019t get immediate feedback. The result was he had to fill in and submit the form again.</p> <p>In the story about the co-worker the form was the front-end of a workflow in vRealize Orchestrator. Assuming the project code was always going to be a number, this issue could\u2019ve been fixed by setting the data type of the input to \u201cnumber\u201d. A better approach would\u2019ve been to use data binding and bind a list of valid project codes to the input.</p> <p></p> <p>PowerShell has a good range of options for validating input as well, including regular expressions.</p>","tags":["automation"]},{"location":"2019/06/20/thoughts-on-user-experience-and-automation/#expose-complexity-when-required","title":"Expose Complexity When Required","text":"<p>I consider the wizard for creating an EC2 instance in AWS to be a good benchmark for this. It\u2019s possible to only select the \u201cWhat\u201d of the instance (what it does). After this, the consumer can immediately select \u201cReview and Launch\u201d from the instance type screen. If they want more control over the details, then they can take a different path. However, taking the \u201cexpress path\u201d requires very little input or technical knowledge to provision the instance.</p> <p>A bad example of this: a provisioning workflow prompted for about 15 pieces of information, including which data center the server would live in, which storage, what network zone and so on. In this case, an overwhelming amount of complexity was being exposed by default. It required the consumer of this workflow to have intimate knowledge of the organisation\u2019s infrastructure. This immediately restricts the potential scope of consumers for this workflow, and thus reduces the benefits it would realise.</p> <p>A middle ground example was a private cloud system I supported, that used similar technology to the bad example above. Consumers could provision a virtual machine with a few abstract pieces of information (what is the server\u2019s role/job? Is it a dev, test or prod system?). However, under certain conditions, like the server\u2019s home being production, more complexity was exposed. The consumer was prompted for things like backup settings. In this case, more complexity was exposed in a way that was appropriate.</p>","tags":["automation"]},{"location":"2019/06/20/thoughts-on-user-experience-and-automation/#closing-thoughts","title":"Closing Thoughts","text":"<p>There are many more problematic user experience scenarios outside the ones I\u2019ve mentioned. These issues can be negated with some consideration on what is presented to them. This results in a positive experience of those consumers. A positive experience means they\u2019re likely to use it more often and that\u2019s a good outcome.</p>","tags":["automation"]},{"location":"2019/07/21/cumulative-update-for-vrealize-automation-76/","title":"Cumulative Update for vRealize Automation 7.6","text":"<p>UPDATE</p> <p>Since I wrote this post, VMware have taken down this update because it causes issues with multi-tab XAAS forms. Specifically, it appears that each tab of a XAAS form will submit a request. One of these requests is \u201creal\u201d and will process properly, while the others will error out. As of 24th October, the hotfix is still offline. The issue with Chrome not rendering buttons was resolved in Chrome 76.</p> <p>Earlier this month, VMware released the first Cumulative Update for vRealize Automation 7.6. The knowledge base article for it is available at https://kb.vmware.com/s/article/70911 The patch file is about 1.1GB.</p> <p>The first resolved issue in the article is most likely the one that people are experiencing and the easiest to see \u2013 \u201cDeployment forms are missing the Submit, Next and Cancel buttons\u201d when viewed in Chrome 75. Below is a comparison of how the form is rendered in Chrome 75 versus Microsoft Edge.</p> Chrome 75 rendering Microsoft Edge Rendering <p>This issue by itself is a big showstopper if an organisation uses this functionality. I suspect this update will be a \u201cmust have\u201d for them.</p>","tags":["vrealize automation"]},{"location":"2019/07/21/cumulative-update-for-vrealize-automation-76/#update-prerequisites","title":"Update Prerequisites","text":"<p>The prerequisites for this update are standard for this product. The main areas of focus are ensuring there\u2019s enough storage on the Virtual Appliance and IAAS nodes, and that DNS resolution will work correctly.</p>","tags":["vrealize automation"]},{"location":"2019/07/21/cumulative-update-for-vrealize-automation-76/#installation","title":"Installation","text":"<p>The Virtual Appliance admin interface provides the option to upload the update patch file to the appliance. Once the file is uploaded, the appliance will so some validation and then display a summary with options to install or remove.</p> <p></p> <p>When the Install button is clicked, we are prompted to confirm that all the prerequisite steps have been completed. Once this prompt is ticked, the Install button is active.</p> <p>For my home lab setup, the installation took about 10 minutes. At this point the UI changed to indicate it was finished. However, it appears there\u2019s some service restarting and other pieces happening after this point. The administration UI stopped showing errors after an additional 10 minutes.</p>","tags":["vrealize automation"]},{"location":"2019/07/21/cumulative-update-for-vrealize-automation-76/#finishing-up","title":"Finishing Up","text":"<p>After applying the hotfix, I cleared my browser\u2019s cache as per the article\u2019s instructions and tried the form I used as a test again. This time, the buttons did render correctly in Chrome, as shown below.</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/08/24/managing-local-admins-via-vrealize-automation/","title":"Managing Local Admins via vRealize Automation","text":"<p>One of the major benefits of vRealize Automation (vRA) is the ability to add and extend the \u201cActions\u201d available. These Actions enable self-service by the customer. One scenario I wanted to try was allowing someone to manage local administrators on a virtual machine they had provisioned.</p>","tags":["vrealize automation","vrealize orchestrator"]},{"location":"2019/08/24/managing-local-admins-via-vrealize-automation/#creating-the-workflow","title":"Creating The Workflow","text":"<p>The starting point with this is creating a Workflow in vRealize Orchestrator (vRO). Managing local administrators would mean being able to add and remove members, so if I wanted it as a single workflow, there would be some sort of branching logic, such as the flowchart below: </p> <p>In this scenario, the Local Admin access is controlled by an Active Directory group that has a standardised naming format that includes the VM\u2019s name. Since the group is in Active Directory, this presented a couple of options on how to perform the tasks related to it. vRO has native methods and objects relating to Active Directory. vRO can also execute scripts via PowerShell Hosts, and the functionality in PowerShell for managing Active Directory is quite mature now. I did end up using the native coding due to issues with using PowerShell.</p> <p>For Inputs at the workflow level, the first item that\u2019s needed is the Virtual Machine object. This Input isn\u2019t represented when run as an Action in vRA because of the association that vRA does with the Workflow and the VM object. Other Inputs were the operation being performed and arrays for members to be added or removed.</p> <p>Since I ended up using the built-in methods for manipulating the group membership, I used two built in Workflows to do the heavy lifting. For adding members, I used the \u201cAdd users to group members\u201d Workflow, while for removing members I used the \u201cRemove users from group members\u201d Workflow. These are just two of a good range of built-in Workflows available for managing users, groups and other objects in Active Directory.</p> <p></p> <p>When the main pieces are assembled into the Workflow, it ends up looking very similar to the abstracted flowchart from earlier.</p> <p></p>","tags":["vrealize automation","vrealize orchestrator"]},{"location":"2019/08/24/managing-local-admins-via-vrealize-automation/#controlling-the-user-experience","title":"Controlling The User Experience","text":"<p>I wanted there to be a certain user experience when using this Action. While it\u2019s possible to expose native \u201cpicker\u201d UI elements to the user, I didn\u2019t find that desirable as it would mean showing the entire Active Directory structure. The consumer of this Action may not know where the people they want to add are in Active Directory. There was also the issue of how to present the two choices.</p> <p>For the choice of operation, I used the first page of the Workflow presentation, with an option presented. Then I created a page for each operation option, with controls to show or hide them depending on the option. The controls for the Remove Member page are shown below:</p> <p></p> <p>For the Add Member page, I wanted a UI that didn\u2019t expose the complexities of Active Directory, allow a simple search by name and allow multiple users to be entered. By using the \u201cSelect value as\u2026List\u201d property, I was able to get this effect. The result was an interface where the user can enter free text to search for users and add multiple entries to the list.</p> <p></p> <p>For the Remove Members page, I wanted the current members to be shown and the ability to select some or all to remove. By using an Action on the input item to populate the values, I was able to get this effect.</p> <p></p> <p>Since most of the UI controls were enacted in the vRO Workflow, these flowed through to how they are presented in vRA and require no change. The only thing I did change was the Operation selection \u2013 by default this will render as a drop-down and I changed it to radio buttons.</p>","tags":["vrealize automation","vrealize orchestrator"]},{"location":"2019/08/24/managing-local-admins-via-vrealize-automation/#lessons-learned","title":"Lessons Learned","text":"<p>Earlier I mentioned the two possible paths of how to achieve the technical aspects of this Workflow \u2013 using the native AD methods in vRO or using PowerShell via a PowerShell host. Initially I tried the PowerShell approach but ran into issues, specifically getting errors about not being able to find the domain controller. I suspect this was most likely a \u201cdouble-hop\u201d scenario that many PowerShell users run into when using PowerShell remoting.</p> <p>The other major learning was to leverage what\u2019s already available, in this case, the existing AD workflows. This helped accelerate the creation of the Action.</p>","tags":["vrealize automation","vrealize orchestrator"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--installation/","title":"vRealize Automation 8 First Impressions \u2013 Installation","text":"<p>vRealize Automation 7 has been travelling along for a while now. While it\u2019s now at a level of maturity, it\u2019s always been a complicated application, even just in terms of infrastructure (with the need for Windows-based \u201cIAAS\u201d servers). vRealize Automation 8 would appear to represent a tipping point for a lot of things VMware has been working on in the background across multiple products and technologies.</p> <p>The deployment architecture in vRA 8 represents a significant shift from prior versions. The installer deploys three virtual machines. These VMs will consume a total resource set of 12 vCPU, 44GB of RAM and about 246GB of disk space.</p> <p>The vRealize Automation 8 (vRA 8) installer appears to be a unified installer, including Lifecycle Manager and Identity Manager. This means the download comes in at about 9.6GB. It uses a guided GUI-based installer dubbed \u201cvRealize Easy Installer\u201d. It has a look that is very similar to the installer used in recent vCenter releases.</p> <p></p> <p>The Introduction page gives a little explanation of what the Easy Installer is, and some text about the 3 products it can install.</p> <p></p> <p>The Appliance Deployment Target page asks for some standard vCenter Server details, such as hostname, username and password.</p> <p></p> <p>The next section prompts for a location, compute resource and storage location to place the deployment items. The Storage Location page has an option to use Thin Provisioning.</p> <p></p> <p>The Network Configuration page is standard, with IP assignment type, default gateway, etc. The Password Configuration page will prompt for a password. This password will be set for all the deployed products.</p> <p></p> <p>The three pages for the products share common elements, wanting a Virtual Machine name, IP and hostname. For vRA 8, it will also want a license key. It should be noted that there appears to be no validation of the key that is entered. This means the Easy Installer will accept a version 7 key, but later deployment tasks will fail. The Summary page will show all the details selected so far with the option to submit to doing the installation. A progress screen will appear showing the sequence of things.</p> <p></p> <p>The first Virtual Machine created is Lifecycle Manager. The VM is configured with 2 vCPU, 6GB of RAM and almost 50GB of disk capacity across 4 drives. Next the binaries for the other products are staged. Following this, the Identity Manager Virtual Machine is created. The Identity Manager VM has the same vCPU and RAM specifications as Lifecycle Manager\u2019s. Around the time the Identity Manager VM is up, the progress UI will display a link to the Lifecycle Manager interface.</p> <p></p> <p>The last Virtual Machine is for vRealize Automation and it\u2019s the largest. It has 8 CPUs, 32GB of RAM and about 136GB of storage across 4 disks. When the installer is done, the status will update to include a link to the vRA UI.</p> <p></p> <p>At this point the installation is finished and configuration can be performed.</p>","tags":["vrealize automation"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--installation/#closing-thoughts","title":"Closing Thoughts","text":"<p>The new Easy Installer format definitely makes it very easy to install vRealize Automation 8. The ease of use is greatly helped by vRA\u2019s simplified architecture. The only downside is I couldn\u2019t find anything in the directory structure that allowed a command-line based installation for Windows. Overall, the Easy Installer experience was an improvement over that for vRA 7.x</p>","tags":["vrealize automation"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--lifecycle-manager/","title":"vRealize Automation 8 First Impressions \u2013 Lifecycle Manager","text":"<p>vRealize Lifecycle Manager (LCM) is the first component installed by vRA 8\u2019s unified \u201cEasy Installer\u201d. One of its primary functions is the deployment of VMware\u2019s vRealize products. As mentioned in my Installation post, a link to the LCM UI appears towards the end of the installation process. The Dashboard of LCM has five items: Lifecycle Operations, Locker, User Management, Content Management and Marketplace</p>","tags":["vrealize automation"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--lifecycle-manager/#lifecycle-operations","title":"Lifecycle Operations","text":"<p>By logging into this UI, it\u2019s possible to see some of the activity that happened during the installation process. This activity includes the creation of Environments and a Datacenter object.</p> <p></p> <p>As shown in the screenshot above, the Datacenter location defaults to Palo Alto in California. A single Datacenter object is created during the setup, called \u201cDefault_datacenter\u201d, with an associated vCenter instance of named \u201cDefault_vcenter\u201d, using the details supplied during the Easy Install Wizard. The setup process will create two Environments \u2013 a \u201cglobaldefault\u201d environment which contains Identiy Manager and a \u201cvRealize Automation\u201d environment that contains the vRA application.</p> <p></p> <p>Drilling down into an Environment will expose more information about the individual Products assigned to each, such as network settings, infrastructure, etc. Administrative tasks can also be initiated in this area.</p> <p></p> <p>The Settings area has some items of note. A Proxy server can be configured and includes details of what hostnames to whitelist.</p> <p></p> <p>The \u201cMy VMware\u201d item is used to link Lifecycle Manager to a VMware account for licenses, product downloads and show content in the Marketplace.</p>","tags":["vrealize automation"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--lifecycle-manager/#locker","title":"Locker","text":"<p>The Locker appears to act as a sort of secrets management area, with three sections: Certificate, License and Password. In Certificate, certificates can be generated, imported or a CSR generated. The License section allows the storing of product licenses. Password allows the management of stored passwords. Many of the UI elements in LCM won\u2019t allow free text entry for passwords. Instead a dropdown of the password objects created in this Passwords area is used.</p>","tags":["vrealize automation"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--lifecycle-manager/#directory-management","title":"Directory Management","text":"<p>In the Directory Management area, it\u2019s possible to add an Active Directory to act as an authentication source. The experience of doing this is very similar to vRA 7 and is the same if the directory is added via the Identity Management appliance. The other section in Directory Management is Users, where it\u2019s possible to assign users or groups to roles within LCM.</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--lifecycle-manager/#content-management","title":"Content Management","text":"<p>The Content Management area is for defining the items and structured used for the CI/CD style model VMware has created for moving content. The first item in this area is Endpoints. Out of the box, there are no Endpoints defined.</p> <p></p> <p>To be able to add some of these Endpoints, there needs to be items in the Content Settings area. For example, to add a Source Control Endpoint, a Source Control Server needs to be added under Content Settings. Content Settings is where a vSphere Template Repository would be defined, which is a prerequisite to adding the vCenter Server Endpoint.</p>","tags":["vrealize automation"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--lifecycle-manager/#marketplace","title":"Marketplace","text":"<p>The Marketplace exposes a lot of the content available on VMware\u2019s Solution Exchange site, within LCM. At the time of writing, there was 329 items. In terms of vendor breakdown there were:</p> <ul> <li>156 from Bitnami</li> <li>44 from Blue Medora</li> <li>107 from VMware</li> </ul> <p>The content covers items for vRealize Automation (blueprints and virtual appliances), vRealize Log Insight, vRealize Orchestrator (plugins and workflows) and vRealize Operations Manager. Items can be downloaded and deployed via the Marketplace interface.</p> <p></p> <p>The Marketplace opens a whole new avenue of new applications that can be quickly deployed.</p>","tags":["vrealize automation"]},{"location":"2019/10/23/vrealize-automation-8-first-impressions--lifecycle-manager/#closing-thoughts","title":"Closing Thoughts","text":"<p>I\u2019ve had a brief look at earlier versions of Lifecycle Manager in the past, specifically version 1.2 and it\u2019s clear that the product has matured. One item about the automated configuration that may annoy some is that it doesn\u2019t seem to be possible to edit the location of the default Datacenter object that\u2019s created. Apart from that, the product seems like a good progression from past versions.</p>","tags":["vrealize automation"]},{"location":"2019/10/25/vrealize-automation-8-first-impressions--getting-started/","title":"vRealize Automation 8 First Impressions \u2013 Getting Started","text":"","tags":["vrealize automation"]},{"location":"2019/10/25/vrealize-automation-8-first-impressions--getting-started/#initial-login-and-quickstart","title":"Initial Login and Quickstart","text":"<p>Towards the end of the Easy Install wizard for vRealize Automation 8 (vRA 8), a link is provided for the vRealize Automation 8 UI. Accessing this link will load a landing page that shows a short piece of text and a link to a login button.</p> <p></p> <p>The logon prompt is very similar to the 7.6 experience and uses the default configuration admin user that was specified in the Easy Installer. A Quickstart wizard is shown when logging in for the first time.</p> <p></p> <p>The Quickstart will prompt for a number of items, such as a vCenter server, NSX manager and so forth.</p> <p></p> <p>The Basic Configuration section is used to specify resources used to create the first Catalog Service, using an existing template and other items. This step will take a few minutes.</p> <p></p> <p>The last section, Policies, cover Lease period (default of 7 days) and the Machine name format. The format defaults to the requester\u2019s name with a numeric sequence. Following the completion of the Quickstart, we are returned to the landing page, which exposes the main areas of use in vRA 8</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/10/25/vrealize-automation-8-first-impressions--getting-started/#services-overview","title":"Services Overview","text":"<p>vRealize 8 breaks out its interface into a number of Services. These Services are:</p> <ul> <li>Cloud Assembly \u2013 has interfaces for managing infrastructure, creating blueprints and managing deployments. This would be the area that blueprint designers would be accessing most often.</li> <li>Service Broker \u2013 Acts as the service catalog for your consumers, as well as management of policies (ie. governance)</li> <li>Code Stream \u2013 Management of continuous integration/continuous deployment (CI/CD)</li> <li>Orchestrator \u2013 VMware\u2019s automation workflow product, vRealize Orchestrator</li> </ul>","tags":["vrealize automation"]},{"location":"2019/10/25/vrealize-automation-8-first-impressions--getting-started/#results-of-quickstart","title":"Results of Quickstart","text":"<p>As shown in the screenshot from the Quickstart process, there is mention of a Catalog Service being created and deployed. Accessing the Cloud Assemble service for the first time will show this automatic deployment. The results look something like what is shown below.</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/10/25/vrealize-automation-8-first-impressions--getting-started/#guided-setup","title":"Guided Setup","text":"<p>The Guided Setup is a wizard that helps assist the end-to-end scenario of creating a Cloud Account through to a Project and finally a Deployment. There\u2019s even a nice diagram of it that VMware provide.</p> <p></p> <p>The first step of the Guided Setup is picking a Cloud Account to add. If your vRA 8 instance is on an Enterprise license, there will be the public cloud providers as options. If your license is the Advanced one, there is just vCenter and VMware Cloud on AWS. Each of the public cloud options will require the appropriate details for API access. For example, if using AWS this would mean an Access Key ID and Secret Access Key. The specifics of what each public cloud requires is detailed at \u201cBefore you begin with vRealize Automation Cloud Assembly\u201d.</p> <p></p> <p>In the case of adding an AWS account, it\u2019s possible to limit provisioning to specific regions as part of adding the Cloud Account, which the selected regions created as a Cloud Zone. Since the creation of a Cloud Zone is automatic when using AWS, the next step in the Guided Setup is creating a Project. Projects could be thought of in the same way as Business Groups in vRA 7 \u2013 they are the bridge between users and resources. When creating a Project, users are added as members. Then a Cloud Zone can be associated with the Project. As shown below, the AWS Cloud Zone has been added to a Project.</p> <p></p> <p>After the Project has been created, the Guided Setup moves to the creation of Flavor Mappings. Flavor Mappings are effectively \u201ct-shirt sizing\u201d. Since vRA 8 is multi-cloud, these mappings allow you to define universal t-shirt sizes across all platforms. Since I have AWS and vCenter configured, I can create a Flavor Mapping that covers both these platforms.</p> <p></p> <p>The next step in the Guided Setup is to create an Image Mapping. Similar in concept to Flavor Mappings, Image Mappings are about being able to reference an abstracted OS image entity, which is resolved to the appropriate item for each platform. Fortunately when adding an entry for AWS, it\u2019s possible to past in just the AMI ID and it will find the item. It\u2019s possible to add \u201cCloud Configuration\u201d which would appear to be comparable to userdata for AWS.</p> <p></p> <p>The last step in the Guided Setup is creating a Blueprint. The Blueprint designer UI in vRA 8 seems to rely more heavily on setting details in the YAML text. Fortunately there\u2019s a decent amount of intellisense-style behaviour with it presenting available options when clicking in areas that need values.</p> <p></p> <p>Once the Blueprint has been created, the Guided Setup instructs the user to deploy it. A window appears asking for details, including the Blueprint Version. At this point, there\u2019s only a draft version, but it does give a hint at what\u2019s possible with the new versioning support in vRA 8. When the deployment request is submitted, the Guided Setup switches to the Deployment\u2019s detail screen.</p> <p> Deployment details The History tab is laid out in a way that reminds me very much of AWS CloudFormation.</p> <p></p> <p>When looking at the machine deployed in AWS, it\u2019s clear that vRA 8 has a lot going on. A bunch of tags are created on the EC2 instance by vRA.</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/10/25/vrealize-automation-8-first-impressions--getting-started/#closing-thoughts","title":"Closing Thoughts","text":"<p>It seems clear from what I\u2019ve written about here that VMware have realised that vRealize Automation is a completed application. This is even more true in version 8 with the tight integrations with public cloud providers. With things like the Guided Setup, VMware have made it very easy to run through processes that needed to get things up and running. The use of the two new Mapping types removes the need for a lot of \u201cmanagement code\u201d that may have been written in vRO behind the scenes. Overall, it\u2019s much easier to get in and see outputs from deployments.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--cloud-assembly/","title":"vRealize Automation 8 First Impressions \u2013 Cloud Assembly","text":"<p>The Cloud Assembly section of vRealize Automation 8 is the one that vRA Administrators will most likely spend the most time. In vRA 7 terms, it constitutes aspects of the Infrastructure and Adminstration areas, plus the Blueprint Designer.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--cloud-assembly/#infrastructure","title":"Infrastructure","text":"<p>The Infrastructure tab contains the bulk of items relating to the configuration of vRA 8. The first item that most vRA administrators will have to head to is Cloud Accounts, under the Connections heading. This is where the account details for various public cloud and VMware offerings are configured. A typical scenario in this area could be vCenter and a couple of public clouds configured here.</p> <p></p> <p>The other item under Connections is Integrations, where a range of VMware and third party integrations can be added. By default, there will already be an entry for the embedded vRealize Orchestrator (vRO) instance.</p> <p></p> <p>The next section of interest is Configure, which contains the items of Projects, Cloud Zones, Kubernetes Zones, Flavor Mappings, Image Mappings, Network Profiles, Storage Profiles and Tags. Some of these are mentioned during the Guided Setup (as shown in the Getting Started post).</p> <p>Network Profiles allow the creation of objects that control network behaviour and settings. The options that become available when creating a Network Profile depend on the Cloud Account selected. For AWS, the settings include the ability to create on-demand networks or security groups, and the selection of existing networks. Tags can also be applied. For vCenter-based Network Profiles, there are options to add IP ranges. When selecting existing networks to use, discovered items are shown with extra information. For AWS, this can include the CIDR or whether public IPs are enabled.</p> <p>Storage Profiles control the way storage is provisioned for virtual machines. For vCenter Cloud Accounts, the standard set of settings are exposed, such as Storage policy, thin/thick provisioning, and datastore. For other Cloud Account types, the expected options are exposed.</p> <p></p> <p>The Tags item lists all the tags discovered across all the Cloud Accounts. It\u2019s then possible to select one or more tags and see what objects are currently assigned those tags. A good use case for this would be if a cost center tagging system was being used. It would be possible to see what resources a particular cost center is using across all platforms.</p> <p></p> <p>The Resources section sits under Configure and has a by-type breakdown of all resource items that vRealize Automation can see. These items are Compute, Networks, Security, Storage, Machines, Volumes and Kubernetes. Compute lists high level abstractions of the compute capability in each Cloud Account and the administrator can\u2019t do much in here except apply tags.</p> <p>The Networks item has more content and actions available. It lists all networks discovered across Cloud Accounts. There\u2019s also the ability to manage IP ranges, review individual IP addresses being managed, view load balancers and view \u201cnetwork domains\u201d. These network domains are the top-level network object for each platform type (ie. for AWS, that would be VPCs).</p> <p>The Security item lists only Security Groups that have been discovered. In the case of my configuration, this meant security groups from AWS. The only action that can be performed in this area is to add or remove tags.</p> <p>Storage has three tabs \u2013 Storage Policies, Datastores/Clusters and Storage Accounts. The first two tabs are vCenter-focused, with Storage Policies listed the discovered policies. Tags can be managed on these policies. Datastores/clusters is has similar functionality, appearing to be mainly for informational and tagging purposes. Storage Accounts lists any defined Azure storage accounts and can be tagged.</p> <p>The Machines item lists all the machines across Cloud Accounts and includes information like status, IP address, Project, Owner and tags. There\u2019s also the ability to filter the list by a variety of criteria.</p> <p></p> <p>Volumes lists all the \u201cvolumes\u201d that have been discovered, which seems to include CD-ROM and floppy disk drives on vCenter VMs. Unfortunately the interface doesn\u2019t list what machine the volume is attached to in the list view. It is possible to find this information by drilling down into the object.</p> <p>The last area of any real interest under the Infrastructure tab is Onboarding. This area relates to creating \u201conboarding plans\u201d for machines the vRA has discovered but isn\u2019t managing. A benefit of this onboarding process is that vRA will create a blueprint based on the machines imported.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--cloud-assembly/#extensibility","title":"Extensibility","text":"<p>The Extensibility tab appears to share a lot of common elements with vRA 7, allowing the use of \u201cSubscriptions\u201d for triggering Orchestrator workflows. The menu items for this tab are shown below.</p> <p></p> <p>The Workflows item lists the 463 vRealize Orchestrator workflows that come with version 8, but it doesn\u2019t seem to be anything to do in this area except look at them. The Actions item has no items listed, even though Orchestrator has over 400. Creating a new Action loads a code editor where the administrator can opt to write their code. By default, it seems to load the \u201cCustom script\u201d template which exposes the new ability to write python or nodejs code.</p> <p></p> <p>These Python-based scripts are part of the new \u201cAction-based Extensibility\u201d (ABX) that VMware have introduced in vRA 8. They are similar to how one might use vRealize Orchestrator workflows in vRA \u2013 to have automated tasks run at specific triggers. There is a reference in the documentation on ABX that suggests the code is actually run in the cloud, specifically using AWS Lambda. As such, an AWS subscription is required.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--cloud-assembly/#marketplace","title":"Marketplace","text":"<p>The Marketplace tab is an extended version of that which appears in Lifecycle Manager (LCM). It has three main areas \u2013 Blueprints, Images and Downloads. Images is a like-for-like match to LCM\u2019s Marketplace content, containing virtual appliances from Bitnami and other vendors.</p> <p></p> <p>The Blueprints section is the more traditional blueprint items. Some are clearly designed for cloud-based deployments, such as one that uses AWS Redshift. Others are VMware-focused, relying on NSX. At the time of writing, only 18 blueprints are available.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--cloud-assembly/#blueprints","title":"Blueprints","text":"<p>The Blueprints tab represents a major area of use for infrastructure engineers and developers. This is where blueprints can be created, tested and reviewed. At a high level, the Blueprint Designer in version 8 has a lot of common elements with version 7.</p> <p></p> <p>Two major changes are visible in the screenshot above. Firstly, with the Infrastructure-as-Code (IAC) approach in vRA 8, the actual code of the blueprint is shown on the right side. A lot of the detail defining in a blueprint is now performed in this code area, as opposed to GUI elements in version 7. The other major shift is the expanded range of components from cloud services. It\u2019s now possible to have cloud services like AWS\u2019s RDS or Lamba or Azure\u2019s Key Vault directly on the blueprint.</p> <p>Verison 7 had a nested approach where certain objects could be nested inside of others. This seems to be gone now in version 8, with objects related to each other. A good example of this is the Configuration Management items like Puppet and Ansible. Adding items to the blueprint will generate a skeleton of code on the right side.</p> <p></p> <p>In some cases, a lightbulb will appear which assists in filling out the code. In the case of the vSphere machine, clicking this icon displayed a list of optional parameters that could be added. When using certain blueprint items, the fields that require values will show available options when clicking inside the quotes. For example, when clicking in the image or flavor properties for a Cloud Agnostic VM, Image and Flavor Mapping items will be listed.</p> <p></p> <p>The code view also has real time syntax checking to point out errors. This helps ensure the code is valid and will work when deployed. It\u2019s possible to define a number of inputs for a blueprint, removing the need to hardcode a lot of values. There\u2019s a decent range of controls that can be applied to the inputs to prevent bad values being entered.</p> <p></p> <p>With the Infrastructure as Code focus, blueprints now have built in versioning support. This helps avoid the sort of thing I\u2019ve seen in some organisations where version control is done by copying the blueprint repeatedly with an incrementing number in the name. With this proper versioning capability comes support tools like code diff.</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--cloud-assembly/#closing-thoughts","title":"Closing Thoughts","text":"<p>Now that I\u2019m getting into the actual areas of day-to-day use of vRealize Automation 8, it\u2019s clear that in some areas there\u2019s a lot that\u2019s changed under the hood and for the better. The versioning support is a good acknowledgement that there\u2019s often more than just one vRA administrator in many organisations and it helps avoid that \u201cstepping on each other\u2019s toes\u201d situation. The code-based approach for blueprints may be a change for some. Those who have already come from IAC backgrounds (especially AWS Cloudformation or Terraform) should feel very comfortable with the concepts in the code-based blueprints.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--service-broker/","title":"vRealize Automation 8 First Impressions \u2013 Service Broker","text":"<p>The Server Broker section of vRealize Automation 8 contains the items that your consumers will interact with the most \u2013 the Catalog, and the Deployments tab where they can review the status of their requests. It also has some administration areas, such as Content &amp; Policies and Infrastructure</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--service-broker/#infrastructure","title":"Infrastructure","text":"<p>The Infrastructure tab under Server Broker contains an abbreviated set of the options that are available under Cloud Assembly. Specifically, these items are Projects, Cloud Zones, Cloud Accounts and Integrations. All of these items function as like their counterparts in the Infrastructure area of Cloud Assembly.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--service-broker/#content-policies","title":"Content &amp; Policies","text":"<p>The first 3 items under this are relate to getting content available in the Catalog. The others relate to polices and notifications.</p> <p></p> <p>Content Sources allow the defining of sources for blueprints and other items. When adding a new Content Source, a number of items are available.</p> <p></p> <p>Each option will slightly alter the rest of the form. For example, selecting the AWS CloudFormation Template option will bring up fields relating to bucket policies. This process of adding Content Sources could be similar to publishing a blueprint in version 7.</p> <p>Following this importing process, a blueprint can then be shared to other users. This process happens under the Content Sharing menu item. A Project is selected, then the items from that project.</p> <p></p> <p>At this point, the shared item is available in the Catalog. The Content menu item lists all the imported blueprints and templates. From here it\u2019s possible to also create a custom icon or form for the item.</p> <p></p> <p>The Policies menu item has 2 sub-items \u2013 Definitions and Enforcement. The Quickstart process creates a policy item that controls the lease period of systems provisioned in that Project. At the time of writing, there are two policy types that can be defined \u2013 Lease Policies for controlling lease periods on provisioned items and Day 2 Actions Policy which controls what Day 2 Actions users can perform.</p> <p>Lease Policies can be scoped to the Organisation or Projects. The area of effect can be further narrowed using Deployment Criteria. Using the criteria, it\u2019s possible to limit the Policy to a particular blueprint ID. Interestingly, it\u2019s not possible to use tags as part of the Criteria.</p> <p></p> <p>The Enforcement menu item shows an audit log of policies that have been applied. Day 2 Action Policies are similar in terms of the request form.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--service-broker/#catalog","title":"Catalog","text":"<p>The Catalog look and feel is very similar to the new UI that was introduced in vRealize Automation 7.6, with large tiles for each catalog item.</p> <p></p> <p>Submitting a request will direct the UI to the Deployments tab with the deployment progress shown.</p> <p></p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--service-broker/#deployments","title":"Deployments","text":"<p>The Deployments tab in vRA 8 follows the same design used in 7.6. When a Deployment is finished, it\u2019s possible to see some surface information about the items within, like power state and IP address. The Actions available on a Deployment are relatively standard. The \u201cUpdate\u201d item appears to replace the \u201cReconfigure\u201d item in version 7.</p> <p></p> <p>Running the Update Action allows the user to change the Deployment based on the input items available. Clicking Next in the wizard for the Update Action will show a \u201cPlan\u201d (very similar to the output of Terraform\u2019s plan command) which lists what will be added, removed or changed.</p> <p></p> <p>It\u2019s possible to drill down into the Deployment and select individual deployed items. At this point, the Day 2 Actions of those items are available. The available options change slightly depending on the object. For example, snapshot-related actions (ie. Create Snapshot) are only available if the item is a virtual machine on vSphere or Google Cloud Platform.</p>","tags":["vrealize automation"]},{"location":"2019/10/29/vrealize-automation-8-first-impressions--service-broker/#closing-thoughts","title":"Closing Thoughts","text":"<p>The Service Broker area of vRealize Automation 8 seems very slick and functional. The Policies area is a step back from what was in version 7, where there was quite a lot of control that could be exercised over what needed approvals or not. There\u2019s a hint that the Tagging system may be intended as a replacement for that.</p>","tags":["vrealize automation"]},{"location":"2019/12/08/vra-8--getdiskinfo-error-partition-name-buffer-too-small/","title":"vRA 8 \u2013 GetDiskInfo: ERROR: Partition name buffer too small","text":"<p>After spending a lot of time looking at the web interface for vRealize Automation 8 (vRA 8), I decided to look under the hook a bit. One of the first things I looked at was the logs. It seems one of the primary logs that vRA 8 uses is <code>/var/log/vmware-vmsvc.log</code> Upon viewing this log, I was greeted with the following spam: <pre><code>[2019-12-05T11:47:54.126Z] [ warning] [guestinfo] GetDiskInfo: ERROR: Partition name buffer too small\n[2019-12-05T11:47:54.126Z] [ warning] [guestinfo] Failed to get disk info.\n[2019-12-05T11:48:24.128Z] [ warning] [guestinfo] GetDiskInfo: ERROR: Partition name buffer too small\n[2019-12-05T11:48:24.128Z] [ warning] [guestinfo] Failed to get disk info.\n[2019-12-05T11:48:54.127Z] [ warning] [guestinfo] GetDiskInfo: ERROR: Partition name buffer too small\n[2019-12-05T11:48:54.128Z] [ warning] [guestinfo] Failed to get disk info.\n</code></pre></p> <p>As shown by the timestamps, this error will repeat every 30 seconds, resulting in this log being totally flooded with this error. I also confirmed this error was happening in another instance than my own. Upon googling the message, I found a Github issue entry that referenced this and how it can be caused by the very long paths with Kubernetes. vRA 8 uses Kubernetes heavily. The code fix that resolved this issue appears to have been folded into the v11.0.1 release of the open-vm-tools. When checking the version on the vRA 8 appliance, we can see the following:</p> <p></p> <p>When checking the package info via yum, the versions available range from 10.2.0 to 10.3.10 from the repositories that vRA is configured to use. So it appears updating isn\u2019t an option at this time.</p>","tags":["vrealize automation","open-vm-tools"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/","title":"Calling System Center Orchestrator Runbooks from vRealize Orchestrator","text":"<p>Sometimes you end up having to put in place an implementation that\u2019s pretty crazy to get something (non-production) over the line. This was the case recently where I used vRealize Orchestrator (vRO) to call System Center Orchestrator (SCORCH) Runbooks. That is, using Orchestrator to call Orchestrator\u2026</p> <p></p> <p>A lot of the credit for figuring out how to do this goes to Laurie Rhodes and their blog post about calling SCORCH runbooks via REST using Powershell. It was my starting point for this piece of work and I was able to adapt the core pieces of this for my scenario.</p>","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/#vro-configuration","title":"vRO Configuration","text":"<p>Assuming there\u2019s existing SCORCH and vRO instances, the first task is to add the SCORCH server as a REST host in vRO. This can be achieved by running the \u201cAdd a REST Host\u201d workflow that comes with vRO. The \u201cOrchestrator Web Service\u201d runs on port 81, so that will affect the settings for the host.</p> <p></p> <p>For Host Authentication, I used Kerberos. I didn\u2019t have a lot of time to experiment with this. Kerberos was the first I used and it worked. The accout specified will need appropriate permissions to call SCORCH Runbooks. The rest of the settings in the Add A Host workflow can take defaults appropriate for your environment. Once the workflow has finished there will be a new entry for the SCORCH host.</p> <p>The next step is to add some REST Operations that will correspond to tasks required. These tasks are finding the GUID for a runbook, executing a runbook and checking its run state. Using the Add A REST Operation workflow achieves this easily.</p> <p></p> <p>The reason for this sequence is because the GUID is needed to execute a runbook, but we don\u2019t always necessarily know the GUID and it may not stay the same. With the first step we can definitively get the correct GUID. The details of the REST Operations are in the table below:</p> TASK METHOD URL Get Runbook GUID GET /Orchestrator2012/Orchestrator.svc/Runbooks?$filter=Name%20eq%20%27{Name}%27 Get Runbook Parameters GET /Orchestrator2012/Orchestrator.svc/Runbooks(guid'{Guid})/Parameters Execute Runbook POST /Orchestrator2012/Orchestrator.svc/Jobs/ Get Job Status GET /Orchestrator2012/Orchestrator.svc/Jobs(guid'{JobGuid}\u2019)","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/#creating-the-vro-workflow","title":"Creating the vRO Workflow","text":"<p>The vRO workflow ends up being a sequence of REST Operations being called with certain information being extracted and passed on. In a general flowchart format, the workflow might end up as below:</p> <p></p>","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/#get-the-runbook-guid","title":"Get The Runbook GUID","text":"<p>In the first step, we want to extract the Runbook GUID and make it available for the next step. The code for this first piece of the workflow is somewhat simple \u2013 perform the REST Operation and extract out the Runbook GUID.</p>","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/#get-runbook-parameters","title":"Get Runbook Parameters","text":"<p>As part of initial discovery, the next step would be to find out the parameters for the SCORCH Runbook. These are the inputs it expects to be able to execute. For example, it could be something like a computer name. By calling the appropriate REST Operation and passing it the GUID from the first step, it\u2019s possible to find these parameters and their GUIDs. Depending on one\u2019s approach or goals, you could just use this step to discover the parameter GUIDs and hard-code them into the execution payload, or you could programmatically generate the payload for a more general purpose set of code.</p>","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/#execute-the-runbook","title":"Execute The Runbook","text":"<p>To execute the Runbook, a REST POST operation is performed with an XML-formatted payload. SCORCH expects this payload to be structured a certain way. In his blog post, Laurie Hodes details what this payload looks like normally and variant needed for Powershell. In other examples, the payload is modified to use CDATA. With vRO, it seems this modification is needed and the CDATA option is the cleaner of the two. The payload format is below:</p> <p>Now we can see why the parameter discovery done earlier is important because firstly the parameter GUID is needed in the payload, but also we may have more than one parameter, while the example above has only one. In cases where there\u2019s more than one parameter, you just need to repeat the <code>&lt;parameter&gt;</code> block in the XML and have the appropriate values.</p> <p>If execution request is successful a status code of 201 is returned as well as XML content with details of the Job that was created. Some of the top of this response is shown below, with the Job GUID highlighted.</p> <p></p> <p>This Job GUID will match the value in the SCORCH console and it\u2019s possible to see a matching entry for it there.</p> <p></p>","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/#checking-job-status","title":"Checking Job Status","text":"<p>The last REST call in this sequence is to get information about the Job. Depending on the nature of the job, it may take some time to run and we want to be able to confirm it completed successfully. The REST call itself is simple, requiring a single input parameter of the Job GUID. We want the status value extracted so it can be examined.</p> <p>The next steps involve checking the status value and doing behaviours based on it. If the status is \u201cpending\u201d, we wait for a bit, as the Job is still running. If the status is \u201ccompleted\u201d, then the workflow can end. Any other values should throw an error of some sort as any other status would indicate an error or unexpected outcome.</p>","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/#final-workflow-form","title":"Final Workflow Form","text":"<p>The end result of implementing these steps in a vRO Workflow is a Workflow Schema like the one shown below.</p> <p></p> <p>Aspects of this could be expanded or tweaked, but as a starting point it\u2019s a good base.</p>","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2019/12/13/calling-system-center-orchestrator-runbooks-from-vrealize-orchestrator/#referencesresources","title":"References/Resources","text":"<p>The following resources were very helpful in figuring out how to get this piece of work completed.</p> <ul> <li>Calling Orchestrator Runbooks (&amp; retrieving output) via REST by Laurie Rhodes</li> <li>Orchestrator Web Service, Start a Runbook by Microsoft</li> <li>Starting Runbooks and Stopping Jobs Using the System Center 2012 Orchestrator Web Service by the Microsoft System Center Team</li> </ul>","tags":["vrealize orchestrator","system center orchestrator"]},{"location":"2020/01/01/vmug-vrealize-suite-2019-and-vra-8/","title":"VMUG vRealize Suite 2019 and vRA 8","text":"<p>VMUG recent added the vRealize Suite 2019 to their EVALExperience offering. For those not familiar with it, EVALExperience is part of the paid \u201cAdvantage\u201d member in VMUG. This paid membership includes discounts on training and other benefits. This is on top of benefits of free membership.</p> <p>This new addition means it\u2019s now possible to get a 365-day license for all the components of the vRealize Suite 2019, including vRealize Automation 8. The license is for personal use in a home lab. I had previously tried updating the license on my vRA 8 installation from an Advanced to an Enterprise one, using Lifecycle Manager. It didn\u2019t like that.</p>","tags":["vrealize automation"]},{"location":"2020/01/01/vmug-vrealize-suite-2019-and-vra-8/#using-vracli","title":"Using vracli","text":"<p>One of the major changes in vRA 8 was the removal of the \u201cVAMI\u201d administrative interface for managing the virtual appliance. Some of the administrative tasks performed via this interface in version 7 is available in version 8 via the <code>vracli</code> command. License management is one of those tasks. The starting point for all this is running the license command with the help switch to see what options are available. The output from this command is shown below:</p> <p></p> <p>Running the license command without the help switch will list details of the license assigned. As shown below, the one in use has expired.</p> <p></p> <p>In the case of that happening, vRA itself will display a 402 error when attempting to login via the web interface. The next step would be to add a new key. This is done using the add argument.</p> <p></p> <p>A bunch of output is generated that has detail about the license that\u2019s been added. This includes what edition the license is for, limits on how many VMs can be provisioned and the expiration. When running the license command again, there are now two entries \u2013 the original expired evaluation license and the new one from VMUG.</p> <p></p> <p>For the sake of verification, it\u2019s possible to use the command <code>vracli license current</code> which will list the active license key and the features enabled or disabled under the license. This feature information is useful to validate that your license is exposing the right features. Some of the listed feature items explicitly reference public cloud and configuration management, which are features not available under the Advanced license.</p> <p>Lastly, for the sake of cleaning up, it\u2019s probably a good idea to remove the old key and reboot.</p>","tags":["vrealize automation"]},{"location":"2020/01/09/vrealize-automation-801-update-walkthrough/","title":"vRealize Automation 8.0.1 Update Walkthrough","text":"<p>VMware have released a minor update for vRealize Automation (vRA) 8. This is my experience of attemtping to update the instance running in my home lab.</p>","tags":["vrealize automation"]},{"location":"2020/01/09/vrealize-automation-801-update-walkthrough/#update-preparation","title":"Update Preparation","text":"<p>In the Release Notes for 8.0.1 there\u2019s a section for performing an upgrade. A couple of items in this section jump out. Firstly, that the vRA product supports upgrading from vRealize Suite Lifecycle Manager (LCM), with a link on the process. The second is an explicit mention of disk space requirements. Based on this, the first thing I checked was the free space for the two partitions mentioned.</p> <p></p> <p>As shown, in the case of my vRA 8 system, I had 4.2GB free on the root partition, when I need at least 20GB. The data partition has 64GB free when I need at least 48GB. The Upgrade section includes instructions on how to extend the disks without needing to power off the VM. I performed the process. At the end, I had the figures shown below:</p> <p></p> <p>Moving to the documentation about performing the upgrade using LCM, one of the prerequisites is to perform the binary mapping for the upgrade ISO. This is set on the LCM system under Lifecycle Operations &gt; Settings &gt; Binary Mapping &gt; Product Binaries. One issue that seems to come up straight away is LCM 8.0 has no visibility of vRA 8.0.1 in this area. Version 8.0 and earlier versions will appear but not the new patch.</p> <p></p> <p>This issue would be because when one clicks on the link about supported product versions, there\u2019s no mention of 8.0.1.</p> <p> Reviewing supported product versions The logical step to get past this would be to upgrade the LCM system first. To initiate this, go to Lifecycle Operations &gt; Settings &gt; System Upgrade. Generally I prefer to download the update media first and then use the ISO option but in this case VMware\u2019s website was being uncooperative so I tried the \u201cCheck Online\u201d option. After clicking the Check Upgrade button I got confirmation that a new product version was avialable.</p> <p> Starting the upgrade of LCM Upon clicking the Upgrade button, a progress area appeared with the download state of 8 packages. I could also see network traffic being generated by the LCM appliance. The download process took about 30 minutes for me. At this point, the progress area changed to indicate an install was in progress. Once the update is complete, the login screen is shown. After logging in, I was able to confirm that 8.0.1 was a supported version.</p> <p> Supported product versions after updating LCM At this point I was able to get the upgrade binary via the VMWare site and uploaded it to the LCM appliance so it was available locally.</p>","tags":["vrealize automation"]},{"location":"2020/01/09/vrealize-automation-801-update-walkthrough/#executing-the-update","title":"Executing The Update","text":"<p>At this point, I was now able to go back to the LCM documentation about updating vRA 8. To initiate the update, go to Manage Environments &gt; View Details and click on the Upgrade button.</p> <p></p> <p>After clicking this, a prompt will appear about performing an inventory sync. If the option to proceed to upgrade is selected, we are then asked for the Repository Type. Since I had already performed a mapping of the update ISO, it\u2019s available.</p> <p></p> <p>The next section will run some prechecks like whether SSH is enabled. This part didn\u2019t take long and I could then progress to the Upgrade Summary tab. On this tab, the Next button changes to a green Submit.</p> <p></p> <p>When the Submit button is clicked, a request is created to perform the upgrade. The request workflow diagram shows the rather complicated nature of what will be performed.</p> <p></p> <p>For my update, Stage 1 took a bit over an hour, while Stage 2 (an inventory update) took 533ms. I\u2019ve seen some people have reported their attempts ran for about 5 hours before timing out, so I suspect if an upgrade runs for 2 hours or more, something has gone wrong.</p>","tags":["vrealize automation"]},{"location":"2020/01/09/vrealize-automation-801-update-walkthrough/#verifying-the-update","title":"Verifying The Update","text":"<p>At this point, I was able to go to the vRA 8 landing page and see the build/version details had changed.</p> <p></p> <p>I could also see the new GUI editor interface for blueprints. Unfortunately, the issue I previously wrote about, where the open-vm-tools spams a log file, isn\u2019t resolved in this update.</p>","tags":["vrealize automation"]},{"location":"2020/02/03/automating-vulnerability-scans--nexposepowershell/","title":"Automating Vulnerability Scans \u2013 Nexpose/Powershell","text":"<p>One of the common things I\u2019ve seen in automation implemented by infrastructure teams is a lack of rigor around testing. That is to say, code that tests the task that is being automated is actually successful. A script could execute to its end without errors, but that doesn\u2019t necessarily mean it actually did what it was supposed to.</p> <p>This leads into a concern I\u2019ve seen raised by stakeholders, about visibility of what is happening in an automation pipeline. One of the key stakeholders in this sort of security is the IT Security team, who often want visibility of certain outputs (like virtual machines) to determine if those are secure. In a couple of environments I\u2019ve raised the idea of performing automated vulnerability scans on newly provisioned assets, as a way of ensuring what is delivered is at an acceptable level. By automating this task, we place no extra burden on those involved and ensure consistency.</p>","tags":["powershell"]},{"location":"2020/02/03/automating-vulnerability-scans--nexposepowershell/#design","title":"Design","text":"<p>In the example design I\u2019ve used for this scenario, I\u2019ll be using Nexpose as the vulnerability scanner. I\u2019ve used this product in a couple of roles and it has a good REST API to allow the automation tasks required. A reference to the REST API can be found by clicking on the Help icon, then the \u201cAPI Documentation\u201d item in the Nexpose interface. The automation layer will be done via some basic Powershell scripts. Since Nexpose doesn\u2019t use a token-based approach for authentication, we can easily skip that step and end up with a workflow similar to below:</p> <p></p> <p>The specifics of how some of these steps may look depends on one\u2019s environment and needs. In this case, I\u2019m trying to keep it very basic.</p> <p>Scanning New Asset To initiate the scan on a new asset, it has to exist in a \u201cSite\u201d in Nexpose that\u2019s already been configured. This is because the REST call needs a site ID. It also needs a properly formatted body that specifies the hosts to be scanned and the ID of the scan template to use. In the code below, the \u201cFull Audit\u201d template is being used. <pre><code># Set the body to be posted\n$body = @{\n  name       = 'QA scan by API'\n  templateId = 'full-audit'\n  hosts      = @($scanTarget)\n}\n</code></pre> A simple Invoke-RestMethod is used with the appropriate values. An example of this is available at https://github.com/jpboyce/powershell-library/blob/master/nexpose/start-scan.ps1 The response from the REST call includes the ID of the scan, which is needed for subsequent steps.</p>","tags":["powershell"]},{"location":"2020/02/03/automating-vulnerability-scans--nexposepowershell/#getting-scan-results","title":"Getting Scan Results","text":"<p>When retrieving scan results, only high level details like the count of vulnerabilities is available. Depending on your needs, you may just use a certain threshold value for this as a pass or fail tollgate. In the get-scan script I\u2019ve written, I\u2019ve included a switch block to allow the output of the specific vulnerability types or just the entire response object.</p> <p>If more detail is required, then the individual vulnerabilities can be outputted. In an example script for this, the response object is just outputted. This response has an ID for each vulnerability which has the CVE as part of the name, meaning it could be possible to filter this list to look for specific vulnerabilities. An example of the output is shown below.</p> <p></p>","tags":["powershell"]},{"location":"2020/02/03/automating-vulnerability-scans--nexposepowershell/#generating-a-scan-report","title":"Generating A Scan Report","text":"<p>Nexpose has some rich reporting options, including remediation reports. This could allow some slick outcomes from this automation piece, like generating a report and emailing it to the IT Security Team, or attaching it to the new asset\u2019s entry in the CMDB. Alternatively, if the results of an asset are particularly bad, then generating a remediation report may be a good option.</p> <p>The values required for generating a report vary between report types. In the example script I wrote, a remediation plan report is created and generated. Fortunately, remediation plan reports can accept a scan ID is a filtering option.</p>","tags":["powershell"]},{"location":"2020/02/03/automating-vulnerability-scans--nexposepowershell/#applying-to-a-scenario","title":"Applying To A Scenario","text":"<p>To see how this would all look, I decided to run this all against a completely un-patched, unsecured Windows 2019 VM and then against the same VM after it\u2019s patched. The table below compares the vulnerability counts:</p> Severity Un-patched Patched Critical 3 0 Severe 31 4 Moderate 7 2 Total 41 6 <p>34 of the 41 vulnerabilities on the un-patched system were related to Microsoft CVEs which strongly implies they could be addressed by simply patching the VM. In fact, when generating the remediation report, it listed only 1 Windows update patch to apply. After patching, the VM had 6 vulnerabilities left. Most of these would be items addressed in a standard corporate environment (such as password complexity and history). Based on this, in some circumstances, the total vulnerability count could be use as a pass/fail tollgate.</p>","tags":["powershell"]},{"location":"2020/02/03/automating-vulnerability-scans--nexposepowershell/#final-thoughts","title":"Final Thoughts","text":"<p>The things I\u2019ve written about cover a good starting point for automating vulnerability scans. This process could be extended a number of different ways, including automatically isolated non-compliant systems or attempting to remediate them. It does address the initial issue of increasing visibility for stakeholders.</p>","tags":["powershell"]},{"location":"2020/03/06/powershell-70--introduction-history-installation/","title":"PowerShell 7.0 \u2013 Introduction, History, Installation","text":"<p>Microsoft has finally announced the General Availability (GA) release of PowerShell 7.0. This represents a fairly significant milestone in PowerShell\u2019s history. In this post, I\u2019ll go through some of the history prior to this point, what\u2019s new in this release and how it works in practice.</p>","tags":["powershell"]},{"location":"2020/03/06/powershell-70--introduction-history-installation/#history","title":"History","text":"<p>Before 2006, the options for scripting and automation on Windows were limited. There was VBscript, which could interact with some systems like Active Directory. Interfaces to Active Directory, Exchange and other systems weren\u2019t standardised. Doing any sort of automation during that time was difficult. There were some third party tools but they were all proprietry. For those of us around back then, it was a dark time.</p> <p>PowerShell Version 1.0 was released in 2006. In subsequent releases, more features were added and PowerShell was included with Windows itself. A major feature of PowerShell was modules, which allowed the extending of capability. The Microsoft Exchange team was one of the first to make a module availbale for their product (Exchange 2007). This module was my first true exposure to PowerShell. This development path continued until version 5.1 and contained the hints of divergence that would come. Version 5.1 had two editions \u2013 Desktop and Core. Desktop was like prior versions, built on top of the .NET framework. The Core edition was built upon .NET Core (which could run on Linux). At this point, development of this line of PowerShell ceased.</p> <p>In 2018, PowerShell Core 6.0 was released, for Windows, macOS and Linux. Rather than replacing version 5.1, Core ran side-by-side. For the Windows system administrator, this version of PowerShell represented a step back. Many modules that had been created in the past simply didn\u2019t work on this new Core release. Many commands that relied on the full .NET stack also didn\u2019t work. In a roadmap posted in 2017, the PowerShell team started an intent to leverage .NET Standard as a method to allow these modules to work.</p> <p>Verison 6.1 included an effort to address the losses in 6.0, via the Windows Compatibility Pack for .NET Core. By including this Pack, version 6.1 was able to make more than 1,900 cmdlets available in 6.1. While this represents a major step in the right direction, I suspect that version 6.0 broke too many things for Windows system administrators. This is reflected in my own anecdotal experience. Many organisations I\u2019ve worked at still stuck with version 5.1. This is also backed up by Microsoft\u2019s own data about PowerShell, shown in the graph below:</p> <p></p> <p>Startups on Linux platforms represent the lion\u2019s share, with growth on Windows remaining very flat. This graph was front and center in a post by the PowerShell team April 2019, which discussed the \u201cnext release\u201d. The inability to run v5.1 code and modules on the 6.x was cited as a potential cause behind the lack of option on the Windows platform. The \u201cnext release\u201d was to be an effort to creating a \u201cfull replacement of Windows PowerShell 5.1\u201d.</p>","tags":["powershell"]},{"location":"2020/03/06/powershell-70--introduction-history-installation/#what-does-powershell-7-bring-to-the-table","title":"What Does PowerShell 7 Bring To The Table?","text":"<p>Looking at the release notes and features listed, PowerShell 7 brings some nice sounding items to the party. Some that caught my eye were:</p> <ul> <li>The return of .NET framework APIs (enabled by the shift to .NET Core 3.1, which should ease a lot of module compatibility issues)</li> <li>A \u201cproxy module\u201d capability to enable importing of modules that are still incompatible with version 7</li> <li>A change in support model, resulting in a longer support lifespan</li> <li>Parallel execution with the ForEach-Object cmdlet</li> </ul> <p>There\u2019s also a bunch of general optimisations that I\u2019m hoping will resolve some of PowerShell\u2019s more sluggish aspects.</p>","tags":["powershell"]},{"location":"2020/03/06/powershell-70--introduction-history-installation/#installing-powershell-7","title":"Installing PowerShell 7","text":"<p>For Windows systems, there\u2019s a MSI package that runs on Windows 7 SP1, 2008 R2 or later. Like version 6.x, it runs side-by-side with the 5.1 version, but will replace any existing 6.x installation on a system. In terms of deployment flexibility, this is perhaps one of best options as it can be folded into template building, during provisioning with configuration management tools, or exposed via software deployment tools like SCCM. The MSI installer starts with a standard welcome splash screen.</p> <p></p> <p>The installer prompts for some standard inputs we would expect, like an installation path. There\u2019s some additional options like adding PowerShell to the path variable and enabling PowerShell Remoting.</p> <p></p> <p>After these options, it\u2019s possible to start the installation.</p> <p>On some distributions and versions of Linux, it\u2019s possible to install PowerShell 7 relatively easily. In the case of CentOS 7, this process involves registering the Microsoft repository and then installing using the yum install command. Alternatively, it can be downloaded as an RPM (for CentOS/RHEL) and installed using that file. <pre><code># Download RPM File for CentOS 8\nwget https://github.com/PowerShell/PowerShell/releases/download/v7.0.0/powershell-7.0.0-1.centos.8.x86_64.rpm\n\n# Install using yum\nyum install powershell-7.0.0-1.centos.8.x86_64.rpm\n</code></pre></p>","tags":["powershell"]},{"location":"2020/03/06/powershell-70--introduction-history-installation/#summary","title":"Summary","text":"<p>So far, PowerShell 7 is hitting a lot of good notes. I\u2019m hoping to write some followups where I put it through its motions, specifically how well it supports PowerShell tasks that Windows system administrators may use and the performance improvements.</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/","title":"PowerShell 7.0 \u2013 Cmdlet Compatibility","text":"<p>In my first post about PowerShell 7.0, I made mention of a major issue that most likely prevent adoption of PowerShell 6.x by Windows system administrators. This issue was the loss of functionality in 6.x, specifically that some cmdlets would not be available due to 6.x being built on .NET Core. The release of 7.0 is meant to help address this, which is the intent of this post.</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#cmdlet-compatibility-approach","title":"Cmdlet Compatibility Approach","text":"<p>In reviewing the compatibility for 5.1 and 7.0, I used the management server in my home lab. This is a Windows Server 2019 virtual machine, so it shares the common set of cmdlets that Windows 10 has. As a management server, it also has some additional modules/cmdlets available:</p> <ul> <li>Windows Server Role-based modules such as ActiveDirectory and DnsServer</li> <li>SQLPS from the installation of SQL Server 2014</li> <li>cmdlets associated with VMware\u2019s PowerCLI</li> <li>cmdlets associated with ChefDK</li> <li>AWS\u2019s PowerShell module</li> </ul> <p>While not comprehensive, this does give good coverage of built-in, Microsoft-provided and third-party provided items.</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#core-powershell-cmdlets","title":"Core PowerShell Cmdlets","text":"<p>The first place to start would be the core cmdlets provided by PowerShell itself. These are derived from the Microsoft.PowerShell modules. A graph comparing the cmdlet counts for each module by version is shown below:</p> <p></p> <p>The numbers are the same for six of the modules. The higher count for Microsoft.PowerShell.Archive for version 7.0 comes from having a newer version of some cmdlets, resulting in duplication. The reason for version 5.1 Microsoft.PowerShell.Core being higher is due to some cmdlets not being available in version 7.0. These appear to be related to very old functionality (like adding/removing snapins). The Microsoft.PowerShell.Management module shows the greatest difference, with 89 cmdlets available in 5.1 compared to 61 in 7.0. Some of the missing cmdlets relate to creating System Restore Points. Others relate to WMI functionality. The increased count for version 7.0 cmdlets in the Microsoft.PowerShell.Utility module seem to be from items added, such as Join-String and Markdown-related cmdlets.</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#base-windows-cmdlets","title":"Base Windows Cmdlets","text":"<p>There are about 78 modules that enable almost 1,500 cmdlets in a basic installation of Windows Server 2019. For the sake of clarity, I\u2019ve only included those with different counts in the graph below.</p> <p></p> <p>As shown, there is hardly any version 7.0 functionality for the AppvClient module. The situation is worst for UEV, with no cmdlets available in version 7.0. However, there is documentation from Microsoft that suggests it does work in Windows 10 1903 or higher. The higher count for PowerShellGet is a similar situation to Microsoft.PowerShell.Archive \u2013 there are newer versions of the cmdlets. The same is also true for PSDesiredStateConfiguration and PSDiagnostics.</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#windows-server-role-cmdlets","title":"Windows Server Role Cmdlets","text":"<p>In my home lab, most of my servers are running Windows Server Core installations to reduce overhead. My management server has the various administration roles and features installed to allow administration of the other servers. This includes Active Directory, DHCP, DNS and IIS. Most of the modules had what appeared to be equal support under version 5.1 and version 7.0. The exceptions to this related to IIS.</p> <p></p> <p>As shown in the graph, the IISAdministration module has no cmdlets under 7.0. The WebAdministration has only two, relating to commitment of changes to IIS.</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#sql-server-2014-cmdlets","title":"SQL Server 2014 cmdlets","text":"<p>When SQL Server 2014 is installed, two PowerShell modules are available \u2013 SQLPS and SQLASCMDLETS. It appears the same amount of cmdlets is available in PowerShell 5.1 and 7.0</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#powercli-cmdlets","title":"PowerCLI cmdlets","text":"<p>PowerCLI is VMware\u2019s module for management of their vSphere products (generally via interacting with vCenter). It appears, like the SQL Server cmdlets, all of them are available under both versions of PowerShell.</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#chef-dk-cmdlets","title":"Chef DK cmdlets","text":"<p>The Chef Development Kit (DK) is a set of tools to assist in local development of cookbooks. Many of these tools relate to Test Kitchen, which is used to execute and test the cookbook code against a \u201cdev\u201d target system. There appears to be a few extra utility cmdlets exposed for version 7.0, giving a higher count. </p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#aws-powershell-cmdlets","title":"AWS PowerShell cmdlets","text":"<p>When I initially checked the AWS cmdlets for both versions, version 7.0 showed none at all. Upon checking the version I was running and what was available, there was a bit of a gap (v3.3.563.1 from 7 months ago vs 4.0.4.0 released a month ago). After updating, both versions showed equal numbers of cmdlets.</p>","tags":["powershell"]},{"location":"2020/03/09/powershell-70--cmdlet-compatibility/#summary-and-moving-forward","title":"Summary and Moving Forward","text":"<p>On the surface, cmdlet compatibility in PowerShell 7.0 seems to be strong. Earlier I mentioned documentation about support of the UEV cmdlets. Microsoft has a page detailing the compatibility of various modules. A good percentage are listed as \u201cNatively compatible\u201d which is a good sign. The VMware and AWS cmdlets being listed as available in 7.0 is a good sign as well.</p>","tags":["powershell"]},{"location":"2020/04/25/updating-to-vrealize-automation-76-hotfix-7/","title":"Updating to vRealize Automation 7.6 Hotfix 7","text":"<p>I had been burned by updating vRealize Automation a little too quickly following a hotfix release. Chrome 75 caused some rendering issues in the deployment forms. These issues were resolved by Hotfix 1, which introduced some extra issues. The most visible one is the duplicate requests on an XAAS (Anything as a Service) blueprint. An example of this behaviour is shown below</p> <p></p> <p>The second issue that I\u2019ve seen as cosmetic. It was resolved in Hotfix 2. In this issue, labels on an XAAS blueprint rendered correctly in the Designer but when requesting the blueprint via the catalog, the label text would wrap. An example of this is in the image below:</p> <p></p> <p>On the left is the layout in the Designer, while on the right is how it renders when making a request. Not a very good look.</p> <p>A key item introduced in Hotfix 5 was support for LDAP Signing and LDAP Channel Binding. These were changes that Microsoft made to improve the security of Active Directory. Given these and other changes, it seems like a safe option to update to the latest hotfix (hotfix 7)</p>","tags":["vrealize automation","home lab"]},{"location":"2020/04/25/updating-to-vrealize-automation-76-hotfix-7/#performing-the-update","title":"Performing the Update","text":"<p>Unlike other updates, this one isn\u2019t easily performed using the update function in the management interface. The process for installing this update is detailed in KB article 70911. For the sake of providing some visual context, I\u2019ve repeated the steps with screenshots. Prior to starting, I shutdown both the virtual appliance and IAAS VM, and took snapshots. I also made sure I met all the prerequisites.</p> <p>I logged into the management interface of the virtual appliance and navigated to the Patches section.</p> <p></p> <p>From the Patch Management screen, we need to upload the hotfix patch file that was downloaded. This file was about 4.2GB for me. To do the upload, in the Patch Management screen, click on the New Patch tab, then the Upload Patch button and select the file to start the upload.</p> <p></p> <p>As per the documentation, I had to switch to an SSH session after the patch upload to run a command. This is because my last hotfix version was version 1.</p> <p></p> <p>On my instance, this patch took about 7 minutes to run. I did run into an issue where the script threw an error. Rerunning it as per the documentation didn\u2019t seem to resolve this. On a whim, I renamed the patch file to .zip and I was able to open it with WinRar. Inside was an updated management agent installer. I installed this on my IAAS server and now the script executed successfully. Logging back into the management interface and going to Patch Management, there was now an option to run a precheck.</p> <p></p> <p>One issue I ran into with the Precheck is it errored out because the vRealize Business for Cloud (vRBC) appliance was powered off. So it does seem to do some checking outside of just the vRealize Automation appliance. Once I deregistered the rRBC instance, the prechecked passed. At this point, I clicked the Install button to install the Hotfix. Interestingly, the patch details had changed to HF2. On my instance, the patch took about 20 minutes to install. Following this, it did list HF2 as installed.</p> <p>I again uploaded the HF7 patch in an attempt to install it. Fortunately this time it went through properly.</p> <p></p>","tags":["vrealize automation","home lab"]},{"location":"2020/04/25/updating-to-vrealize-automation-76-hotfix-7/#post-install","title":"Post Install","text":"<p>After the hotfix was applied and I restarted my browser, the XAAS request form rendered in a much more pleasant fashion.</p> <p> Submitting an XAAS request yielded only one item in the Deployments area, which is the correct behaviour. So the hotfix was installed eventually.</p>","tags":["vrealize automation","home lab"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/","title":"vRealize Automation 8.1 New Features Walkthrough","text":"<p>VMware announced the general availability of vRealize Automation 8.1 a couple of weeks ago. This update includes a wide range of new features and capabilities. Some of these items restore functionality that was lost in transitioning from 7.x to 8.x (such as Approval Policies).</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#governance-and-policy","title":"Governance and Policy","text":"<p>Version 8.1 adds some new items under Governance and Policy. Some of these include Approval Policy, limits on resources and view-only roles.</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#approval-policy","title":"Approval Policy","text":"<p>Approval Policies have been expanded to be more in line with the functionality of what was in 7.x. In one of my first impressions posts about version 8.0, I noted there was only 2 policy types (Lease and Day 2 Actions). There is now a third option called simply Approval Policy.</p> <p></p> <p>The form that\u2019s presented when creating an Appproval Policy has a lot of content, as shown below. The initial scoping of the policy can be for the entire organisation or a particular project. It can also be further constrained in scope using the Deployment Criteria, such as a particular blueprint. The Auto Expiry Decision and Trigger are a nice touch, allowing things to keep moving if approvals haven\u2019t been granted in a timely fashion. The Actions section defines which actions the policy applies to. This can be deployment tasks such as create or delete, or more specific like creating a snapshot. There are 73 actions available.</p> <p></p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#resource-limits","title":"Resource Limits","text":"<p>When creating a new Project in 8.1, it is now possible to apply limits on the resources it can use. This is similar to how Reservations worked in 7.x The limits apply per Cloud Zone assigned to the Project. Storage limits can be set only for vSphere Cloud Zones. The UI for this is shown below.</p> <p></p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#view-only-roles","title":"View Only Roles","text":"<p>This release expands the default set of built in roles to include View-Only roles. With the exception of vRealize Orchestrator\u2019s roles, it\u2019s possible to assign a Viewer right to all the other services within vRA 8, including the migration assessment.</p> <p></p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#cmp","title":"CMP","text":"<p>Three items were added to CMP in this release \u2013 quick setup of vSphere endpoints, health badges and pricing for private cloud deployments.</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#vsphere-endpoint-quick-setup","title":"vSphere Endpoint Quick Setup","text":"<p>The main change in this area is the addition of Cloud Foundation as a Quickstart/Quick Setup option. When selecting the Quickstart, we are now presented with two options.</p> <p></p> <p>Selecting the Cloud Foundation option presents a form asking for the information relevant for that platform such as the SDDC Manager details, vCenter server and so on.</p> <p></p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#rate-cards","title":"Rate Cards","text":"<p>Rate or Pricing Cards allow pricing data to be associated with deployments. This concept is similar to what vRealize Automation 7.x had when integrated with vRealize Cloud for Business. In vRA 8, this functionality relies on integration with vRealize Operations. Pricing cards can be applied to Projects (the default) or Cloud Zones. The Basic Charges section overs CPU, Memory and Storage. It can be set on a \u201cCost\u201d model (a percentage of a base value) or at a \u201cRate\u201d model where specific values need to be provided. The Rate model UI is shown below.</p> <p></p> <p>As shown, the rate can be based on an hourly, daily, weekly or monthly model and can apply based on other criteria. There can also be a charge applied for the operating system. This can be a one-off charge, a rate factor linked to resources (CPU for example) or recurring.</p> <p></p> <p>Additional charges can be applied on the basis of tags. These follow a similar model to the OS charges (one time, recurring, etc). When clicking in the Tag field to add the charge item, it will automatically look up the tags defined in your vCenter. This could be used for various purposes, like charging more for high performance storage or a higher standard of backup routine.</p> <p></p> <p>The last charge option relates to Custom Properties. These are set using a Property Name and Value pairing and have the same sort of charging options as the other settings. Finally there is an overall charge that can be applied, with options to specify a one time charge and recurring. Once cards have been created, it\u2019s possible to calculate estimates prior to performing a deployment. A Daily Price Estimate element appears in the New Request form, with a button to calculate the rate.</p> <p></p> <p>Clicking on the Details link will display a price breakdown. As shown, it\u2019s not a comprehensive estimate because it doesn\u2019t include some charges.</p> <p></p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#xaas","title":"XAAS","text":"<p>vRealize Automation 8.1 adds a few new features for XAAS (Anything as a Service). Most of them relate to custom resources.</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#custom-resources-based-on-vro-types","title":"Custom Resources based on vRO Types","text":"<p>Custom Resources allow you to map a resource type from a vRealize Orchestrator (vRO) workflow into a vRA resource that can be used in Blueprints. The example in the vRA 8.1 documentation uses the AD:User vRO resource type. As shown below, the Custom Resource is associated with vRO workflow that will create or destroy the resource. In this case, the Create workflow is one that would create an AD user (vRO has a couple of workflows that achieve this already) and the Destroy would delete the user.</p> <p> When the workflows are added, the schema box on the right will update with the properties of the resource. This gives an idea of whether you\u2019ve picked the right workflow for what you\u2019re trying to achieve. Further down on the form, Day 2 Operations can be defined for the Custom Resource. In the case of the AD User example, \u201cChange password\u201d might be a valid option here. After creating the Custom Resource, it will appear as an item in the Blueprint Designer.</p> <p></p> <p>When the Blueprint is created and deployed, the appropriate inputs are displayed and we get some autocomplete functionality with the OU field.</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#custom-day-2-operations","title":"Custom Day 2 Operations","text":"<p>One of the most powerful features in vRealize Automation 7.x was the ability to make your own Day 2 Operations actions. One I\u2019ve written about was related to managing local administrators on a Windows server. In 8.1, this functionality makes a return. The form to create a new Resource Action has a lot of similar elements to the Custom Resource form. The relationship being defined this time is the Resource Type of the things we want to run this action on, as well as the workflow to execute. Once the workflow is selected, the Property Binding box on the right will populate.</p> <p></p> <p>The Property Binding takes the name of the resource (in this case, a VM) and finds the vCenter object that the Migration workflow needs as an input. This is the link between the various pieces to make the Resource Action work. The look and feel of the request form can be customised at this point. Conditions can also be set on the Action, constraining what scenarios it will be available. In this case of this example, the constraints could be related to CPU or memory or the role of the VM. Once all this configuration is done, the new Resource Action should appear when you drill down to an existing deployment.</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#other-custom-form-improvements","title":"Other Custom Form Improvements","text":"<p>One of the cool things that was possible in vRealize Orchestrator 7.x was to have the default value of one input field be calculated on the value of another. This behaviour would pass through to blueprints in vRA 7.x and could allow things like a basic cost calculator (ie. taking the CPU and memory values and performing some formula on it). The custom form designer in vRealize Automation 8.1 has this sort of functionality return. Like 7.x, it uses vRealize Orchestrator actions to achieve it. This is a feature I\u2019ve used in 7.x as a way of improving the quality of request forms that are presented to consumers and I\u2019m glad to see it make a return.</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#powershell-for-abx","title":"Powershell for ABX","text":"<p>Action Based Extensibility (ABX) was a new form of extending vRA functionality that was introduced in 8.0. In 7.x, this extending was performed by using vRO workflows. In the 8.0 release, ABX could be written in node.js or Python. In 8.1, they can now be written in Powershell. An important cavet with this is like the 8.0 languages, this Powershell also runs on AWS Lambda and is subject to the limitations and requirements of that platform.</p> <p></p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#iaas-provisioning","title":"IAAS Provisioning","text":"<p>IAAS Provisioning had the longest list of imrpovements added in 8.1. Some of them are support for new products (vSphere 7, NSX-T 3.0, etc). Users now have the option to perform a bulk deployment of a request. This has to be enabled first, as it defaults to a value of 1 per request. It can be set to a maximum of 10. If it is set to a value above 1, a new field is added to the request form, where the user can set an appropriate number of deployments.</p> <p></p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#ecosystem","title":"Ecosystem","text":"<p>Version 8.1 adds some updates with the integrations supported. Openshift is now an integration option, although at this stage I\u2019m not clear what functionality is granted by adding it. There is also a Terraform provider.</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#catalog-content-sources","title":"Catalog Content Sources","text":"<p>There\u2019s two new features in this area. First, VMware Marketplace templates can be published to the catalog. This involves adding the Marketplace as a content source</p> <p> The addition of the Marketplace is easy, just requiring a name and the \u201csource integration account\u201d (ie. a myVMware account). After that, like any other content source, the content needs to be shared with a project, so users in that project can deploy it.</p> <p> Because these are effectively OVAs being deployed, the catalog request form has the same sort of fields as you would expect. This can lead to request forms that are overly complex, but it is possible to customise the form.</p> <p>The second item added in this area is the ability to publish Code Stream pipelines as catalog items. This is another good option to extending the available options to users in the catalog.</p>","tags":["vrealize automation"]},{"location":"2020/04/29/vrealize-automation-81-new-features-walkthrough/#final-thoughts","title":"Final Thoughts","text":"<p>For a .1 release, there is a lot of new and improved stuff in 8.1. Some of the items I haven\u2019t covered. I think what I\u2019m most pleased about is the restoration of the features that were lost in 8.0, such as approval policies and day 2 tasks. The day 2 tasks especially where a great area for an IT department to be able to extend and grow self-service options to vRA users, so it\u2019s good to have that functionality back. With this release, vRA 8.1 does feel like a relatively mature product and it\u2019ll be interesting what 8.2 gives us.</p>","tags":["vrealize automation"]},{"location":"2020/05/10/home-lab-expansion/","title":"Home lab expansion","text":"<p>For a while now I\u2019ve been hitting the capacity limit of the single server in my home lab. This is the nature of running a lot of VMware\u2019s more recent products on it. The plan had been to get another server, shared storage and a larger switch. This week I was able to bring it all together. There were some problems.</p>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#what-i-already-had","title":"What I already had","text":"<p>The original setup I had included the following items:</p> <ul> <li>1 Dell R710 with a Samsung 970 Evo Plus M.2 SSD (in a M.2 to PCIe adapter), a 128GB SATA SDD and a 1TB SATA SSD</li> <li>1 5-port Gigabit switch</li> <li>1 5-outlet power board</li> </ul>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#the-second-server","title":"The Second Server","text":"<p>For the second server, I tried to get something that was as close to my current one as possible. This meant another Dell R710 with a similar model of CPU and about 128GB of RAM. Fortunately I was able to easily find one for sale on ebay. It came with 4 x 300GB SAS drives. I ended up moving 2 of these to the original server and creating a RAID 1 on each server.</p>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#shared-storage","title":"Shared Storage","text":"<p>The primary concern for the storage was capacity and some level of redundancy over rare performance. Both the original server and my PC have SSD storage, which is limited in capacity. My old home lab had a Synology 8-bay NAS but I could never get the iSCSI feature working. I decided on a Synology DS918+ which is a 4 bay NAS. Historically I\u2019ve used Western Digital for hard drives but recently there\u2019s been a bit of bad press about them. Instead I went for Seagate\u2019s Ironwolf in a 4TB capacity.</p> <p>It was with these drives that I ran into my first problem with this setup. One of the drives was dead out of the box. Trying to source a replacement turned into a bit of a drama but I was able to get one and get the NAS up and running. Another task I did was take the Samsung 970 Evo Plus that had been in the first server and put it in the Synology as a SSD cache. Hopefully this will help with performance.</p>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#networking","title":"Networking","text":"<p>I had previously been running an 8-port Gigabit switch in my home lab. I wanted to be able to populate all the network interfaces on all the devices, which meant getting a larger switch. This again caused a bit of drama. However I was able to get one and get all the ports hooked up.</p>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#power","title":"Power","text":"<p>Originally I had been using a basic 5 output power board to run the home lab. It meant that only one power supply for the servers was connected. My desired intent was to get something with more outlets and surge protection. I managed to get a 10 outlet board to do this.</p>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#thoughts-on-the-home-lab-procurement-process","title":"Thoughts on the home lab procurement process","text":"<p>A couple of issues that happened when getting all these pieces left a bad taste in my mouth. I made the order for the NAS and hard drives on Monday morning at a store in the same metro area as me, yet it took until Friday to arrive. This was after paying for \u201cexpress shipping\u201d. Part of this delay was potentially caused by the fact that my order sat in a \u201cprocessing\u201d state for at least a full 24 hours.</p> <p>At the point I found out I had a dead hard disk, I still needed to get a switch, so I figured I would order the switch and another hard disk. This time I would collect it in person. This time it took 15 hours to clear. As a counterpoint, the same day I did an online order with another store. The time from order confirmation to pickup confirmation was under an hour. So I think it\u2019s obvious who I\u2019ll be using next time.</p> <p>The other issue I ran into was when I called the first shop about the dead hard disk. This disk clicked quite loudly when plugged in, a good sign it\u2019s dead. Hard disks are DOA all the time. And in this case, we\u2019re talking a $180 inventory item, not a lot. Instead of offering on the spot replacement or a refund, the shop wanted to put me through an onerous process. They wanted the hard disk in its original package, which is just an anti-static bag. While this isn\u2019t difficult to cover in this case, I\u2019m sure this is actually a violation of Australian consumer law. Additionally, they wanted to \u201ctest\u201d it to verify it was broken, then send it back to the vendor. This potentially could take weeks and still leave me short a hard disk.</p>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#connecting-storage-to-servers","title":"Connecting Storage to Servers","text":"<p>I decided to try using both storage protocol options. Part of this is because it\u2019s easier to transfer files to an NFS share from my PC (because it\u2019s also exposed as SMB). The other part was I had significant issues with iSCSI when I used it on my old NAS. The Synology iSCSI implementation can support VAAI (vStorage API for Array Integration). VAAI allows VMware to \u201coffload\u201d certain tasks to the storage device. These can be tasks like cloning. After connecting the servers to the iSCSI LUN, it was possible to see whether it was supported or not. To do this, I used the command esxcli storage core device vaai status get. The results for the iSCSI LUN are shown below.</p> <p></p> <p>For block storage like iSCSI, there are 4 so-called \u201cprimitives\u201d available:</p> <ul> <li>ATS \u2013 Atomic Test &amp; Set. Generally there is more than one host accessing the same iSCSI LUN, so there needs to be a way of ensuring 2 hosts aren\u2019t accessing files at the same time (if they were, it could lead to corruption). This creates the need for a \u201clocking mechanism\u201d to signal when a host is manipulating a file. ATS is a form of this locking. In earlier releases of vSphere, SCSI reservations were used, which used LUN-level locking. ATS is more granular and according to VMware, helped address contention issues that happened with SCSI reservations</li> <li>Clone \u2013 In the official documentation, this is called XCOPY (Extended Copy). If XCOPY isn\u2019t available, then something like a clone operation is run through the Data Mover driver, which consumes resources on the host. When XCOPY is available, the array can perform a full copy in a more efficient manner.</li> <li>Zero \u2013 Also called Write Same in VMware\u2019s documentation. It\u2019s leveraged when performing an \u201ceager zeroing\u201d of a VMDK. In the zeroing process, the storage for a VMDK is allocated and then zeroes is written to it. Like the clone operation, if VAAI isn\u2019t available, it consumes resources on the host and for large disks, can be incredibly time consuming. If VAAI is available and this primative supported, this task can be offloaded to the array.</li> <li>Delete \u2013 This last item relates to space reclaimation. If the LUN is capable of handling SCSI UNMAP commands, then it will list this as supported. In the case of the LUN in the screenshot above, I didn\u2019t enable this.</li> </ul>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#storage-testing","title":"Storage Testing","text":"<p>To test whether this works, I created a Windows 2019 VM using packer and had it situated on the appropriate datastore. The intent was to clone it. When viewing the relevant fields in esxtop, it\u2019s possible to see VAAI activity is happening.</p> <p></p> <p>Before the clone, the clone_rd and clone_wr were zero. After the clone there\u2019s a change in the numbers. I\u2019d also say it seemed to do the clone very quickly. The original VM had a 50GB thin provisioned VMDK, taking about 9GB of space. The clone took about 30 seconds. This represents a speed of about 300MB/sec, which vastly exceeds what is possible over the ethernet interface of the NAS. This suggests the operation was performed entirely on the NAS. The lack of network traffic by the host during this time appears to back this up.</p> <p>As a followup to this, I storage vMotioned the VM around to get the VMDK to be thick provisioned. So now it was taking up 50GB of space. I performed a clone and it was done in seconds. I did it a 2nd and 3rd time. Each time it took 3-5 seconds. Both test scenarios (thin and thick provisioned disks) are the sort of performance I wanted to get out of my old NAS but never could. In my old home lab, I was waiting up to 5 minutes for these sort of cloning operations.</p>","tags":["home lab"]},{"location":"2020/05/10/home-lab-expansion/#summing-up","title":"Summing Up","text":"<p>After a little too much drama, I\u2019ve got my home lab expanded and at a state I\u2019m happy with. Before I started this process, I had to delete some VMs I deemed not critical (or at least, ones that would be relatively easy to setup again) to free up space. The next phase will be provisioning those servers again.</p>","tags":["home lab"]},{"location":"2020/05/14/an-introduction-to-packer/","title":"An Introduction to Packer","text":"<p>Recently I had an extended discussion about Packer with a fellow IT professional and recalled some material I had previously put together about Packer. Looking back at this material, I thought an expanded version of it might be worth writing up, with Windows Sysadmins in mind (since that\u2019s my background).</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#what-problem-does-it-solve","title":"What Problem Does It Solve?","text":"<p>I feel sometimes it\u2019s best to start with what problem we are trying to solve, to help provide context for how a tool might be useful. A number of user stories can capture the problems Packer can solve.</p> <p><code>As an Infrastructure Engineer, I want standardised templates for multiple platforms, so that template management is easier and consistent</code></p> <p>This talks about a task that many infrastructure engineers and systems administrators have had to perform \u2013 regularly updating virtual machine templates on vCenter and other platforms. In some cases, this can be very onerous, so we want an easier way. In other cases, where the templates aren\u2019t maintained, it can lead to increased deployment times (because the \u201cInstall Windows Updates\u201d task during the SCCM task sequence takes longer and longer).</p> <p><code>As a Security Engineer, I want our infrastructure across multiple platforms to adhere to the same security standards, so that my organisation\u2019s data and reputation are protected</code></p> <p>The Security Engineer user story is about ensuring things are the same. A server in the cloud should have the same sort of security settings as a server hosted on prem. This is really a very focused view on configuration management (focused on just security) but there\u2019s no reason why we can\u2019t also have a broader view on that. If our templates are always running the latest updates and configured securely (via some magical process), then our Security Engineers are happy.</p> <p><code>As an Infrastructure Engineer, I want infrastructure that deploys rapidly, so it is available for use by my customers</code></p> <p>This third user story relates to the \u201cbaked in\u201d outputs that Packer can do. Across these three stories, the main points are consistency across platforms.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#what-is-packer","title":"What Is Packer?","text":"<p>This is answered on the first page of the Packer documentation. <code>Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration.</code> Expanding from this, Packer uses configurations defined in JSON format, with distinct blocks. These files are called Templates. This makes Packer part of the ever growing suite of Infrastructure As Code (IAC) tools out there. This means you get the benefits that come with the ecosystem that typically surrounds IAC tools, such as version control and testing (via CI/CD tools).</p> <p>The machine images that Packer creates can be VMware templates, VirtualBox box files, EC2 AMIs and more. The process of building these images involves taking some sort of source (sometimes an ISO, sometimes an existing machine), applying configuration to it and outputting an artifact of some sort (the image).</p> <p>As a Windows systems administrator, you may already have an image building process via SCCM. But generally that build process is only available for on-prem platforms (ie. vSphere), doesn\u2019t have the benefits of IAC tools and so on. Packer can potentially open up a range of opportunities for image creation and maintenance.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#components-of-packer","title":"Components of Packer","text":"<p>The template file that Packer uses has a number of components. These are Builders, Provisioners and Post-Processors.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#builders","title":"Builders","text":"<p>Builders are the platform the actual creation of the image takes place. This can be cloud providers like AWS or Azure. It can also be traditional hypervisors (like VMware vSphere, Virtualbox and Hyper-V). Generally the builder used will dictate what kind of output will be created.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#provisioners","title":"Provisioners","text":"<p>Provisioners are what performs the task of configuration management on the machine image being created. These could be something as simple as installing the latest security updates for the operating system. Configuration management tools like Ansible, Chef and Puppet can be used as a Provisioner, allowing you to leverage existing resources. What the Provisioner does can be as simple or as complex as you need.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#post-processors","title":"Post-Processors","text":"<p>Post-processors perform certain tasks after the build has completed. These tasks might be uploading the image to vSphere, generating Vagrant box files or importing into AWS.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#the-packer-workflow","title":"The Packer Workflow","text":"<p>The Packer workflow takes a defined source (usually an operating system installation media in ISO format or an existing machine), creates a machine on the builder, executes the Provisioner\u2019s defined tasks and generates an output. Doing this on Hyper-V may look like what is shown below.</p> <p></p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#the-template-file-format","title":"The Template File Format","text":"<p>The Template File has sections that match the Packer components, plus a few others.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#builders-section","title":"Builders Section","text":"<p>This section is where one or more builders can be defined. So it would be possible to build an EC2 AMI and a VMware vSphere VM template using the same template file. A partial example of how this could look is shown below. <pre><code>\"builders\": [{\n  \"type\": \"virtualbox-iso\",\n  \"vm_name\": \"windos-2019\",\n  \"output_directory\": \"e:/Packer/windows-2019-base-{{isotime \\\"20060102\\\"}}.{{isotime \\\"150405\\\"}}/\",\n  \"guest_os_type\": \"Windows2016_64\",\n  \"iso_url\": \"{{user `iso_url`}}\",\n  \"iso_checksum\": \"{{user `iso_checksum`}}\",\n  \"iso_checksum_type\": \"{{user `iso_checksum_type`}}\",\n  \"disk_size\": \"{{user `disk_size`}}\"\n}]\n</code></pre></p> <p>In this example, the type is being defined first (in this case, a VirtualBox VM that will start with an ISO file). Some values are dependant on the Builder platform used, such as guest_os_type which will have different values for different platforms. The second half of the sessions refer to variables (as indicated by the {{user 'disk_size'}} entry for example. Variables are useful in allowing you to define values you may use multiple times or that contain sensitive data.</p> <p>A key section that is defined in the Builders section is the communicator. This defines the transport that Packer will use to transfer files to the machine or execute commands. The two main options are SSH and WinRM. This section will always define the type of communicator and a username/password combination for it.</p> <p>The Builder section will also define characteristics about the machine being built, and these settings will vary from platform to platform. For example, if building on a VMware server, it\u2019s possible to set the network card to VMXNET3.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#provisioners-section","title":"Provisioners Section","text":"<p>The Provisioners section define the commands or tools that will be used to configure the machine while it is built. This section can be left empty, resulting in an image that has just the operating system installed. The Provisioner code structure is a lot more simple, in many cases, being just 2 lines \u2013 defining the type and then defining what will run. A brief example of this is shown below. <pre><code>\"provisioners\": [{\n  \"type\": \"powershell\",\n  \"script\": \"scripts/windows/install-vbox-guest-additions.ps1\",\n}]\n</code></pre> In this example, the type is being set to PowerShell and the thing to be run is a script that installs the Virtualbox guest additions. A common set of items that might be in the Provisioners section would be to install tools like the guest additions, install security updates, reboot the machine and perform cleanup tasks.</p> <p>As mentioned previously, configuration management tools like Ansible, Chef and Puppet can be used in this section, allowing you to create \u201cbaked in\u201d images that already have applications installed and configured.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#post-processors-section","title":"Post Processors Section","text":"<p>This section defines the tasks to be performed after the image is created. In some cases, there may be nothing to put in this section. One scenario there there may be content is when the desired output is a \u201cbox\u201d file. An example of this is below. <pre><code>\"post-processors\": [\n  [{\n    \"type\": \"vagrant\",\n    \"vagrantfile_template\": \"vagrant-templates/centos-7.template\",\n    \"output\": \"./boxes/centos-7-base-{{isotime \\\"20060102150405\\\"}}.box\"\n  }]\n]\n</code></pre> Like the other sections, it follow the same general format of defining the type and then some values for it. In this case, the type is vagrant, with the options of what template file to use and what the output should be.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#variables-section","title":"Variables Section","text":"<p>In some of the earlier example snippets, there were placeholder values that referenced variables (designated by the {{ }} notation). The real values for these are defined in the variables section or they can be specified via the command line. An example of this is shown below. <pre><code>\"variables\": {\n  \"http_directory\": \"kickstart/centos7\",\n  \"kickstart\": \"ks.cfg\",\n  \"iso_url\": \"iso/CentOS-7-x86_64-Minimal-1804.iso\",\n  \"iso_checksum_type\": \"md5\",\n  \"iso_checksum\": \"fabdc67ff3a1674a489953effa285dfd\",\n  \"cpu_count\": \"2\",\n  \"memory_size\": \"4096\",\n  \"disk_size\": \"40960\",\n  \"headless\": \"false\",\n  \"skipexport\": \"false\",\n  \"ssh_user\": \"vagrant\",\n  \"ssh_pass\": \"vagrant\"\n}\n</code></pre> In this case, some values being defined relate to the source media (iso_url for example) while others will affect the configuration of the machine being built (disk_size). If multiple Builders are defined in the template, then variables can help with maintenance of that file by keeping values like these set in one place. Towards the end, the example is doing something which is perhaps not a good practice \u2013 listing a password in the clear. One approach is to ensure this user account is deleted when provisioning a machine from the templates you build with Packer. Alternatively, you could insert the password into the build process using a more appropriate method (like retrieving it from a secrets management system).</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#packer-in-action","title":"Packer In Action","text":"<p>At this point, I\u2019ve covered the use cases for Packer and the pieces needed in a template to build an image. So the next item is what the experience of using it is like. A while ago I made a video of building a VirtualBox image for demo purposes. </p> <p>Right at the start, the contents of the template file is being displayed. This includes some of the settings I\u2019ve written about earlier, plus a bunch of others. Because this is a Windows 2012 OS being installed, an unattend.xml file can be fed in the help with some tasks, and this is listed under the floppy_files item. Some key timeline events are:</p> <ul> <li>At 0:16, the command to run is visible.</li> <li>At 0:33, we can see the Virtualbox VM is booting like a normal VM would. It boots off the CD-ROM</li> <li>At 0:42, Packer enters a waiting state, waiting for WinRM to be available so it can process the Provisioner commands From about 0:56 to 1:29 we see a very normal Windows install process, automated thanks to the values in the unattend file</li> <li>At 1:41, the VM has reloaded and autologged in (as per settings in the unattend file) to run the script to enable WinRM</li> <li>At 1:53, we can see Packer progressing again because WinRM is avaialble. The first thing it does is upload the VirtualBox guest additions ISO. Meanwhile inside the VM, I\u2019m loading Task Manager so the activity happening inside the VM is visible</li> <li>At 2:11, when switching to the Performance tab in Task Manager, under Ethernet, the VM is receiving traffic (about 10Mbps) which is the ISO transfer</li> <li>At 2:51, the transfer is finished and now Provisioners are being run. The first one is a PowerShell script to install the guest additions. In Task Manager, we can see entries for this, running under the Vagrant account</li> <li>At 3:10, the Provisioning has finished and the VM is shutting down.</li> <li>At 3:31, the shutdown is finished and Packer starts cleaning up the Virtual Machine. This includes tasks like removing the floppy drive. Following that, it begins to export the Virtual Machine (to OVF format in this case)</li> <li>At 3:39, the export is complete and Packer finishes by removing the VM from Virtualbox.</li> <li>After that, I change into the output directory to show the created files \u2013 the OVF and the VMDK</li> </ul>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#extending-packer","title":"Extending Packer","text":"<p>Given that Packer can perform provisioning tasks based on existing configuration management tools like Ansible, Chef and Puppet, it\u2019s possible to create what some call \u201cbaked-in images\u201d. I\u2019ve used this term a couple of times earlier in this piece and it addresses the 3rd use case \u2013 getting things up and running fast. Some applications, like SQL Server, can take a long time to install. In the case of a bare OS image, the time to deploy the OS is shortened (because we\u2019re just cloning the image, which can be very fast) but still installing the applications traditionally. If the applications are \u201cbaked-in\u201d, then we are cloning a slightly larger image. This can be useful for developers who require a lot of tools be available in systems they\u2019re developing and testing on. The down side is an increase in management overhead \u2013 more configuration templates to manage, more artifacts to manage.</p> <p>Another option for extending Packer is making it part of a CI/CD pipeline. Firstly this allows the automation of your image building, but also allows additional steps in the process to enhance the process. These steps could be running some sort of testing or validation over the image, or transferring it to various destinations.</p>","tags":["packer"]},{"location":"2020/05/14/an-introduction-to-packer/#summing-up","title":"Summing Up","text":"<p>I\u2019ve touched on some of the basic concepts about what Packer does and how it works. It can be used to build a range of operating systems for different platforms. In one case, I used it to build Windows images for Nutanix\u2019s AHV platform. Once you have a set of template files established, it can help ease that task of regularly updating the virtual machine images in your organisation or home lab.</p>","tags":["packer"]},{"location":"2020/05/27/installing-ansible-awx-on-centos-8/","title":"Installing Ansible AWX on CentOS 8","text":"<p>AWX is a web application that sits on top of Ansible, providing a \u201cuser interface, REST API and task engine for Ansible\u201d. Since AWX can integrate into vRealize Automation, I decided to stand up an instance of it in my home lab.</p>","tags":["automation","ansible","awx"]},{"location":"2020/05/27/installing-ansible-awx-on-centos-8/#installing-ansible","title":"Installing Ansible","text":"<p>Since I was starting with a complete fresh CentOS 8 system, the first thing I needed to do was install Ansible. Unfortunately, Ansible is not available in the default repos configured in CentOS 8, so the repo for it needs to be added first.</p> <pre><code># Add EPEL repo\nyum install epel-release\n# Install Ansible\nyum install ansible\n# Confirm installation and version\nansible --version\n</code></pre>","tags":["automation","ansible","awx"]},{"location":"2020/05/27/installing-ansible-awx-on-centos-8/#installing-docker","title":"Installing Docker","text":"<p>AWX comes in a containerised form that can run on Docker, Kubernetes or OpenShift. If you have the latter 2 already in place, it may be easier to leverage them. Since I had nothing in place and Docker seemed to be the easiest to get up and running, I decided to use it. One issue I ran into with this part is the Docker installation seems to fail due to some version issue between the docker-ce package and the containerd package. To get around this, I installed containerd separately first as per this issue thread and Docker installed properly after that. <pre><code># Disable firewalld\nsystemctl disable firewalld\n# Stop firewalld\nsystemctl stop firewalld\n# Install Yum utils\nyum install yum-utils\n# Add the docker repo to yum\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n# Install containerd\nyum install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm\n# Install docker CE\nyum install docker-ce docker-ce-cli\n# Start docker service\nsystemctl start docker\n# Enable service to start after reboots\nsystemctl enable docker\n# Confirm version and operational status\ndocker version\n# Install docker-compose python module\npip3 install docker-compose\n</code></pre> Another point where I tripped up in my first run was assuming that docker would automatically be running after installation. However, it needs to be explicitly started, as noted by the step that starts the docker service.</p>","tags":["automation","ansible","awx"]},{"location":"2020/05/27/installing-ansible-awx-on-centos-8/#other-prerequisite-items","title":"Other Prerequisite Items","text":"<p>A few utility items are also required before AWX can be installed, as shown below. <pre><code># Install make\nyum install make\n# Install git\nyum install git\n</code></pre></p>","tags":["automation","ansible","awx"]},{"location":"2020/05/27/installing-ansible-awx-on-centos-8/#get-and-run-awx-installer","title":"Get and Run AWX Installer","text":"<p>At this point, it\u2019s now possible to acquire the installer for AWX. This is done using git to clone the code repo and run the installer. At the time of writing, 11.2.0 is the latest release, so I used that. There\u2019s a lot of settings that can be configured, but for a home lab deployment, it seems the defaults work well enough. <pre><code># git clone repo\ngit clone -b 11.2.0 https://github.com/ansible/awx.git\n# Change into repo folder\ncd awx\n# Change to installer folder\ncd installer\n# Run installer\nansible-playbook -i inventory install.yml\n</code></pre> The installer will show output like one would expect from any other Ansible playbook being run. It will sit for a while on the step for starting the containers.</p> <p></p> <p>During this stage, it\u2019s possible to see an increase in CPU, disk and network activity during this period. This took about 5 minutes on my test VM. Following installation, it\u2019s possible to confirm the containers exist using docker ps.</p> <p></p>","tags":["automation","ansible","awx"]},{"location":"2020/05/27/installing-ansible-awx-on-centos-8/#post-installation","title":"Post Installation","text":"<p>After the playbook has finished running, the awx_task container will run various setup tasks. The web interface won\u2019t be available until these finish. You can track the progress by viewing the logs using the command docker logs -f awx_task.</p> <p>It was during this phase that I ran into issues during my first run. Firstly, I had installed postgresql normally, since the documentation mentioned it as a prerequisite. This seems to have caused some conflict with the postgres container that\u2019s deployed since the awx_task phase would fail, with errors relating to connection.</p> <p>Another issue I ran into was when I disabled the firewall, thinking that was causing issues. This seemed to cause more problems, like preventing the playbook from running when it tried to create an iptables rule. To deal with this, I\u2019ve placed steps at the start to disable the firewall service, which seems to get around this (although not a desirable situation).</p> <p>Once the default organisation and related items are created, it\u2019s possible to login to the AWX interface.</p> <p></p> <p>At this point, it\u2019s possible to login to the AWX web interface and progress with adding inventory, credentials and other items to use it for performing Ansible tasks.</p>","tags":["automation","ansible","awx"]},{"location":"2020/06/18/vrealize-automation-8--cloudconfig/","title":"vRealize Automation 8 \u2013 cloudConfig","text":"<p>With the increased focus on cloud-based services in vRealize Automation (vRA) 8, VMware have added a lot of new features. One of the key ones for Infrastructure As A Service (IAAS) provisioning is initialising a machine via \u201ccloudConfig\u201d.</p>","tags":["vrealize automation"]},{"location":"2020/06/18/vrealize-automation-8--cloudconfig/#how-we-used-to-do-it","title":"How We Used To Do It","text":"<p>Historically, when provisioning a Virtual Machine (VM) either via vRA or directly via vCenter, we would use a Customisation Specification. These were files that controlled certain settings when a VM booted for the first time, such as the administrator password.</p> <p></p> <p>In AWS, Userdata scripts were used to perform similar tasks. This was executed via the EC2Config service/agent that was installed on the AMI templates that were used for deploying EC2 instances. Azure has similar functionality.</p>","tags":["vrealize automation"]},{"location":"2020/06/18/vrealize-automation-8--cloudconfig/#cloudconfig-and-cloud-init","title":"cloudConfig and Cloud-init","text":"<p>vRA 8 introduces a set of \u201ccloud agnostic\u201d Blueprint items, which can be placed in a Blueprint regardless of the ultimate target platform. To perform the same sort of boot-time configuration across different target platforms, the Blueprint has a cloudConfig section where these actions can be performed. The commands used in this section follow the [cloud-init standard(https://cloud-init.io/)]. Cloud-init was originally developed for the Ubuntu distribution of Linux on AWS, so a lot of the settings and behaviour will feel familiar to AWS users..</p> <p>These commands can include tasks such as setting the host name of the system, installing software, creating/editing users or execute scripts.</p>","tags":["vrealize automation"]},{"location":"2020/06/18/vrealize-automation-8--cloudconfig/#cloud-init-and-windows-support","title":"Cloud-init and Windows Support","text":"<p>Due to cloud-init\u2019s origins, its support on Linux platforms is very well established. There are packages for most major Linux distributions such as Red Hat and Suse. In the case of distributions like CentOS, cloud-init is available via one of the default repositories. Fortunately a company called CloudBase Solutions has ported cloud-init to Windows and called it Cloudbase-init. Because of the nature of what cloud-init is, it can be easily integrated into your template creation process as another step.</p>","tags":["vrealize automation"]},{"location":"2020/06/18/vrealize-automation-8--cloudconfig/#creating-a-vsphere-template-with-cloudbase-init","title":"Creating a vSphere Template with Cloudbase-init","text":"<p>Creating a Windows-based template that can be used in cloud-agnostic blueprints is relatively easy. I\u2019ve used an existing Windows 2019 template as a starting point, deploying a new Virtual Machine from it. When the Virtual Machine is up and running, download the Cloudbase-init installer. These are linked on the Cloudbase-init Github repository site or on the Downloads page. A key point about the installer: since this is a vSphere tempate, the OvfService metadata provider is used. This means the installer needs to be version 0.9.12.dev72 or later. At the time of writing, the latest \u201cstable\u201d release is is v1.1.1 and the latest \u201cbeta\u201d is 1.1.2dev5. This version concern is less of an issue on cloud platforms as the metadata providers (at least in AWS\u2019s case) are available in the stable version.</p> <p>The installer can be run in standard interactive fashion or with parameters for a silent installation process. When run interactively, a standard MSI installer introduction screen is presented. </p> <p>The installer can be progressed in a fashion with default values. The only area that needs changing is the Configuration options screen. The Username will default to \u201cAdmin\u201d and needs to be changed to \u201cAdministrator\u201d. The \u201cRun Cloudbase-init service as LocalSystem\u201d checkbox also needs to be ticked.</p> <p></p> <p>The install wizard can be continued until the point where installation actually happens. Once installation is complete, leave the installer on the final confirmation screen. The configuration files need to be edited before Virtual Machine is shutdown. The configuration files will be located in C:\\Program Files\\Cloudbase Solutions\\Cloudbase-Init\\conf. The first file to change is the cloudbase-init-unattend.conf file. The metadata_services value needs to be changed to use the OvfService value. The final content is shown below.</p> <p>The other configuration file cloudbase-init.conf also needs to be edited. Firstly, the metadata_service value needs to be set, using OvfService as per the first file. Next, first_logon_behaviour needs to be set. Lastly, the plugins value needs to be set. The final content is below.</p> <p>At this point, it\u2019s possible to return to the Cloudbase-init installer and finalise it. Check the two available checkboxes and click Finished. This will sysprep the virtual machine and shut it down.</p> <p></p> <p>Once the Virtual Machine has shut down, convert it to a Template in vCenter.</p>","tags":["vrealize automation"]},{"location":"2020/06/18/vrealize-automation-8--cloudconfig/#deploying-the-template","title":"Deploying the Template","text":"<p>Now that the Template has been created, some preliminary tasks need to be performed to deploy it. Firstly, you may need to force an update of the template on vRA 8. By default, vRA 8 checks for images on a vCenter instance every 24 hours. This process can be manually triggered by going into the vCenter Cloud Account entry and clicking on the Sync Images button.</p> <p></p> <p>After this, a new Image Mapping needs to be created. Because of the manual image sync, the image should be available in the available options.</p> <p></p> <p>At this point, it\u2019s possible to create a new Blueprint and add a Cloud Agnostic Machine to it. The default YAML code generated by placing the component in the Blueprint isn\u2019t complete, so some extra values need to be added. Image and Flavor are two easy ones. Using the remoteAccess properties, a username and password can be set, which are exposed to the OvfService metadata service that was configured earlier. These values are used to create users or set the password of the specified user. After this, we can specify the cloudConfig property to perform some tasks. An example of this is shown below.</p> <p>At this point, it\u2019s possible to click Deploy to attempt to deploy the Blueprint. In vCenter, you should see tasks relating to the new Virtual Machine being created.</p> <p></p> <p>Once the deployment is complete, it\u2019s time to verify what was done during the process. The first task in doing this is to login with the credentials specified in the blueprint. The Virtual Machine will accept these credentials (indicating they were successfully set) but then immediately prompt for the password to be changed. This behaviour is a reflection of the first_logon_behaviour value that was set in the Cloudbase-init configuration files.</p> <p></p> <p>The other items like the test file and the custom host name should also have been set as expected.</p>","tags":["vrealize automation"]},{"location":"2020/06/22/bulk-add-flavor-mappings-using-vra-8-rest-api/","title":"Bulk Add Flavor Mappings Using vRA 8 REST API","text":"<p>One of the features added in vRealize Automation 8 (vRA 8) was Flavor Mappings. Flavor Mappings allow various instance types on different cloud providers to be associated with a platform-agnostic label. While it was possible to do something similar in vRA 7, it required a lot of scripting to handle the logic of the choice made. Like many of VMware\u2019s newer products, vRA 8 has a REST API for executing most tasks, and this includes management of Flavor Mappings. Because adding these in bulk can be tedious, I looked at how it might be done with a bit of automation.</p>","tags":["powershell","rest api","vrealize automation"]},{"location":"2020/06/22/bulk-add-flavor-mappings-using-vra-8-rest-api/#workflow-overview","title":"Workflow Overview","text":"<p>The vRealize Automation 8.1 API Programming Guide is a good starting point for looking at automating tasks in vRA 8. It has the steps relating to getting authentication done, as well as some general administrative tasks. In the case of what I was trying to achieve, the general workflow looks like this: </p> <p>The authentication steps can vary depending on the version of vRA 8 in use. In 8.0, authentication could be performed using just the Get API Token step (which involves the Identity Service API), or the two steps shown in the diagram. In subsequent versions, like 8.0.1 and 8.1, the two step process is the only method available.</p> <p>Most of the steps are simply REST API calls. Constructing the Flavor Mapping payload requires some conditional logic because the format for vSphere mappings differs from other cloud providers. While other cloud providers will simply accept the named value for their instance type (i.e t2.small for AWS), vSphere mappings need to explicitly state values for CPU and memory.</p>","tags":["powershell","rest api","vrealize automation"]},{"location":"2020/06/22/bulk-add-flavor-mappings-using-vra-8-rest-api/#proof-of-concept-script","title":"Proof of Concept Script","text":"<p>To test out these ideas, I wrote a script in Powershell. It\u2019s available at https://github.com/jpboyce/powershell-library/blob/master/vra8/add-flavormappings.ps1 Most of the steps in workflow diagram have been encapsulated as functions within the script. It has three required input values \u2013 the vRA 8 server name/IP address, a username and a password.</p> <p>The script references a data file, cloudInstanceTypes, which is a CSV file mapping the friendly name for the Flavor Mappings to values for vSphere, AWS, Azure and Google Cloud Platform (GCP). As shown with the sample data I\u2019ve used, there\u2019s explicit values for vSphere CPU and RAM, while the cloud providers use the appropriate named instance types respectively.</p> <p></p> <p>The result of running the script is 7 Flavor Mappings, like in the data file.</p> <p></p> <p>The Nano Flavor Mapping correctly has only 3 Account/Region settings because there\u2019s no sizing entry for it on GCP. Reviewing the Mappings can validate they have the correct values. </p>","tags":["powershell","rest api","vrealize automation"]},{"location":"2020/06/22/bulk-add-flavor-mappings-using-vra-8-rest-api/#functionality-in-blueprints","title":"Functionality in Blueprints","text":"<p>After the Mappings have been added, I can go into a Blueprint and alter the flavor value to one of the seven available options, as shown in the screenshot below.</p> <p></p>","tags":["powershell","rest api","vrealize automation"]},{"location":"2020/06/22/bulk-add-flavor-mappings-using-vra-8-rest-api/#conclusion","title":"Conclusion","text":"<p>Using this approach is a good way of doing bulk configuration tasks in vRA 8, especially if you\u2019re supporting more than one instance. Another reason to look at this approach is API rate limiting by the cloud providers. When I first looked at vRA 8, I ran into a situation where I hit the rate limit for Azure\u2019s API. I can\u2019t say for certain if this was vRA\u2019s fault or mine for causing too many lookups, but it did prevent me from progressing for a few hours. This approach should avoid that issue happening.</p>","tags":["powershell","rest api","vrealize automation"]},{"location":"2020/08/14/im-a-vexpert/","title":"I\u2019m a vExpert!","text":"<p>In June this year, VMware opened applications for their vExpert program. For those not familiar with it, it\u2019s VMware\u2019s \u201cglobal evangelism and advocacy program\u201d. A key part of it is giving back to the community. This can be via blog posts, helping people on VMware\u2019s forums, participating in VMware user groups and so on. I threw my hat into the ring for it, without any real expectations.</p> <p>Anyone who has worked with VMware products for any amount of time has ended up relying on the output created by people who are vExperts. I know I have. When viewing the list of people in the vExpert Directory, there was a lot of blog URLs that I recognised.</p> <p>So when I got the email this week saying I was one of the lucky ones, it was a pleasant surprise. It\u2019s an honor to be recognised by VMware for this. Congratulations to everyone else who got their vExpert awards this year.</p>","tags":["vmware","vexpert"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/","title":"vRA/vRO 8.1 Powershell \u2013 Peaking Under The Hood","text":"<p>One of the new features in vRealize Automation (vRA) and vRealize Orchestrator (vRO) 8.1 was support for PowerShell. This means there are now 4 scripting language options for Action Based Extensibility (ABX) in vRA and Workflows in vRO. In this post, I\u2019m going to have a look at some of the technical details of the PowerShell implementation.</p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#why-we-should-care","title":"Why We Should Care","text":"<p>There\u2019s two items that come to mind about why we should care about the PowerShell implementation. The first relates to the history of PowerShell itself. Up until 2016, PowerShell had been based upon the full .NET framework. In that year, Microsoft announced PowerShell Core, which was based on .NET Core. This allowed PowerShell to be used on non-Windows platforms like Linux. This new \u201cbranch\u201d of PowerShell had reduced functionality, with many modules no longer working. Eventually PowerShell Core was re-branded to a 6.x version line. In March 2020, PowerShell 7 was released. This version was an attempt to close the gap in functionality between the two branches.</p> <p>The second item is how PowerShell was used in vRA/vRO 7.x. In 7.x it was possible to add a PowerShell host. The PowerShell host was a Windows system configured to allow vRO to remote into it to execute commands. This created an incredible amount of flexibility because you could install any modules you liked on the host. On the down side, it added complexity (more moving parts to manage) and security issues (like ensuring the PowerShell Host had a network path to each target, and Kerberos double-hopping issues).</p> <p>With this background in mind, it becomes relevant to figure out what implementation of PowerShell is used in vRA/vRO and other information about the implementation.</p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#vrealize-orchestrator-details","title":"vRealize Orchestrator Details","text":"<p>The first hint on what\u2019s going on happens when adding a Scriptable Task to a vRO workflow and selecting the scripting language from a dropdown. As shown below, it lists a Powershell version of 6.2, and a reference to PowerCLI 11.</p> <p> Some of the items I was curious about were things like environment variables, what commands were availble and what modules were installed.</p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#environment-values","title":"Environment Values","text":"<p>The first item I checked was the values exposed in the environment variables. On a Windows machine, this list can be quite long. In vRO, it seems to be a very short list.</p> <p>A few items jump out in regards to this. The 10.244 IP range referenced a few times is not the IP range that the main vRO appliance is running on. Also, the HOSTNAME value is an exact match for the name of one of the kubernetes pods running on the appliance. When running the command to get all the services running in Kubernetes, the one running on 10.244.4.51 is a proxy service. The various paths referenced in the environment values don\u2019t exist in the main appliance\u2019s file system, but presumably exist inside the container.</p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#psversiontable","title":"PSVersionTable","text":"<p>The variable $PSVersionTable contains information about the Powershell version that\u2019s running, as well as the platform. The results of outputting this variable are below:</p> <p>Based on the information, we can see confirmation of the exact version of Powershell in use, as well as the OS/platform it\u2019s running on.</p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#modules","title":"Modules","text":"<p>With the reference to PowerCLI, I was curious as to what modules were available in the vRO Powershell implementation. Unfortunately, when using the cmdlet Get-Module -ListAvailable, it would throw an error. Using Get-Module -All would work, but only listed 2 modules.</p> <p>Taking a different approach, I decided to extract the value for PSModulePath and see what was in each. While there were 4 paths defined in the variable, only 2 returned content.</p> /root/.local/share/powershell/Modules /usr/lib/powershell/Modules PowerNSX Microsoft.PowerShell.Host PowervRA Microsoft.PowerShell.Management VMware.DeployAutomation Microsoft.PowerShell.Security VMware.ImageBuilder Microsoft.PowerShell.Utility VMware.PowerCLI PackageManagement VMware.Vim PowerShellGet VMware.VimAutomation.Cis.Core PSDesiredStateConfiguration VMware.VimAutomation.Cloud PSReadLine VMware.VimAutomation.Common VMware.VimAutomation.Core VMware.VimAutomation.Hcx VMware.VimAutomation.HorizonView VMware.VimAutomation.License VMware.VimAutomation.Nsxt VMware.VimAutomation.Sdk VMware.VimAutomation.Security VMware.VimAutomation.Srm VMware.VimAutomation.Storage VMware.VimAutomation.StorageUtility VMware.VimAutomation.Vds VMware.VimAutomation.Vmc VMware.VimAutomation.vROps VMware.VumAutomation","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#vrealize-automation-details","title":"vRealize Automation Details","text":"<p>PowerShell is exposed in vRealize Automation 8.1 as one of the language options in Active Based Extensibility (ABX).</p> <p> One of the settings available in the scripting window is the \u201cFaaS provider\u201d. Because AWS had recently added PowerShell as an option for their Lambda service, I had assumed that PowerShell was added to vRA 8.1 for similar reasons. Upon selecting AWS or Azure as the provider, an error is displayed. Using \u201cAuto Select\u201d or \u201cOn Prem\u201d doesn\u2019t display the error.</p> <p></p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#environment-values_1","title":"Environment Values","text":"<p>The environment values available in a PowerShell-based ABX are more in number than those in a vRO workflow.</p> <p>Like vRO, ABX PowerShell runs in a Kubernetes container. These containers appear to live in the openfaas-fn name space. <pre><code>root@svr83 [ ~ ]# kubectl -n openfaas-fn get pods\nNAME                                                              READY   STATUS    RESTARTS   AGE\nabx-11609d65654e7549026967a20f059e8bb48a19d6cf446b5c87b66af5pbw   1/1     Running   0          9m34s\nabx-54453c405ffb2f70833f631eb1a375da5cf00b453ecc1e0bbb0b4bv5vrv   1/1     Running   0          57d\nabx-563a5edfc2986031e89849dd43fbbcc3534072d481da7b176af1dbhwlp6   1/1     Running   0          63d\nabx-f22a9a49ae8db57a20427d590f6da862556a744db95b23869db8bdnvc2j   1/1     Running   0          14m\n</code></pre></p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#psversiontable_1","title":"PSVersionTable","text":"<p>The PSVersionTable values exposed in a PowerShell ABX appear to be exactly the same as in a vRO workflow.</p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#modules_1","title":"Modules","text":"<p>There is also 4 module paths exposed in vRA ABX, although only 3 are accessible. The modules in those directories are shown below.</p> /root/.local/share/powershell/Modules /usr/lib/powershell/Modules /function PowerNSX apply-hardening abx_wrapper.ps1 PowervRA Backup-VCSA client_main.psm1 VMware.DeployAutomation ContentLibrary VMware.ImageBuilder CrossvCentervmotion VMware.PowerCLI DatastoreFunctions VMware.Vim Get-NewAndRemovedVMs VMware.VimAutomation.Cis.Core Get-NICDetails VMware.VimAutomation.Cloud Get-VMmaxIOPS VMware.VimAutomation.Common InstantClone VMware.VimAutomation.Core Konfig-ESXi VMware.VimAutomation.Hcx Microsoft.PowerShell.Host VMware.VimAutomation.HorizonView Microsoft.PowerShell.Management VMware.VimAutomation.License Microsoft.PowerShell.Security VMware.VimAutomation.Nsxt Microsoft.PowerShell.Utility VMware.VimAutomation.Sdk NSXT VMware.VimAutomation.Security PackageManagement VMware.VimAutomation.Srm PerVMEVC VMware.VimAutomation.Storage PowerShellGet VMware.VimAutomation.StorageUtility ProactiveHA VMware.VimAutomation.Vds PSDesiredStateConfiguration VMware.VimAutomation.Vmc PSReadLine VMware.VimAutomation.vROps PSvLIMessage VMware.VumAutomation rCisTag Recommend-Sizing Set-CBT SRM Start-UNMAP Validate-ESXiPackages VAMI vCenter.Alarms vCenterCEIP vCenterManualMigration VCHA Vi-Module VMCPFunctions VMFSIncrease VMToolsManagement VMware-vCD-Module VMware-vCD-TenantReport VMware.Community.CISTag VMware.CSP VMware.DRaaS VMware.HCX VMware.Hosted VMware.Hv.Helper VMware.VCGChecker VMware.VMC VMware.VMC.NSXT VMware.VMEncryption VMware.VsanEncryption vSphere_Hardening_Assess_VM_v1a <p>While the first folder seems to have the same modules as in vRO, the second folder has a greatly expanded set of modules available.</p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/08/14/vravro-81-powershell--peaking-under-the-hood/#final-thoughts","title":"Final Thoughts","text":"<p>As shown by the information about the PowerShell implementation, VMware didn\u2019t use the latest major release of PowerShell. We do get the PowerCLI modules out of the box, which is a plus. One thing I would like to investigate in the future is importing extra modules.</p> <p>One thing I did notice is it seems every time a PowerShell ABX is run, the associated kubernetes pod is restarted or re-initiated. This seems to give the feel of these executing slower than a vRO Workflow. Depending on what the ABX is doing, this may or not not be an important thing.</p> <p>Depending on what task you\u2019re trying to achieve, the ability to use PowerShell directly in vRA or vRO may allow you to avoid the complexity and trouble involved in setting up and maintaining a separate PowerShell Host system. This resulting reduction in complexity means your overall systems doing these tasks would be more reliable, which is a good outcome.</p>","tags":["powershell","vrealize automation","vrealize orchestration"]},{"location":"2020/11/12/vrealize-automation-82-released/","title":"vRealize Automation 8.2 Released","text":"<p>VMware have released the 8.2 update for their vRealize Automation product. As the release notes start, this update brings the 8.x product closer to the capabilities we had in 7.x and adds some new things. The major changes include:</p> <ul> <li>A new version of the REST API</li> <li>Blueprints are now called \u201cCloud Templates</li> <li>More Terraform functionality</li> <li>Multi-tenancy support</li> <li>Custom Roles Based Access Control (RBAC)</li> <li>More XAAS Functionality</li> <li>More NSX feature support</li> </ul> <p>In this post, I\u2019ll have a look at some of these updates that are relevant to my background and prior use of vRealize Automation.</p>","tags":["vra8","vrealize automation","vrealize automation 8"]},{"location":"2020/11/12/vrealize-automation-82-released/#vmware-cloud-templates","title":"VMware Cloud Templates","text":"<p>The first item is the change from Blueprints to \u201cCloud Templates\u201d. VMware have a blog post that talks about this change. At this stage, it appears to be simply a label change, as there\u2019s no other real changes I could see in terms of functionality. However it may indicate a shift in direction going forward.</p>","tags":["vra8","vrealize automation","vrealize automation 8"]},{"location":"2020/11/12/vrealize-automation-82-released/#xaas-custom-day-2-changes","title":"XAAS Custom Day 2 Changes","text":"<p>In 8.2, it\u2019s now possible to have three types of binding for Day 2 actions. Originally there was only one binding type, \u201cin request\u201d. The other two options are \u201cwith binding action\u201d and \u201cdirect\u201d. \u201cWith binding action\u201d is available when inputs types are of a certain sort, such as VC:VirtualMachine. Direct is available for input properties that are primitive data types.</p> <p>What this can result in is a much simplified request form that is presented to the user when performing the action. The screenshot below shows the inputs for a vMotion action, using the \u201cin request\u201d bindings. This effectively simulates the 8.1 behaviour. The first field is obviously redundant because the action should know what Virtual Machine is being migrated. </p> <p>When the binding is updated to \u201cwith binding action\u201d, the first field. Behind the scenes, the mapping between the Cloud_vSphere_Machine object in vRA and the vCenter VM object is happening. This is a nice change that could help clean up the look of these Day 2 actions.</p>","tags":["vra8","vrealize automation","vrealize automation 8"]},{"location":"2020/11/12/vrealize-automation-82-released/#approval-policies","title":"Approval Policies","text":"<p>Approval Policies has seen an increased set of functionality in 8.2 They can be applied to all catalog items including vRealize Orchestrator workflows and ABX actions. Since these are typically used for Day 2 Actions, you can now put appropriate approvals in place for these Actions.</p> <p>The list of criteria for applying an Approval Policy has been expanded and can be applied to pre-provisioning of blueprints. This could allow the creation of policies that act as safeguards to requests that may be out of normal scope.</p> <p>Lastly, approvers will see all the input data of the request, when reviewing the approval. This will allow them to make a more informed decision when reviewing approval requests.</p>","tags":["vra8","vrealize automation","vrealize automation 8"]},{"location":"2020/11/12/vrealize-automation-82-released/#summary","title":"Summary","text":"<p>Overall this seems like a good release from VMware. Two areas of the 7.x product that I always liked was the Day 2 Actions and the XAAS functionality. Both of these went hand in hand to extend the product beyond \u201cDay 1 provision\u201d tasks and just doing things with Virtual Machines. It meant that many things could be automated and presented as a catalog item (some of which I\u2019ve previously written about). In 8.0, we lost a lot of that capability, but we\u2019re getting it back.</p> <p>Similarly, Approval Policies is something that had been brought up in a few organisations I\u2019ve worked in. The limited functionality of the policies in 8.0 meant that those organisations couldn\u2019t really consider that version of vRA. With 8.2, I think Approval Policies are at a point where those organisations could consider it again.  </p>","tags":["vra8","vrealize automation","vrealize automation 8"]},{"location":"2020/12/07/vra-8-and-phpipam/","title":"vRA 8 and phpIPAM","text":"<p>IP address management (IPAM) is one of those areas that suddenly becomes important when infrastructure automation is implemented. In the past, an IT team may have used an Excel spreadsheet for manually tracking address assignments. When the number of servers being provisioned increases due to automation, this manual approach isn\u2019t viable anymore. An alternative could be to just allow dynamic addressing using DHCP but that raises its own issues (for example, some server-based applications insist on a static address).</p> <p>There\u2019s a lot of products out there that do IPAM. I\u2019ve previously used the product from SolarWinds, in my home lab I use phpIPAM. In both situations, the use of the IPAM product was similar \u2013 a vRealize Orchestrator (vRO) Workflow would request an IP address during the provisioning of a virtual machine. A matching Workflow would execute when the virtual machine was decommissioned, signalling the IPAM system to release the IP address for reuse.</p> <p>As part of the process in moving my home lab setup from vRA 7.x to 8.x, this is one of the functions I\u2019d like to carry across.</p>","tags":["ipam","phpipam","vrealize automation","vrealize automation 8","vrealize orchestrator"]},{"location":"2020/12/07/vra-8-and-phpipam/#setup-a-rest-host","title":"Setup a REST Host","text":"<p>In my vRA 7.x implementation, I leveraged the REST API functionality in phpIPAM. My plan is to do the same for version 8. The first step in this is to add a REST host entry in vRealize Orchestrator. Under Inventory, you will find the different type of hosts that vRO can use in Workflows and Actions. To add the REST Host, we need to run the Workflow under Workflows &gt; Library &gt; HTTP-REST &gt; Configuration.</p> <p></p> <p>Select the Workflow and click on \u201cRun\u201d to start it. The Workflow will ask for some details about the host, like a URL and login details. Since I am already using the REST API in phpIPAM, I already have an account I can use for this.</p> <p></p> <p>Once all the details are entered, click on the Run button. Assuming everything is successful, the Workflow will finish with a Completed status and an entry for it will appear under the Inventory.</p> <p>My preferred approach for using REST APIs in vRO is to use Operations, which sit under the Host object. This allows me to keep my code minimal and control what values can be submitted to the API. Effectively only two Operations are needed \u2013 one to get the next available IP address and one to release an IP address in use. Depending on how the REST API of your IPAM system works, you may also need a Workflow or Action for handling authentication/getting a token. The Workflow for this is under the same area as the one to add the host and is called \u201cAdd a REST Operation\u201d.</p> <p></p> <p>Once all the Workflows are done, I\u2019ve created an entry for the API username and password in Configurations so I can refer to them in Workflows without having to hardcode these values or expose the password.</p>","tags":["ipam","phpipam","vrealize automation","vrealize automation 8","vrealize orchestrator"]},{"location":"2020/12/07/vra-8-and-phpipam/#creating-the-workflows","title":"Creating The Workflows","text":"<p>The next task is to create the Workflows and Actions. phpIPAM uses a token-based approach, so the first item to setup will be that, since the others rely on it. In my old vRA 7.x setup, I made this an Action. The Action uses the \u201cLogin\u201d REST Operation, username and password as Inputs and returns the token as a String. Once that is working, I can move over the other Workflows, such as the reservation and release Workflows. A high level view of how these workflows link together is shown below.</p> <p></p> <p>When the Workflows for the IP management side of things are done, I need Workflows relating to the Lifecycle of the Virtual Machine managed by vRA. In vRA 7.x, I had a Workflow that ran during the BuildingMachine part of the Lifecycle, which updated the properties of the Virtual Machine with the new address. In vRA 8, there are more Lifecycles.</p> <p>The ability to set the IP address relies on an Extensibility Subscription existing, which creates the link between what vRO Workflow should be run and under what conditions (ie. at a particular lifecycle stage). This link also facilitates the passing of information (a payload) about the deployment to the vRO Workflow that can be leveraged. This data is in a defined format called a schema and I already know the schema in vRA 8 is different from 7. So the first thing is to create a generic \u201cpayload parsing\u201d Workflow that simply outputs the payload. As shown below, it seems one of the fields is an obvious choice, called \u201caddresses\u201d.</p> <p></p> <p>In vRA/vRO 7, a Properties object would be outputted at the end of the Workflow, which could contain a number of values relating to the payload. In vRA/vRO 8, it appears that we use outputs relating to the items under the payload. So to set the IP address, we would have to have an output called \u201caddresses\u201d of the appropriate data type.</p> <p>An additional consideration is not all the fields can be altered, nor are the fields consistent across Lifecycle Event Topics. For example, the Compute Provision event has the field for IP addresses, but it\u2019s read-only. The Network Configure event also has a field for IP Addresses, but it can be altered. However it\u2019s missing the ResourceNames field, which could be useful. What all this means is it may not be a simple task of lift-and-shift for our existing vRA 7.x Workflows into vRA 8.</p> <p>If I want the entry in phpIPAM to include the hostname of the Virtual Machine being provisioned, I need to split the operation into two steps at different Lifecycle stages. Fortunately, the release workflow is much easier to get working.</p>","tags":["ipam","phpipam","vrealize automation","vrealize automation 8","vrealize orchestrator"]},{"location":"2020/12/07/vra-8-and-phpipam/#challenges-acknowledgements-and-summary","title":"Challenges, Acknowledgements and Summary","text":"<p>As suggested above, the Lifecycle phases in vRA 8 aren\u2019t a one for one mapping to vRA 7 and have different value behaviours depending on the phase. This means one can\u2019t simply pull Workflows into vRA 8 that are linked to Lifecycles. There needs to be an aspect of review and potentially rewriting the Workflows.</p> <p>I also had some self inflicted problems. I was originally using a template with cloud-init in it and this seemed to interfere with the IP address assignment. Once I created a new template that didn\u2019t include that, the address assignment worked.</p> <p>I also ran into issues where the documentation wasn\u2019t very clear on how to actually update values. Fortunately I was able to figure it out with some of these resources:</p> <ul> <li>vRealize Automation - Customizing IP Assignments with vRO by vNuggets \u2013 This was my starting point in getting all this working</li> <li>Updating Custom Properties in a vRealize Automation 8 Deployment with a vRealize Orchestrator Workflow by VMwarebits \u2013 Although I didn\u2019t use it this time, it has a good reference on how to update custom properties in a vRA 8 deployment</li> <li>vRA with Cloud-init and Static Networking by vNuggets \u2013 This was the article that made me relase Cloud-init might be causing problems for me and helped me get around that issue</li> <li>vRA 8 \u2013 My First Extensibility Workflow \u2013 vRO Edition by vBombarded \u2013 This post had some good general information of Extensibility workflows</li> <li>Lastly, the vRA 8 Extensibility Samples has a good template workflow that outputs all the values available during a Lifecycle phase. This can help figure out what values are available and can be updated</li> </ul> <p>I plan on continuing to work on these Workflows and refining them, hopefully to a point where I\u2019m comfortable releasing them.</p>","tags":["ipam","phpipam","vrealize automation","vrealize automation 8","vrealize orchestrator"]},{"location":"2020/12/24/vrealize-automation-8-pricing-cards/","title":"vRealize Automation 8 Pricing Cards","text":"<p>Pricing cards allows your vRA consumers make informed decisions on the costs of infrastructure they provision. The functionality is similar to what was available when vRealize Cloud for Business was integrated with vRA 7.</p> <p>Enabling Pricing Cards To enable Pricing Cards, a few prerequisites need to be undertaken. Firstly, the vRealize Operations appliance needs to be configured to use the same time zone as the vRealize Automation appliance. By default, both appliances will use UTC as their timezone setting. So as long as you haven\u2019t changed anything on either, this is just a verification step. You also need to configure a currency in vROPS.</p> <p>The next step is to add a vROPS endpoint. To do this, login to vRA 8 and go to Infrastructure &gt; Connections &gt; Integrations &gt; Add Integration. From the list of Integration Types presented, select vRealize Operations Manager. Fill in the form with the appropriate details and click Validate. </p> <p>You will be prompted to accept the certificate. Review and click Accept if it\u2019s ok. The lower half of the form should update with details of vCenter servers being managed by the vROPS server. If all is good, click the Add button.</p>","tags":["pricing card","vrealize automation","vrealize automation 8"]},{"location":"2020/12/24/vrealize-automation-8-pricing-cards/#creating-a-pricing-card","title":"Creating a Pricing Card","text":"<p>To create a Pricing Card, navigate to Infrastructure &gt; Configure &gt; Pricing Cards. By adding the vROPS endpoint, there should be a default card already created. Click on the New Pricing Card button. In the form that loads, start filling out details, like the name and description. As shown in the screenshot below, the currency has defaulted to Australian dollars, which is the currency that is configured in my vROPS instance. </p> <p>To get more granular control over the pricing details, click on the Pricing tab. In this area, the Basic Charges can be switched between a cost ratio or a set of flat rates. Flat rates have a lot of flexibility on when to charge. For example, since memory is only consumed when a VM is on, you might set it to only charge for that when the VM is on. The Guest OSes section allows you to add a charge per OS, to cover licensing or support. In the example below are ongoing monthly charges for maintaining the OS. </p> <p>Charges for individual tags can be applied. There\u2019s a number of use cases for this and is limited by the effort you wish to put into your pricing model. Custom Properties is another way of doing bespoke style charges, based on properties of a virtual machine. Lastly are overall charges, with one time costs and recurring. Once all the details have been specified, Assignments button and assign to the appropriate projects. Lastly, click on the Create button.</p>","tags":["pricing card","vrealize automation","vrealize automation 8"]},{"location":"2020/12/24/vrealize-automation-8-pricing-cards/#using-pricing-cards","title":"Using Pricing Cards","text":"<p>The functionality of Pricing Cards is exposed in a few places. Firstly, when requesting a new deployment, a new area will appear in the request form relating to a Daily Pricing Estimate.</p> <p> Clicking on the Calculate button will cause the system to think a bit while it works out the costs. Once done, it will update the Daily Estimate and the Details link will become active. Clicking on that link will give a breakdown of where the costs are coming from.</p> <p> As noted in the screenshot, Guest OS and one time prices aren\u2019t included and I think this is a bad thing. These cost items can often be significant items that need to be paid up front (like OS or software licensing). This creates a lack of visibility at provisioning time.</p> <p>Another area where pricing card functionality is exposed is after a deployment has been running for a period of time.</p>","tags":["pricing card","vrealize automation","vrealize automation 8"]},{"location":"2020/12/24/vrealize-automation-8-pricing-cards/#conclusion","title":"Conclusion","text":"<p>The inclusion of Pricing Cards is a useful feature, especially if you intend to broaden your base of consumers of vRA services. Many organisations that I\u2019ve worked at that would have vRA in place also already have vROPS, so the pairing of them to give this functionality removes the need for additional systems.</p>","tags":["pricing card","vrealize automation","vrealize automation 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/","title":"VMware Horizon 8/2012 \u2013 Install Walkthrough &amp; First Impressions","text":"<p>This month VMware announced the latest release of their desktop virtualisation product, Horizon. It\u2019s been a while since I\u2019ve gotten really into this product so I decided to have a look at what\u2019s changed.</p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/#major-changes-architecture","title":"Major Changes &amp; Architecture","text":"<p>The most prominent change that I saw from the release notes was the retirement of the View Composer component. The Composer was a system that managed linked clones. The Connection Server remains and unfortunately it\u2019s still a Windows server. Hopefully at some point, a virtual appliance option becomes available.</p> <p>A number of recent Linux releases have been added as supported operating systems including RHEL/CentOS 8.3 and Ubuntu 20.04. In the content of when I last used Horizon what\u2019s changed, the architectural diagram for v8 is a good starting point. </p> <p>When I last used Horizon, it was only really available to manage virtual desktops on VMware platforms, with some support for RDS. Like some of VMware\u2019s other products (vRealize Automation being a good example), Horizon has branched out to support technologies both on-prem and in the cloud, as well as those by other vendors.</p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/#setup-of-horizon-8","title":"Setup of Horizon 8","text":"<p>Due to the simplified architecture, the setup process is a bit easier than in the past. First are some pre-requisite tasks in Active Directory. One task is creating an Organisational Unit (OU) for the remote desktops, and a group for the users. Next is two service accounts, one for accessing vCenter and another for handling the instant clone operations.</p> <p>The vCenter service account\u2019s specific rights depend on whether you want to use instant clones or not. In my setup, I decided to try out instant clones and have the account setup with the permissions to do so. The account for instant clone operations needs rights on the container that will hold the instant clone computer accounts.</p> <p>With those items done, the installer can be run on the server. The first choice to make during the installation is what type of instance to install. Because I want to do a standard installation and it\u2019s the first server, I picked the Standard Server option and left the other values at their defaults.</p> <p> Next we are prompted for a data recovery password. The next screen prompts to automatically configure the Windows firewall to allow the required ports.</p> <p> The next screen will prompt you for the initial Horizon administrators. In my case, it defaulted to the user acount I was logged in as. I changed this to a group.</p> <p></p> <p>Next, we are prompted to join the Customer Experience Improvement Program. Lastly we are prompted to select where the Connection Server is being deployed to. As shown in the screenshot, there\u2019s a number of cloud options.</p> <p> Once the installation is complete, we can see a number of new services have appeared.</p> <p> Like many systems today, Horizon uses certificates as part of its activities and there\u2019s a section on configuring it with properly issued certificates. For my home lab, I\u2019ve decided to skip it. This means the next step is configuration.</p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/#configuration-of-horizon-8","title":"Configuration of Horizon 8","text":"<p>The Horizon Console is the web-based interface for managing Horizon and is used to perform the first time configuration.</p> <p> The first item we are prompted to deal with is adding a license key.</p> <p> Once the license has been successfully applied, the next step is to add a vCenter server. The vCenter Information page of the Add Server wizard has the sort of values we would expect, such as the server address, user account, etc.</p> <p> The next section has Storage settings. I left these values at their defaults and continued.</p> <p> After this, we need to add the Instant Clone operations account that was created earlier. Following this, the settings for the Connection Server should be reviewed. By default, the PCoIP Secure Gateway option isn\u2019t enabled. Additionally, SAML isn\u2019t configured out of the box, so if your environment requires it, this is where you would configure it. Like past versions of Horizon, there is an Event Database that can be configured. Both SQL Server and Oracle are supported. You can also pick the option of sending syslog data to a syslog server.</p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/#creating-a-desktop-pool","title":"Creating a Desktop Pool","text":"<p>At this point, we can start considering the creation of a Desktop Pool. The first step for this is creating a \u201ctemplate\u201d Virtual Machine that will serve as the basis for the machines in the pool. In general, you want to configure this template to run lean, which means disabling or turning down aspects of the operating system that may impact performance. VMware have a detailed guide on how to create an optimised image at https://techzone.vmware.com/creating-optimized-windows-image-vmware-horizon-virtual-desktop</p> <p>As mentioned in the guide, one of the steps is to install the Horizon Agent on the template VM. The Agent installer will prompt for things like the network protocol (IPv4 or v6). The features that are installed may need some consideration, depending on your use case. A number of redirection components are not enabled by default.</p> <p> Once the installation is completed, the Virtual Machine will need to be rebooted. There are some optional components that can be done. The final step in preparing the template involves running the VMware OS Optimisation Tool on it. This tool automates the process to applying settings to the operating system, to help optimise it. The process of using the tool involves performing an analysis of the system with one of the defined templates. If you\u2019re happy with what will be changed, you can then apply the settings defined in the template.</p> <p> After optimising the OS, the Optimisation Tool can also be used to finalise the OS. After some more settings on the VM itself and a snapshot, we can move on to creating the desktop pool inside Horizon. This is done via a wizard style interface which asks a lot of the same questions as prior versions did. At the end, we get a nice summary screen.</p> <p> Once the pool is created, Horizon will start provisioning VMs (if you configured it to do so). Progress can be reviewed in the Pool details.</p> <p> Unfortunately for me, the storage in my home lab isn\u2019t that fast, so the Instant Clones weren\u2019t very instant. It took about 20 minutes to provision a pool of 2 VDI instances.</p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/#horizon-client-experience","title":"Horizon Client Experience","text":"<p>Horizon 8 has a slick HTML 5 interface, which is access via a landing page. This landing page also presents an option to download the Horizon Client.</p> <p> Upon selecting the HTML option, a logon prompt appears, asking for a username and password. Upon logging in, the user sees all the items they\u2019re entitled to. In my case, it\u2019s just the singular Windows 10 Desktop Pool. There\u2019s some UI options available, such as being able to \u201cfavorite\u201d an item and filter to show just those ones.</p> <p>The actual login experience was very quick, much faster than I\u2019d previously experienced with Horizon. Upon logging in I was presented with a standard Windows 10 system, with the token applications I had installed.</p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/#creating-an-application-pool","title":"Creating an Application Pool","text":"<p>The \u201cother half\u201d of Horizon\u2019s functioning is publishing applications. This is done by creating an Application Pool. Before the Application Pool can be created, a Desktop Pool with a session of \u201cApplications\u201d or \u201cDesktop and Applications\u201d needs to exist. When starting the wizard to create the Application Pool, you will select the correct Desktop Pool. After a few moments, all the applications on that Desktop Pool will be listed and you can select which ones to publish.</p> <p> Once the Application Pool is created, Entitlements can be applied to grant access. One nice feature in the settings is \u201cAnti-affinity\u201d. This allows you to define a pattern to keep applications on separate RDS hosts, effectively a form of load balancing. If you have an application that\u2019s known to have a heavy performance impact, this setting could be useful. Upon logging in, an Entitled user will see a batch of new icons for the applications they can run.</p> <p></p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/#other-features-and-items-of-note","title":"Other Features and Items of Note","text":"<p>One thing I did like seeing referenced in the documentation numerous times was \u201csilent installation\u201d. In my past experience, the process of creating or updating a golden image for Horizon was a mostly manual task. The inclusion of silent installation options allows some scope for automation.</p> <p>Security also seems to be taken seriously with this release. Out of the box, this version of Horizon supports TLS 1.1 and 1.2. Older protocols and security cyphers are disabled, which is very helpful if you\u2019re working in an environment with security-related compliance.</p> <p>Horizon has a plugin for vRealize Orchestrator, which enables Orchestrator workflows for common Horizon tasks. These workflows can then be published to vRealize Automation for broader consumption.</p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/01/15/vmware-horizon-82012--install-walkthrough--first-impressions/#summary","title":"Summary","text":"<p>Horizon has clearly matured and improved a lot over the years. The simplified architecture via the removal of the Composer is good, but I had hoped for an appliance version of the Connection server. The management overhead for an appliance is less than a Windows-based server.</p> <p>Part of me also wants to see Horizon move into a similar direction that vRealize Automation has \u2013 in its latest iteration, vRA is almost a cloud broker of sorts, allowing you to provision on all major cloud platforms. Being able to do the same thing with Horizon (ie. provision/manage on AWS Workspaces or Azure\u2019s Windows Virtual Desktops) would make for a very interesting product.</p>","tags":["vdi","vmware horizon","vmware horizon 2021","vmware horizon 8"]},{"location":"2021/03/10/remediating-vmsa-2021-0002--potential-issues/","title":"Remediating VMSA-2021-0002 \u2013 Potential Issues","text":"<p>In late February, VMware published their second security advisory for 2021. It contained contained three items:</p> <ul> <li>CVE-2021-21972 \u2013 A remote code execution vulnerability in vCenter that has a CVSS score of 9.8</li> <li>CVE-2021-21974 \u2013 A vulnerablity in OpenSLP, which is used in ESXi. This one has a CVSS score of 8.8</li> <li>CVE-2021-21973 \u2013 Another vCenter vulnerability that was rated with a CVSS score of 5.3</li> </ul> <p>Given the product versions affected, most organisations with relatively up to date virtualisation infrastructure would be at risk from these items. While testing and simulating the update process, I ran into some issues that might be worth publishing for a broader audience.</p>","tags":["cve-2021-21972","cve-2021-21973","cve-2021-21974","security","vcenter","vmsa-2021-0002","vmware vcenter"]},{"location":"2021/03/10/remediating-vmsa-2021-0002--potential-issues/#vcenter-update-manager-not-available","title":"vCenter Update Manager Not Available","text":"<p>One of the methods for updating ESXi hosts is to use vCenter Update Manager (VUM). VUM\u2019s functionality is exposed via the HTML 5 interface on vCenter as an Updates tab that appears when viewing certain objects in vCenter, such as ESXi hosts.</p> <p>In my testing, these interface elements are not present in earlier versions of vCenter 6.5. This is because vCenter 6.5 existed at a time when VMware were progressively adding functionality to the HTML 5 interface, in preparation for the eventual end of life for Flash.</p> <p>If vCenter 6.5 is updated to the latest version (which also resolves the CVE items listed at the start) then the VUM-related interface elements should appear</p> <p>Another scenario I saw with this problem involved vCenter Server 6.5 on Windows. Even after updating to the latest version of vCenter, the VUM components wouldn\u2019t be available. Given that VMware appears to have abandoned Windows as a platform option long term and the extra complexity in managing a Windows VM, the best option seems to be to migrate to a Virtual Appliance version of vCenter.</p>","tags":["cve-2021-21972","cve-2021-21973","cve-2021-21974","security","vcenter","vmsa-2021-0002","vmware vcenter"]},{"location":"2021/03/10/remediating-vmsa-2021-0002--potential-issues/#updating-esxi-fails-reference-to-signing-certificate","title":"Updating ESXi Fails, Reference to Signing Certificate","text":"<p>The other main issue I experienced was when updating ESXi. When attempting to update using a depot.zip file, it would generate an error referencing \u201cCould not find a trusted signer\u201d. As covered in VMware KB Article 76555, VMware changed to a newer signing certificate for these updates in 2018. If the host is running a version that is older than a certain point in time, it has no awareness of this new certificate and thus doesn\u2019t trust it.</p> <p>The workaround for this is to do an intermediate update to a version that does recognise the new certificate. Once that\u2019s done, it\u2019s possible to then update to the latest version.</p>","tags":["cve-2021-21972","cve-2021-21973","cve-2021-21974","security","vcenter","vmsa-2021-0002","vmware vcenter"]},{"location":"2021/05/12/vrealize-automation-84/","title":"vRealize Automation 8.4","text":"<p>VMware have released another update to vRealize Automation (vRA) 8. Like 8.3, I had issues updating to this version using Lifecycle Manager. This is why I never got around to writing about the 8.3 release. I ended up doing a fresh installation of 8.4 to see what\u2019s new and changed.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2021/05/12/vrealize-automation-84/#whats-new","title":"What\u2019s New","text":"<p>Going through the Release Notes, it seems that this release is a set of incremental changes. There is a change to how the Access Token for the API functions, which could have an impact on those who are leveraging the REST API. As per the notes, there\u2019s also been a lot of improvements to accessibility. It\u2019s good to see VMware pushing ahead with this sort of initiative.</p> <p>There seems to be additional flexibility in how you can configure storage for Virtual Machines (VM). Day 2 actions can now be applied to all disks on a Virtual Machine, with easier identification of them. If the VM is of a vSphere type, you now have granular control over the order in which disks are created and which SCSI controller its attached to. For deployments that may have complex storage configurations (such as a high performance SQL Server) this is a great addition.</p> <p>There\u2019s some extra Azure related functionality. Firstly, the use of the Image Gallery is supported. This is a timely addition for myself as I\u2019m just starting to look at how to create custom images on Azure using Packer. It\u2019s now possible to create and manage snapshots on Azure managed disks.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2021/05/12/vrealize-automation-84/#vrealize-orchestrator-changes","title":"vRealize Orchestrator Changes","text":"<p>Since vRealize Orchestrator (vRO) comes with vRA it\u2019s always a good thing to check the separate release notes for that product. One item mentioned in the release notes of both products is the Plugin. The wording seems to imply it\u2019s a reworking or major revision.</p> <p>A nice feature that was included in vRO 8.3 is the Viewer role. This allows a user to be granted the rights to view vRO objects. But they cannot create, edit or execute items. While a use case doesn\u2019t immedately come to mind, it\u2019s good to see that it\u2019s available.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2021/05/12/vrealize-automation-84/#final-thoughts","title":"Final Thoughts","text":"<p>The issues I\u2019m running into with updating the product via Lifecycle Manager is becoming an annoyance. From a personal point of view, the extra capabilities being added around disks and storage is good. This was an area under vRA 7.x that was lacking and would\u2019ve required a lot of custom code to achieve the same outcomes. Overall a nice set of improvements.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2021/08/22/vrealize-automation-85/","title":"vRealize Automation 8.5","text":"<p>A few days ago, VMware released an update for vRealize Automation (vRA). The list of improvements seems relatively minor this time, as detailed in the Release Notes. It seems the biggest change was mentioned in the blog announcement for this release, where vRA is moving to monthly releases. Since these updates are feature focused, that potentially means a more frequent update cycle for administrators. Hopefully this means the update process will become smoother going forward. From personal experience, it\u2019s been a bit hit and miss.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2021/09/22/vmsa-2021-0020--19-vulnerabilities-on-vcenter/","title":"VMSA-2021-0020 \u2013 19 vulnerabilities on vCenter","text":"<p>VMware published a new security advisory overnight (VMSA-2021-0020) and it\u2019s a big one. In total, it lists 19 vulnerabilities affecting multiple versions of vCenter. The most serious of the vulnerabilities is the first one \u2013 CVE-2021-22005. This vulnerability allows an attacker to upload files to vCenter. This vulnerability could then be used as an avenue to execute code. It\u2019s been giving a CVSS score of 9.8</p> <p>The second most worrying item on the list (CVE-2021-21991) allows an attacker to escalate their priveleges to Administrator level in the vSphere web interface. This vulnerability has been scored at 8.8.</p> <p>The resolution for all these vulnerabilities is to update vCenter to the appropriate version. The advistory lists these, and I\u2019ve produced a condensed version below.</p> Product/Version Update To Notes vCenter 7.0 7.0 U2d The majority of issues are fixed by going to U2c. U2d resolves CVE-2021-22011 and CVE-2021-22018 vCenter 6.7 6.7 U3o This version will resolve all the associated issues with 6.7 vCenter 6.5 6.5 U3q This version will resolve all the associated issues with 6.5 <p>Given the nature of some of these vulnerabilities, this would be one to get onto ASAP.</p>","tags":["security","vcenter","vmsa-2021-0020"]},{"location":"2022/01/26/vrealize-automation-862/","title":"vRealize Automation 8.6.2","text":"<p>Normally I wouldn\u2019t write about a minor release but in this case I think it\u2019s worth mentioning. On 18th January, VMware released version 8.6.2 of vRealize Automation. The big item in this release is that log4j has been updated to resolve some of the vulnerabilities that have been discovered.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2022/01/26/vrealize-automation-862/#log4j-updated-to-217","title":"Log4J Updated to 2.17","text":"<p>Starting in December 2021, a number of vulernerabilities were discovered in the Log4J logging utility. Log4j is used in a lot of other products to allow easy logging functionality. The first vulnerability, dubbed \u201cLog4Shell\u201d, was given a CVSS score of 10. The CVE ID assigned was CVE-2021-44228. As per the Release Notes, this is one of the CVEs that 8.6.2 resolves.</p> <p>The second vulnerability mentioned in the Release Notes is CVE-2021-45046. Like the first vulnerability, it can also be exploited remotely and was considered quite severe. There have been two further vulnerabilities that have been discovered, however according to VMware, they can\u2019t be exploited on their products.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2022/01/26/vrealize-automation-862/#resouce-center","title":"Resouce Center","text":"<p>One key interface change is the Deployments tab is now called Resources. It seems the intent here is to create a consolidated view of all resources and integrate day 2 actions. There\u2019s also the ability to quickly create a simple VM in this area, without the need for a Cloud Template. How useful that ends up being is up for debate.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2022/01/26/vrealize-automation-862/#final-thoughts","title":"Final Thoughts","text":"<p>As with a lot of the updates over 2021, this one adds a few nice improvements. The official fix for Log4j is reason enough to get on this version.</p>","tags":["vra8","vrealize automation 8"]},{"location":"2022/02/28/vexpert-2022/","title":"vExpert 2022","text":"<p>VMware once again requested applicants for their vExpert program. I was forunate enough to be accepted again.</p> <p>|||</p>","tags":["vexpert"]},{"location":"2022/03/06/powershell-quality-of-life-improvements--code-testing/","title":"PowerShell Quality of Life Improvements \u2013 Code Testing","text":"<p>PowerShell has been around for 15 years now. One of the changes that has happened in the IT industry in that time is the rise of DevOps, and the associated tools and technology with it. If we consider the process of creating and consuming PowerShell scripts, it can be applied to the major stages of software development \u2013 test, build, release. By using the methods associated with this, we can look at improving the quality of PowerShell code we deliver.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/06/powershell-quality-of-life-improvements--code-testing/#why-do-code-testing","title":"Why Do Code Testing?","text":"<p>Code testing can cover a broad spectrum of activities. At the most basic end is syntax checking or a \u201clinter\u201d to perform static code checking. At the complex end, there\u2019s things like unit tests. For the sake of this example, I\u2019ll be using a \u201clinter\u201d, the PS Script Analyzer. By using such a tool on our code, we can establish whether it meets minimum quality requirements. PS Script Analyzer comes with an array of built-in rules and can be extended with your own rules.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/06/powershell-quality-of-life-improvements--code-testing/#creating-the-code-test-pipeline","title":"Creating The Code Test Pipeline","text":"<p>The first step is to create a pipeline that will perform the code testing activities. In my case, I\u2019m using Azure DevOps, but a similar approach can be used with Github. The pipeline itself is relatively simple, with two tasks. One will install the PS Script Analyzer module, as it\u2019s not installed by default on Azure DevOps agents. The second task will execute the analysis process. The pipeline code is shown below:</p> <pre><code>name: 'Check PowerShell Code'\n\ntrigger:\n  none\n\njobs:\n  - job: 'CheckCode'\n    displayName: 'Check Code'\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n      - task: PowerShell@2\n        displayName: 'Install Script Analyzer'\n        inputs:\n          targetType: inline\n          pwsh: true\n          script: |\n            Install-Module -Name 'PSScriptAnalyzer' -Scope 'CurrentUser' -Force\n      - task: PowerShell@2\n        displayName: 'Analyze Scripts'\n        inputs:\n          targetType: inline\n          pwsh: true\n          script: |\n            Invoke-ScriptAnalyzer -Path $(Build.Repository.LocalPath) -Recurse -EnableExit\n</code></pre> <p>The <code>Invoke-ScriptAnalyzer</code> command will use the checked out repository path as the starting point. The <code>-Recurse</code> switch will ensure it looks in all subdirectories for scripts. The <code>-EnableExit</code> which will return a numeric value based on the number of code errors found. This will cause the pipeline to register a positive outcome if no errors are found (an exit code of 0) and a negative outcome if one or more errors are found. Once the pipeline has been created, it can be used in a branch policy.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/06/powershell-quality-of-life-improvements--code-testing/#configuring-the-branch-policy","title":"Configuring The Branch Policy","text":"<p>You may already have a branch policy in place with some of the settings mentioned here. It\u2019s common to have a branch policy on the main/master branch that requires peer review of code. This policy will add an extra tollgate that uses the pipeline created earlier. If you\u2019re starting from scratch, the follow process is below:</p> Step Screenshot/Script Navigate to the Branches area of the Respository containing the PowerShell code Click on the 3 dots on the right side for the main change and select Branch Policies If you want peer review as part of the process, turn on the Require a minimum number of reviewers option and set an appropriate number of reviewers (1-2 is usually standard). Adjust the other settings as needed. Under the Build Validation heading, click the Plus button The Add Build Policy window will appear. Select the pipeline that was created earlier. The other options can be left as default. Click Save. <p>At this point, the Branch Policy will be enforced and the settings within will apply whenever you try to do a pull request. The code checking pipeline will automatically run as part of this process. The screenshot below is an example of the output from code with errors: </p> <p>If the pipeline was set as a required item in the policy, it\u2019s impossible to complete the pull request if there are errors, even if the peer review is passed. As shown in the screenshot below, the \u201cComplete merge\u201d button is greyed out. </p> <p>If the code has no errors detected by the PS Script Analyzer, then everything will be green and the merge can be completed as per normal.</p> <p></p>","tags":["powershell","azure devops"]},{"location":"2022/03/06/powershell-quality-of-life-improvements--code-testing/#summary","title":"Summary","text":"<p>By adding a pipeline and branch policy, we can easily start enforcing a minimum level of quality on PowerShell code. PS Script Analyzer is also avaiable within VS Code, so it can be leveraged while writing the code. It has a set of good default rules, including some related to security. The rules can be extended, as well as the pipeline to suit your needs.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/09/vmsa-2022-0007--vmware-tools-vulnerability/","title":"VMSA-2022-0007 \u2013 VMware Tools vulnerability","text":"<p>VMware have published a new security advisory relating to VMware Tools for Windows. It affects v10 and 11 of the Tools. The vulnerability allows a user with local admin rights in the guest OS to acquire system privileges.</p> <p>The version with the fix for this vulnerability is 12.0.0. While this is a major version jump, from the release notes there doesn\u2019t appear to be any major breaking changes. It still supports Windows as far back as 7 SP1/2008 R2 SP1.</p>","tags":["vmsa","vmware tools"]},{"location":"2022/03/13/powershell-quality-of-life-improvements--code-signing/","title":"PowerShell Quality of Life Improvements \u2013 Code Signing","text":"<p>PowerShell uses Execution Policies to control whether a script can be run or not. Some of these policies rely on scripts being digitally signed by a trusted publisher. By signing your scripts, you can look at implementing a more restrictive Execution Policy that increases security. The first step in this process is getting a certificate.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/13/powershell-quality-of-life-improvements--code-signing/#code-signing-certificate","title":"Code Signing Certificate","text":"<p>To sign your scripts, you need a special type of certificate, one for code signing. If your scripts will be for internal use only and your organisation has an internal PKI, then using a code signing certificate from that PKI will be enough. If you are writing scripts or modules for consumption by a broader audience, then you\u2019ll need a code signing certificate from a 3rd party Certificate Authority.</p> <p>Once you have the certificate, you\u2019ll want to get it into a secrets management system that your DevOps system can access securely. In my case, since I\u2019m using Azure DevOps, that secrets sytem is Azure Key Vault. Once it\u2019s in Key Vault, we can head back to DevOps to access it. |Step|Screenshot/Script| |---|---| |Navigate to Azure Devops, then Pipelines &gt; Library | | |Click on the button to create a new variable group | | |Give the group a name. Set the \u201cLink secrets from an Azure key vault as variables\u201d to enabled. Then enter the detauls of the key vault. You may need to authorise access for DevOps to the Azure Subscription and Key Vault. | | |Under Variables, click the Add button. The certificate that was added to the Key Vault should be listed as an available secret. Select it and click the OK button. | | |Click the Save button to save the settings.| |</p>","tags":["powershell","azure devops"]},{"location":"2022/03/13/powershell-quality-of-life-improvements--code-signing/#code-signing-pipeline","title":"Code Signing Pipeline","text":"<p>The Code Signing pipeline will perform 3 tasks \u2013 import the certificate into the key vault of the pipeline agent, sign Powershell scripts in the current folder and then publish the signed scripts. The pipeline code is below:</p> <p>In this pipeline, a trigger has been configured on the main branch. Once a change has happened in main branch (such as a merge from a successful pull request), the code will be signed.</p> <p>The pipeline gets all files named *.ps1 and checks subfolders as well. If your repository contains other PowerShell related assets like modules, then you\u2019d have to include them in the filter too.</p> <p>The result of the pipeline running is that artifacts are associated with the run and can be viewed. </p>","tags":["powershell","azure devops"]},{"location":"2022/03/13/powershell-quality-of-life-improvements--code-signing/#summary","title":"Summary","text":"<p>The code signing process is effectively the \u201cbuild\u201d process in the larger process for our scripts. We now have artifacts that are signed by a trusted authority. They are now ready to be deployed and used.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/20/powershell-quality-of-life-improvements--automatic-versioning/","title":"PowerShell Quality of Life Improvements \u2013 Automatic Versioning","text":"<p>Once we start doing processes like putting PowerShell code into git repositories, signing it and effectively creating new versions of it, it becomes useful to be able to automatically manage the versioning of our scripts and modules. The version number acts as an easy visual indicator of whether the script is the latest or not.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/20/powershell-quality-of-life-improvements--automatic-versioning/#introducing-token-replacement","title":"Introducing Token Replacement","text":"<p>Since the version number will change with each \u201cbuild\u201d, we will want to put in some sort of place holder value \u2013 a token. In my sample module, I change the version value to reference this token:</p> <pre><code># Version number of this module.\nModuleVersion = '#{fullVersion}#'\n</code></pre> <p>In some environments, token replacement occurs quite late in the process (during the release). In the case of our PowerShell scripts and module, doing it this way would alter those files after they\u2019ve been signed, invalidating them. To get around this, the token replacement needs to happen before the code signing. In my case, I added it as a step in the Code Signing Pipeline, right before the actual code signing:</p> <p>Two new variables are declared at the start \u2013 the major and minor version. These are used to construct a semantic versioning format (major.minor.patch) and assigned to the variable fullVersion. The patch number will be incremented by one each time the pipeline is run. The computer version number is used to update the build number (line 23). The image below shows the resulting names when the pipeline is run: </p> <p>The task on line 26 uses Replace Tokens task by Guillaume Rouchon. While DevOps has a built-in token replacement task, it\u2019s limited and Guillaume\u2019s task has a lot more functionality. There are options for target files, token types and others. Since the token format used in the script is the \u201cdefault\u201d type, it doens\u2019t need to be specified. The only file type we\u2019re interested in targeting in this case are Module Manifest files (.psd1).</p>","tags":["powershell","azure devops"]},{"location":"2022/03/20/powershell-quality-of-life-improvements--automatic-versioning/#results","title":"Results","text":"<p>When the pipeline is run, we can see the indications of the token replacement being performed:</p> <p>The artifacts generated by the pipeline will include the correct version. The computed build number can be leveraged later on for the release process.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/27/powershell-quality-of-life-improvements--release/","title":"PowerShell Quality of Life Improvements \u2013 Release","text":"<p>Releasing a build is typically the final step in the process of developing code. For PowerShell, this takes the form of getting our signed scripts and modules onto target servers to be available for use. This can be easily achieved in Azure DevOps.</p>","tags":["powershell","azure devops"]},{"location":"2022/03/27/powershell-quality-of-life-improvements--release/#deployment-groupstargets","title":"Deployment Groups/Targets","text":"<p>The deployment tasks will run on the targets will recieve the scripts. Before that, a Deployment Group needs to be created. This is done by navigating to Pipelines &gt; Deplyment Groups and clicking \u201cAdd new deployment group\u201d. The group needs to be given a name. </p> <p>Once created, a PowerShell script will be displayed. This script needs to be run on the targets to register them as members of the group. Once it\u2019s run, the targets will appear: </p>","tags":["powershell","azure devops"]},{"location":"2022/03/27/powershell-quality-of-life-improvements--release/#creating-the-release-pipeline","title":"Creating The Release Pipeline","text":"<p>Once the Deployment Group and Targets have been setup, the Release Pipeline can be created. Navigate to Pipelines &gt; Releases and click on the New Pipeline button. From the template options, select \u201cStart with an empty job\u201d. The first option to set in the release is the Artifacts it will use. Click on the Add button in the Artifacts area. Then Build as the source type and the build pipeline as the source: </p> <p>This will cause the Release pipeline to pick up the latest artifacts from the Code Signing pipeline. The next step is to define the actual tasks to be performed. Under the Stage 1 task, click on the \u201c1 job, 0 task item\u201d. By default, the job in there will be an Agent job. Since this deployment will run on the targets, we need to remove that and add a Deployment Group Job instead. </p> <p>Once the Deployment Group Job has been added, create a \u201cCopy Files\u201d task under it. The Source folder needs to be defined as well as the Target: </p> <p>There are some Advanced options in the Copy Files task that should be reviewed, such as the ability to Overwrite existing files or to completely clean the Target Folder. In my test scenario, I enabled both of these. Once saved, the pipeline is ready to use. </p>","tags":["powershell","azure devops"]},{"location":"2022/03/27/powershell-quality-of-life-improvements--release/#creating-a-release","title":"Creating A Release","text":"<p>Once the Release Pipeline has been created, a Release can be created that uses its settings and tasks. The creation of a release can either be a manual task or triggers can be enabled to create it automatically. In my test scenario, I enabled an automatic trigger to ensure the deployment targets got the latest files as soon as possible. To do this, we need to edit the Pipeline again. The Artifact section has a lightning bolt icon which opens the trigger configuration: </p> <p>To satisfy my deployment needs, I configured a Build-based trigger that used the build pipeline\u2019s default branch (ie. \u201cmain\u201d). To trigger a Release, we need to ensure the build pipeline (the Code Signing one) is executed. The result is the Release showing the trigger text as \u201cContinuous Deployment\u201d rather than \u201cManually Triggered\u201d </p> <p>While this approach allows easy, automatic deployment of PowerShell assets to target systems, it does lack protections and tollgates. An option that could act as an intermediatory step is to deploy modules to a PowerShell respository server and constrain which versions target systems deploy.</p>","tags":["powershell","azure devops"]},{"location":"2022/04/03/powershell-quality-of-life-improvements--ps-repository/","title":"PowerShell Quality of Life Improvements \u2013 PS Repository","text":"<p>In the last post, we were able to create a Release Pipeline that takes checked and signed Powershell code and deploys it to target servers. In some situations, it may not be desirable or viable to have every server configured as a deployment target. Or there may be a need to have an additional amount of control of the modules that a server gets. To deal with these issues, we can look at setting up a PowerShell Respoistory as an intermediatory step.</p>","tags":["powershell","azure devops"]},{"location":"2022/04/03/powershell-quality-of-life-improvements--ps-repository/#setting-up-the-respository","title":"Setting Up The Respository","text":"<p>The Respository can be as simple as a file share on a server. At the higher end of complexity, it can be a website running NuGet Gallery. For this case, I\u2019ve gone simple. By using a file share, we negate the need for setting up API keys and the like that a NuGet Gallery would need.</p> <p>Once the PowerShell Respoistory is created, it needs to be registered on the relevant targets. This is achieved using the Register-PSRepository cmdlet, as shown below:</p> <p><code>Register-PSRepository -Name psqol -SourceLocation \"\\\\svr14\\psrepo\\\" -PublishLocation \"\\\\svr14\\psrepo\\\" -InstallationPolicy Trusted</code> If the InstallationPolicy value is set to \u201cUntrusted\u201d, then there will be a user prompt when attempting to install modules from the Repository.</p>","tags":["powershell","azure devops"]},{"location":"2022/04/03/powershell-quality-of-life-improvements--ps-repository/#deploying-to-the-repository","title":"Deploying To The Repository","text":"<p>Using the prior Release Pipeline as a starting point, I cloned it to a new Release Pipeline. Because PowerShell Repositories are registered on a per-user basis, the Release Pipeline code needs to perform this registration and cleanup as well. The code in the pipeline I setup is below:</p> <p>By running the Find-Module cmdlet, we get some logging output to verify that the current version was published as well as prior versions. When looking at the file share that is used for the Repository, we can see all the published versions in Nuget package format.</p> <p></p>","tags":["powershell","azure devops"]},{"location":"2022/04/03/powershell-quality-of-life-improvements--ps-repository/#leveraging-the-repository","title":"Leveraging the Repository","text":"<p>One of the benefits of this approach is that modules will now be installed on target systems using the Install-Module cmdlet. This cmdlet has the ability to control what version gets installed, via the use of requiredVersion, minimumVersion or maximumVersion parameters. In theory, this could allow a more controlled rollout and testing of new versions of code.</p>","tags":["powershell","azure devops"]},{"location":"2022/06/13/clearing-imperva-cloud-waf-cache-using-powershell/","title":"Clearing Imperva Cloud WAF Cache using Powershell","text":"<p>Imperva\u2019s cloud-based Web Application Firewall (WAF), previously known as Incapsula, provides protection capabilities as well as caching of website content. In some situations, it may be necessary to clear the cache. This can be easily achieved by using Powershell to interact with Imperva\u2019s REST API.</p>","tags":["powershell","imperva"]},{"location":"2022/06/13/clearing-imperva-cloud-waf-cache-using-powershell/#setting-up-credentials","title":"Setting Up Credentials","text":"<p>At the time of writing, Imperva\u2019s REST API uses an API ID and key to authenticate \u2013 essentially a username and password. The IDs are generated from already existing users in your Imperva account. To generate an ID and key, navigate to the User Management area of the Imperva management portal. Select the user you want to generate an ID for. Expand the API keys area and click \u201cAdd API key\u201d.</p> <p></p> <p>Enter a name for the new key and the other settings. Click Save. At this point, a window will appear with the newly generated API ID and key. It would be a good idea to save this to your secrets management system at this point. </p>","tags":["powershell","imperva"]},{"location":"2022/06/13/clearing-imperva-cloud-waf-cache-using-powershell/#using-impervas-rest-api","title":"Using Imperva\u2019s REST API","text":"<p>At the time of writing, Imperva is in a state of transition with their REST APIs. They have an established \u201cversion 1\u201d of it and are moving to a \u201cversion 2\u201d. The API calls for the process of clearing the cache use the version 1 API.</p> <p>Performing authentication takes the form of including the API ID and Key as part of the body posted to the API URL. Most API activities that relate to things associated with a \u201csite\u201d entry (such as clearing the cache) require that site\u2019s ID as a parameter.</p> <p>With that in mind, it\u2019s useful to have an initial script that can look up the site ID. To achieve this, we use the API endpoint that will list all the sites, as shown below:</p> <p>The returned content will be in JSON format, which can be converted and manipulated to extract the item where the \u201cdisplay_name\u201d field matches the name of the site we want. The Site ID can then be extracted from that item.</p> <p>The cache clearing API endpoint expects a similar body to the last item. This time it also needs the site ID.</p> <p>There\u2019s not much returned by this API call. As long as you get a status code of 200, it can be reasonably assumed that the request was successful.</p>","tags":["powershell","imperva"]},{"location":"2022/06/13/clearing-imperva-cloud-waf-cache-using-powershell/#building-upon-the-basics","title":"Building Upon The Basics","text":"<p>At this point, you can consider some polish and enhancements to the basic workflow. In the implementation I made, I wrapped an Azure Devops Pipeline around the Powershell scripts I wrote. The current list of sites are defined as a list of values in the pipeline, prompting the user to select one. This allows easy execution of the workflow, as well as logging of what happened for each run.</p> <p>Another possible improvement could be sending some sort of notification, such as an email or a message to Teams or Slack. The notification would give wider visibility that the cache has been cleared. Lastly, it may be an option to include this workflow at the end of release pipelines. This would ensure that consumers of your site aren\u2019t loading stale website assets.</p>","tags":["powershell","imperva"]},{"location":"2022/06/27/could-not-create-ssltls-secure-channel-error-when-using-self-hosted-azure-devops-agent/","title":"\u201cCould not create SSL/TLS secure channel\u201d error when using self-hosted Azure DevOps Agent","text":"<p>Recently the team I\u2019m in has been getting into Microsoft\u2019s new Bicep language. As part of a release pipeline, the infrastructure was being deployed \u2013 in this case an Azure App Service. Then the application code was being deployed using the standard \u201cAzure App Service Deploy\u201d task. At this particular task, it would error out:</p> <p><code>Error: Could not complete the request to remote agent URL 'https://&lt;App Service Name&gt;.scm.azurewebsites.net/msdeploy.axd?site=&lt;App Service Name&gt;'. Error: The request was aborted: Could not create SSL/TLS secure channel.</code></p> <p>The pipeline was being run through our \u201cDefault\u201d agent pool, which was a self-hosted agent.</p>","tags":["azure devops"]},{"location":"2022/06/27/could-not-create-ssltls-secure-channel-error-when-using-self-hosted-azure-devops-agent/#troubleshooting","title":"Troubleshooting","text":"<p>The first thing I tried was to recreate the scenario using the Microsoft-hosted agent. When running the process using their agent, it worked. The error message strongly hinted that the issue was related to TLS settings somewhere.</p> <p>When looking at the Bicep code, I noticed when doing a \u201cwhat-if\u201d, it would set the TLS minimum version for the SCM interface from 1.0 to 1.2, even though this setting wasn\u2019t actually defined in the template. When looking at the ARM templates generated from existing App Services, they all listed this setting at TLS 1.0 as well.</p> <p>The next thing I did was go back to the Bicep template and explicitly set the minimum TLS version fot the SCM interface. First I tried 1.0 and the deployment succeeded. Set to 1.2 and it failed. So this established there was a TLS problem somewhere.</p> <p>The agent VM was running Windows 2019 and thus, should support TLS 1.2 by default. The only registry settings controlling encrpyption settings were to disable SSL 3.0. Even after explicitly enabling the TLS 1.2 to enable in the registry, the deployment still failed.</p> <p>Those who have dealt with TLS settings in Windows before will know there\u2019s a setting for which TLS versions at supported at the OS level, and settings for instructing .NET to use strong encryption. By default, this strong encryption setting isn\u2019t enabled, which causes .NET applications to use lower levels of encryption.</p> <p>After enabling the strong encryption setting on the self-hosted agent, the deployment started working.</p>","tags":["azure devops"]},{"location":"2022/06/27/could-not-create-ssltls-secure-channel-error-when-using-self-hosted-azure-devops-agent/#lessons-learned","title":"Lessons Learned","text":"<p>A couple of items come to mind out of all this:</p> <ul> <li>Agent configuration \u2013 It\u2019s important to keep your self-hosted agents\u2019 configuration aligned with that of the Microsoft hosted agents</li> <li>Bicep behaviour \u2013 It\u2019s worth running a \u201cwhat-if\u201d with a Bicep template after it\u2019s initial deployment to see if any values are magically appearing or changing themselves outside of what you\u2019ve defined in the template.</li> </ul>","tags":["azure devops"]},{"location":"2022/08/16/registering-a-vm-with-multiple-azure-devops-environments/","title":"Registering a VM with Multiple Azure DevOps Environments","text":"<p>Azure DevOps has the concept of Environments, a collection of resources which can be used during a pipeline. At the time of writing the only types of resources that can be used are Virtual Machines and Kubernetes resources. The official documentation on registering a VM resource doesn\u2019t explicitly mention there being any issues with using the same resource across multiple environments, apart from \u201cproviding a unique name for the agent\u201d. There\u2019s an important consideration with how the registration process works relating to this.</p>","tags":["azure devops"]},{"location":"2022/08/16/registering-a-vm-with-multiple-azure-devops-environments/#the-registration-process","title":"The Registration Process","text":"<p>Registering a VM resource for an environment involves navigating to that environment and running through the short \u201cAdd Resource\u201d wizard. The wizard will generate a Powershell script to run on the Virtual Machine.</p> <p></p> <p>In short, the script downloads the agent software the VM will run and installs it. The key part that affects multiple registrations is the generated name of the service that is installed. The name is computed using your DevOps Organisation Name, the name of the Environment and the name of the VM.</p> <p></p>","tags":["azure devops"]},{"location":"2022/08/16/registering-a-vm-with-multiple-azure-devops-environments/#how-it-can-break","title":"How It Can Break","text":"<p>The problem with this name format is it uses values that aren\u2019t totally unique. For example, you can have multiple environments in different projects that have the same name. Or Virtual Machines with the same name in different resource groups. So what happens if you try to register the VM again, with another project that has the same Environment name? In the experience I had, not only did the registration process fail to install a 2nd service, it errored out and damaged the existing service so it no longer worked with the first environment.</p> <p>This resulted in me having to delete the resource from the first environment and start the process from scratch.</p>","tags":["azure devops"]},{"location":"2022/08/16/registering-a-vm-with-multiple-azure-devops-environments/#how-to-avoid-issues","title":"How to Avoid Issues","text":"<p>I think the key point is the line I mentioned from the documentation, about ensuring the Agent has a unique name. While it may be tempting to mess with the registration script (especially to change the --agent $env:COMPUTERNAME part) I think the best option is to ensure that your Environment names are unique across your DevOps Organisation. This could be something as simple as including the Project Name in it, as Project Names must be unique within an Organisation.</p>","tags":["azure devops"]},{"location":"2022/08/22/security-agencies-issue-powershell-guidance/","title":"Security Agencies Issue PowerShell Guidance","text":"<p>PowerShell has historically presented some challenges with security. This can lead to the temptation of disabling it outright, removing an avenue of attack opportunities. Recently, a number of government security agencies issued recommendations relating to PowerShell.</p>","tags":["powershell","powershell security"]},{"location":"2022/08/22/security-agencies-issue-powershell-guidance/#a-two-part-approach","title":"A Two-Part Approach","text":"<p>The recommendations from the agencies can be split into two groups \u2013 methods to prevent PowerShell being used and abused by malicious actors, and methods for increasing visibility of abuse attempts.</p> <p>Some of the options in the first group include network controls around PowerShell Remoting (such as appropriate rules in Windows Firewall). Another option proposed is to make PowerShell run in Constrained Language Mode (CLM). CLM forces a number of restrictions on the use of PowerShell, including restricting access to only approved .NET types.</p> <p>The second group of options involve increasing the logging of PowerShell activities. These items actually surprised me. In the past when working on Windows hardening, the CIS Benchmarks have recommended against this sort of logging. When reviewing the latest versions of the Benchmarks, it seems they\u2019re aligned with the advice from the government agencies, with some logging to be enabled.</p> <p>Overall, these recommendations are a good start in securing PowerShell in an organisation.</p>","tags":["powershell","powershell security"]},{"location":"2022/09/20/developing-a-bicep-validation-pipeline/","title":"Developing a Bicep Validation Pipeline","text":"<p>Azure\u2019s Bicep is Microsoft\u2019s newer format for defining Azure resources as code. In terms of look and feel, it\u2019s very similar to Terraform.. If one considers Bicep files as code, then it would be a natural step to ensure that code meets a certain level of quality. In addition to that level of quality, because Bicep is deploying infrastructure, we would want to ensure that infrastructure is well designed and has a chance of successfully deploying.</p> <p>When Bicep started to be adopted by the team I was working in, I became involved in designing a process to meet those quality goals as well as reduce the number of deployment issues.</p>","tags":["azure devops","azure bicep","bicep","psrule"]},{"location":"2022/09/20/developing-a-bicep-validation-pipeline/#the-building-blocks","title":"The Building Blocks","text":"<p>The main building blocks of the process have a lot in common with general code validation processes. In the end, the main stages of the process were:</p> <ul> <li>Linting \u2013 Does the Bicep code meet style and syntax requirements and recommendations</li> <li>Infrastructure Design \u2013 Does the Bicep code deploy infrastructure that is architecturally well designed and secure</li> <li>Test Deployment \u2013 Will the infrastructure actually deploy based on the Bicep code supplied</li> </ul>","tags":["azure devops","azure bicep","bicep","psrule"]},{"location":"2022/09/20/developing-a-bicep-validation-pipeline/#linting","title":"Linting","text":"<p>The Azure Bicep extension for Visual Studio Code already has excellent support for validating the syntax of Bicep code. However, many syntax issues are flagged only as warnings and don\u2019t prevent code from being committed to a repository. This means the first step in the validation process is to do this linting check. As mentioned in the Bicep documentation, the linting can be performed using the CLI. If there are linting issues with the file, it will cause the pipeline to fail. An example of how this could look in a YAML pipeline is below:</p> <pre><code>\u2013 task: AzureCLI@2\n  displayName: 'Linter Test'\n  inputs:\n    azureSubscription: $(azureServiceConnection)\n    scriptType: pscore\n    scriptLocation: inlineScript\n    inlineScript: |\n      az bicep build -f main.bicep\n</code></pre> <p>The build command causes an ARM template file to be generated. Depending on your situation, it may be useful to publish this as an artefact.</p>","tags":["azure devops","azure bicep","bicep","psrule"]},{"location":"2022/09/20/developing-a-bicep-validation-pipeline/#infrastructure-design","title":"Infrastructure Design","text":"<p>The next check makes sure the Bicep code will deploy infrastructure that well designed and well architected. Microsoft has a lot of documentation on how things in Azure should be designed. Fortunately, the PSRule module codifies these and provides an easy mechanism to perform checks. An example task for running PSRule is below: <pre><code>\u2013 task: ps-rule-assert@1\n  displayName: 'PSRules Test'\n  inputs:\n    inputType: inputPath\n    inputPath: 'main.bicep'\n    modules: 'PSRule.Rules.Azure'\n    outputFormat: NUnit3\n    outputPath: reports/ps-rule-results.xml\n</code></pre> It\u2019s worth noting that to use PSRule in this fashion, you need to install the PSRule extension into your Azure DevOps organisation. The modules input allows us to specify what rules will be used. This means you can include your own rules in your own module, but for the sake of this example, I\u2019m using the Azure rules. PSRule can output its results in a number of different formats. NUnit is a standardised format, so if we output in that format we can do things with the results. One follow-up task might be to publish the test results using the PublishTestResults task. By doing this, the data from the tests will be embedded in the UI of the pipeline run, allowing visibility of the results.</p> <p>To get PSRule working properly, a bit of extra work is required. By default, PSRule will only process .JSON files, ARM templates. To tell it to properly process Bicep files, we need to create a configuration file called ps-rule.yaml in the repository with the appropriate configuration. <pre><code>configuration:\n  # Enable automatic expansion of bicep source files.\n  AZURE_BICEP_FILE_EXPANSION: true\n  # Enable automatic expansion of param files\n  AZURE_PARAMETER_FILE_EXPANSION: true\n</code></pre> It\u2019s also possible to specify other settings in file, such as excluding certain tests.</p> <p>When PSRule runs, it will perform a number of checks against each resource defined in the Bicep file. If a check fails, it will cause the pipeline to fail. The output from the failed check will include a description and a link on how to resolve the failed check. The example below is of a PSRule task run against a Bicep file that defines a Storage Account with the minimum required settings. </p> <p>In this example, the bare minimum Storage Account resource in Bicep fails half the tests that PSRule runs.</p>","tags":["azure devops","azure bicep","bicep","psrule"]},{"location":"2022/09/20/developing-a-bicep-validation-pipeline/#test-deployment","title":"Test Deployment","text":"<p>The final step performs a test deployment of the Bicep file. In the pipeline I built, this has several tasks:</p> <ul> <li>Create a Resource Group with a unique name (to prevent name collision issues)</li> <li>Run a What-if deployment as a sanity check</li> <li>Perform the actual deployment using a parameters file</li> <li>Remove the resources and the Resource Group at the end</li> </ul> <p>The reason for doing this step is because I\u2019ve had some situations where a Bicep template would pass the earlier checks, but would then error out when deployed. Doing a test deploy can hopefully catch these sort of issues before the real deployments to production and other environments.</p>","tags":["azure devops","azure bicep","bicep","psrule"]},{"location":"2022/09/20/developing-a-bicep-validation-pipeline/#conclusion","title":"Conclusion","text":"<p>With this sort of pipeline, the final Bicep code achieves a number of goals:</p> <ul> <li>The Bicep code has correct syntax because it\u2019s passed a linting check</li> <li>The Bicep code will deploy infrastructure that is well designed because it\u2019s passed the PSRule checks</li> <li>The frequency of errors on deployment should be reduced because of the prior 2 points and because we\u2019re doing a test deployment</li> </ul> <p>The pipeline can be implemented in a few different ways depending on the needs of one\u2019s situation. In my case, it was implemented as a Build Validation pipeline on the main branch. This means it would be executed when a pull request was created and could act as part of the pull request process.</p>","tags":["azure devops","azure bicep","bicep","psrule"]},{"location":"2022/10/24/azure-defender-for-devops--first-impressions/","title":"Azure Defender for DevOps \u2013 First Impressions","text":"<p>The recent batch of high profile security incidents at various companies in Australia highlights the need for appropriate security measures across all components of an organisation\u2019s infrastructure. Defender for DevOps is a new functional addon (in preview) to Defender for Cloud. It provides security functionality for your code respositories and associated components.</p>","tags":["azure devops","security"]},{"location":"2022/10/24/azure-defender-for-devops--first-impressions/#setup","title":"Setup","text":"<p>When navigating to the Defender for Cloud interface, a new option will appear under the \u201cCloud Security\u201d heading.</p> <p></p> <p>Once we click on this, we are presented with an intro splash page with steps to getting started. The first step is to connect to the environments. Both Azure DevOps and Github repositories are supported environments.</p> <p></p> <p>After clicking on the Add Connector button, we are presented with the Environment settings page. I found this screeen a bit confusing as it wasn\u2019t immediately obvious how to see the new environment. The documentation (at the time of writing this post) doens\u2019t cover this stage of setup. The trick is to click on the Add Environment button and select the appropropriate option. In my case, I\u2019ll use Azure DevOps.</p> <p></p> <p>Next we are presented with a standard style of wizard for setting up a new Azure resource, with the first page asking for a name, subscription, resource group and region. As the form indicates, the only available region is Central US.</p> <p></p> <p>The next step of the wizard lists what plan to use. At the moment this section is fairly basic and the plan is free. There will probably be more options when this offering goes live.</p> <p></p> <p>The third step is to authorise Defender for DevOps on the target. An authorise button is presented.</p> <p></p> <p>When the button is clicked, a window will appear with details of what permissions will be needed. The majority of the permissions are only read, but a few are write access as well. If all this is ok, click the Accept button at the bottom. The Authorisation Connection screen will now have some additional details, such as the organisation and what projects should be used. A summary of the permissions are also listed. I opted to use the auto-discovery option to cover all projects.</p> <p></p> <p>Lastly, like every Azure wizard, we get a Review and create screen with a create button. For me, the creation process took about 3 minutes. Once done, we are redirected back to the Environment settings screen. The Azure DevOps item is now listed.</p> <p></p> <p>If we go back to the Defender for Cloud interface, the DevOps Security blade should be updated with an overview of our environment. At this stage, it won\u2019t really show much of value because further configuration is needed.</p> <p></p>","tags":["azure devops","security"]},{"location":"2022/10/24/azure-defender-for-devops--first-impressions/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>The second item on the Get Started list was about configuring pipelines. For Azure DevOps, this means installing an extension. Navigate to Azure DevOps and click on the shopping bag icon in the top right, then Manage Extensions.</p> <p></p> <p>At this point, the Microsoft documentation indicated that the extension should be listed under the Shared tab. For me, this area was blank. So I clicked on the Browse Marketplace button and was able to find it.</p> <p></p> <p>At this point, we can click on it, review the information and install it. At this point, we can create a new pipeline or edit an existing one to use the new tasks provided by the extension. To start with, I\u2019ll use the example provided by Microsoft\u2019s documentation. <pre><code># Starter pipeline\n# Start with a minimal pipeline that you can customize to build and deploy your code.\n# Add steps that build, run tests, deploy, and more:\n# https://aka.ms/yaml\ntrigger: none\npool:\n  vmImage: 'windows-latest'\nsteps:\n- task: UseDotNet@2\n  displayName: 'Use dotnet'\n  inputs:\n    version: 3.1.x\n- task: UseDotNet@2\n  displayName: 'Use dotnet'\n  inputs:\n    version: 5.0.x\n- task: UseDotNet@2\n  displayName: 'Use dotnet'\n  inputs:\n    version: 6.0.x\n- task: MicrosoftSecurityDevOps@1\n  displayName: 'Microsoft Security DevOps'\n</code></pre> To test the pipeline, I ran it on a repository that had some basic Bicep templates. Using this basic template, it appears to install all the toolset used by Defender for DevOps. While the task doing the install and scan took only 1 minute 20 seconds for me, it may take longer for larger repositories. One of these default tools is the ARM Template Best Practice Analyser. This tool actually ran over the Bicep files in the target repository. I\u2019m not sure if it was because my Bicep files were genuinely bad or were mangled in the Bicep-to-ARM conversion process, but it did result in a bunch of errors.</p> <p>Fortunately it\u2019s possible to add some constraints to the tools used in the pipeline by specifying a category. The Microsoft documentation only mentions IaC as a category. Even when enabling this, it seemed to install the same tools and generate the same amount of logging. The logging can be quite verbose. Fortunately, task will publish results in the SARIF format. Since this is a standardised format, you could run it through the SARIF Azure DevOps extension or pass it onto security tools that can read it.</p> <p>Another example of focused scanning is for secrets. This is done by setting the category value to \u201csecrets\u201d in the pipeline code. When I put a variable named \u201cpassword\u201d in one of my Bicep files, the secret scanning picked it up as a potential credential.</p> <p></p> <p>There is an extension that can display SARIF file output inline with the pipeline run. When this particular run is viewed in that interface, we get a nice summary of the same items.</p> <p></p> <p>The landing page in Defender for Cloud will also update with details of issues as they\u2019re found.</p>","tags":["azure devops","security"]},{"location":"2022/10/24/azure-defender-for-devops--first-impressions/#enabling-pull-request-annotations","title":"Enabling Pull Request Annotations","text":"<p>So far the visibility of issues may be isolated from developers. They are likely not to have access to the Defender for Cloud interface and they may not always check CI-based pipelines that you might setup to perform general checks on code commits. Defender for DevOps can create visibility in Pull Requests by creating annotations. For Azure DevOps this is done by configuring settings in Azure DevOps itself and Defender for Cloud. For the first area, this requires setting a Build Validation pipeline.</p> <p></p> <p>The second configuration is in Defender for DevOps. In the landing page, tick all the relevant repositories and click on the Configure button.</p> <p></p> <p>In the window that appears, set the Pull Request Annotations slider to On. At the moment, the Category and Severity levels are fixed and can\u2019t be changed. Click the Save button.</p> <p></p> <p>After configuring all this, we will see some different behaviour when performing a pull request. Firstly, as expected, the Build Validation pipeline will run. If there is an item picked up, it will be added as a comment in the Pull Request, like shown below:</p> <p></p> <p>The status of the comment can be changed to values like \u201cPending\u201d, \u201cWon\u2019t fix\u201d, \u201cClosed\u201d. One issue I did experience with the default settings is that a user can still complete the Pull Request even if an item is found. There\u2019s two possible ways to resolve this. Firstly, the Build Validation pipeline didn\u2019t register a non-successful exit code when it ran so Azure DevOps percieved the validation process as passing. If it had failed, and the validation was set to required, then it would\u2019ve blocked the ability to complete the Pull Request.</p> <p>The other option is by default, the \u201cCheck for comment resolution\u201d setting on repositories is disabled. When set to enable, it will become another check in the process and block completion of the Pull Request.</p> <p></p>","tags":["azure devops","security"]},{"location":"2022/10/24/azure-defender-for-devops--first-impressions/#final-thoughts","title":"Final Thoughts","text":"<p>Apart from the initial stumbling block with how to create the connection, the documentation and UI were generally clear and easy to use. The supported file types for the secret scanning is comprehensive and should cover most environments. There appears to be support for scanning container images, but the open source tools used don\u2019t appear to do anything for PowerShell.</p> <p>The ability to block merge requests is a nice feature to have, as well as the integration back into Defender for Cloud. Some of the options are limited at the moment, so I\u2019ll have to revisit this product when it hits GA status.</p>","tags":["azure devops","security"]},{"location":"2023/12/04/rendering-issues-with-nodejsnextjs-and-azure-front-door/","title":"Rendering Issues with Nodejs/NextJS and Azure Front Door","text":"<p>Recently at my workplace, a new application using Node JS and NextJS was implemented. As with all our public facing websites, it was placed behind Azure\u2019s Front Door service, to provide web application firewall (WAF) and caching functionality.</p> <p>During testing, it was discovered that the site would sometimes not render properly. However it wasn\u2019t a 100% failure rate.</p>","tags":["azure front door"]},{"location":"2023/12/04/rendering-issues-with-nodejsnextjs-and-azure-front-door/#an-early-theory-geography","title":"An Early Theory \u2013 Geography","text":"<p>Early on a common theme was noticed. If the user device was located in Brisbane, regardless of OS, browser or ISP used, the site would fail to render. If the device was in or close to Sydney, it would render properly. Trying a few other geographical points using VPNs showed similar behaviours.</p>","tags":["azure front door"]},{"location":"2023/12/04/rendering-issues-with-nodejsnextjs-and-azure-front-door/#the-mysteries-of-front-door","title":"The Mysteries of Front Door","text":"<p>It turns out that Front Door uses a mix of Windows and Linux based systems for its infrastructure. There are subtle differences in behaviour between these systems. Forunately Microsoft have made it somewhat easy to figure out what you\u2019re getting by looking at the X-Azure-Ref header. One will start with a datetimestamp style string and the other will appear to be completely random.</p> Header Type 1 Header Type 2 <p>In the case of this issue, the header for systems in Sydney had one type and the systems in Brisbane (and other locations where the site wouldn\u2019t render) had the other type. However there was clearly something in the new app that caused this behaviour as our other applications, written in another language, didn\u2019t have these issues.</p>","tags":["azure front door"]},{"location":"2023/12/04/rendering-issues-with-nodejsnextjs-and-azure-front-door/#the-applications-contribution","title":"The Application\u2019s Contribution","text":"<p>It turns out the application has a particular behaviour that was causing one type of Front Door node to freak out and be unable to serve content properly. When a request is made, the application would return a content range header, indicating the size of the returned data. However this would often not match the actual size of the data returned. In some situations, this is normal behaviour and would cause a HTTP 206 (Partial Content) response code, with the remaining content requested in further responses.</p> <p>In this particular situation, one type of the Front Door node couldn\u2019t cope with this and wouldn\u2019t return the data to the client at all. This would cause the site to not render properly.</p>","tags":["azure front door"]},{"location":"2023/12/04/rendering-issues-with-nodejsnextjs-and-azure-front-door/#the-fix","title":"The Fix","text":"<p>There were two options presented to fix this. Firstly, update the application code to cause the content range header to match the actual content size. This was deemed very difficult, if not impossible to achieve. The second option was to add a rule on Front Door to strip the Accept-Encoding headers from requests. Since being implemented, this rule has prevented the issue from happening again.</p>","tags":["azure front door"]},{"location":"2025/02/03/dora-2024-state-of-devops-report/","title":"DORA 2024 State of DevOps Report","text":"<p>The DevOps Research and Assessment (DORA) produces a yearly research report on the progress of DevOps practices in the industry.  When reading it, a few points stood out.</p>","tags":["devops","it industry"]},{"location":"2025/02/03/dora-2024-state-of-devops-report/#performance-bands","title":"Performance Bands","text":"<p>The majority (60%) of responding organisations sit in the Medium and Low performance level bands.  Compared to 2023, there was only slight movement towards the higher performing bands.  The report did attempt to explain some of this away, stating that longer Change Lead Times and slower Deployment Frequencies could be caused by teams putting more steps in their test/build/deployment processes.  This does seem to line up with the outlier statistic of the Medium performance band having a lower Change Failure rate tha the High band (10% vs 20%).  </p> <p>However, the results do seem to indicate that a significant number of organisations, those stuck at the Low band, are not realising much of the benefit of DevOps.  These organisations are stuck with high failure rates (40%) and very slow paths to getting their code to production.  Given the relative maturity of DevOps methods and tools, this is a concern.</p>","tags":["devops","it industry"]},{"location":"2025/02/03/dora-2024-state-of-devops-report/#ai","title":"AI","text":"<p>The 2024 report also spent a lot of time on Artificial Intelligence, its uses, impacts and so on.  A majority of organisations have shifted to increase the use of AI.</p> <p>87% of respondents had \"some level of trust\" with regards to the code that AI tools can generate.  This is a bit of a contrast to the experience of some of my peers (and my own) where the quality of code can be mixed, resulting in us having much lower trust and regard for the tools.</p> <p>The report observed an interesting effect on AI adoption and its impact on valuable work vs toilsome work.  Many people had the assumption that AI would be mostly utilised on the \"drudge\" work that adds little value, freeing up people to focus on the work that adds value.  The data indicates the opposite is happening.</p>","tags":["devops","it industry"]},{"location":"2025/02/03/dora-2024-state-of-devops-report/#platform-engineering","title":"Platform Engineering","text":"<p>While Platform Engineering can add many benefits, the report found that in some cases it can actually lead to a degree in throughput.  The theory the report put forward is the extra machinery present when using an Internal Developer Platform could be a cause.  Extra tasks and steps in the process, such as testing and security checks, while likely improving quality, impact the throughput.</p>","tags":["devops","it industry"]},{"location":"2025/02/10/managing-azure-api-management-with-apiops/","title":"Managing Azure API Management with APIOPS","text":"<p>Azure's API Management (APIM) resource is one of the most complex in the Azure ecosystem.  It has several configuration pieces such as API endpoints, policy objects and secrets, with relationships between these objects.  This complexity carries into Infrastructure as Code (IaC).  When exporting an APIM instance with a few configuration items, the resulting ARM template can easily be over 15,000 lines line.  Even using Bicep, where we can experience a 4:1 reduction in line count, we would be dealing with a very large file.</p> <p>This leads to a conclusion that managing an APIM instance solely using Bicep or Terraform would be difficult.  Fortunately, Microsoft have provided a toolset called \"API Ops\" which allows easy importing and exporting of APIM configuration.</p>","tags":["api management"]},{"location":"2025/02/10/managing-azure-api-management-with-apiops/#api-ops-overview","title":"API Ops Overview","text":"<p>As the API Ops website states: \"APIOPS places the Azure API Management infrastructure under version control...Rather than making changes directly in the portal, most operations happen through code changes that can be reviewed and audited\".  For organisations that use IaC or have some level of material, these points will meet similar objectives they're trying to achieve (namely, that the APIM configuration is version controlled and can be audited).</p> <p>The toolset uses two programs, an Extractor and a Publisher.  The Extractor exports the configuration to a series of text files, while the Publisher applies a defined configuration to an instance.  Both tools can be run in pipelines.  When used in pipelines, there is the ability to use standard pull request code review practices.</p> <p>The Publisher tool can also use over-ride files.  This is useful in the case of promoting changes to different environments where certain values, such as resource names or secrets, may change.</p>","tags":["api management"]},{"location":"2025/02/10/managing-azure-api-management-with-apiops/#example-implementation","title":"Example Implementation","text":"<p>In my initial investigation of API Ops, I did an example implementation with two APIM instances: a dev instance where developers would do initial development work on APIs and a production instance.</p> <p>The Extractor pipeline has two Stages - the first performs the export task, where it performs an export, a linting check and publishes an artifact for consumption.  The second Stage picks up the artifact and creates a branch from it.  An example of the folder structure the export creates is show below:</p> <p></p> <p>The pipeline will automatically create a pull request which can be reviewed and approved.</p> <p>The Publisher pipeline is triggered by pull requests that are merged to the main branch.  By using Azure DevOps environments and associated approvals, it's possible to put in approval gates for higher environments such as Production, as shown below:</p> <p></p> <p>Since each Environment can have its own approval settings, it's possible to have a specific list for each.  For example, Production may have a more restrictive set of approvers compared to a Test or UAT environment.</p>","tags":["api management"]},{"location":"2025/02/10/managing-azure-api-management-with-apiops/#technical-issues-and-considerations","title":"Technical Issues and Considerations","text":"<p>During the example implementation I did, I ran into a few items that need consideration and thought in how to handle:</p> <ul> <li>Pipeline Permissions - Because the Extractor pipeline is doing tasks such as branch creation and pull requests, it needs additional permissions in Azure DevOps</li> <li>Multiple Subscriptions - Both pipelines assume the APIM instances are in the same subscription and the pipeline identity has access to them.  If this is not the case, then the subscription IDs need to be set an environment variable</li> <li>\"Subscription limit reached\" error - This error is due to the default subscriptions that are created when an APIM instance is made.  This can be resolved by deleting those default subscriptions</li> <li>Handling sensitive values - A Named Value can be configured as a type - \"Plain\", \"Secret\" or \"Key Vault\".  When configured as \"Plain\", it will extract the value as plain text.  This could be an issue if the value is considered a secret or sensitive because it will be committed to the repository.  If the value is configured as a \"Secret\", the Extractor will export an empty value, meaning nothing will be imported.  If it's defined as a Key Vault reference, then the reference will be exported, allowing it to be imported.  </li> <li>Managed Identity must be configured if using Key Vault references - APIM instances use Managed Identity to access Key Vault secrets.  During the Publish pipeline, if Key Vault references are present and the target APIM instance doesn't have Managed Identity enabled, the pipeline will error out</li> <li>Key Vault references must exist - When Key Vault references are made in APIM, such as when using override files, those secrets must exist for the Publisher pipeline to work</li> </ul>","tags":["api management"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"archive/2022/","title":"2022","text":""},{"location":"archive/2021/","title":"2021","text":""},{"location":"archive/2020/","title":"2020","text":""},{"location":"archive/2019/","title":"2019","text":""},{"location":"archive/2018/","title":"2018","text":""},{"location":"archive/2017/","title":"2017","text":""},{"location":"archive/2016/","title":"2016","text":""},{"location":"archive/2015/","title":"2015","text":""},{"location":"category/azure/","title":"Azure","text":""},{"location":"category/api-management/","title":"API Management","text":""},{"location":"category/devops/","title":"DevOps","text":""},{"location":"category/it-industry/","title":"IT Industry","text":""},{"location":"category/defender-for-cloud/","title":"Defender for Cloud","text":""},{"location":"category/security/","title":"Security","text":""},{"location":"category/azure-devops/","title":"Azure DevOps","text":""},{"location":"category/bicep/","title":"Bicep","text":""},{"location":"category/powershell/","title":"Powershell","text":""},{"location":"category/vmware/","title":"VMware","text":""},{"location":"category/vrealize-automation/","title":"vRealize Automation","text":""},{"location":"category/vcsa/","title":"VCSA","text":""},{"location":"category/vrealize-orchestrator/","title":"vRealize Orchestrator","text":""},{"location":"category/horizon/","title":"Horizon","text":""},{"location":"category/vrealize-orchestration/","title":"vRealize Orchestration","text":""},{"location":"category/automation/","title":"Automation","text":""},{"location":"category/packer/","title":"Packer","text":""},{"location":"category/home-lab/","title":"Home Lab","text":""},{"location":"category/nexpose/","title":"Nexpose","text":""},{"location":"category/rest-api/","title":"REST API","text":""},{"location":"category/system-center-orchestrator/","title":"System Center Orchestrator","text":""},{"location":"category/vcenter/","title":"vCenter","text":""},{"location":"category/nutanix/","title":"Nutanix","text":""},{"location":"category/elasticsearch/","title":"ElasticSearch","text":""},{"location":"category/games/","title":"Games","text":""},{"location":"category/blizzcon/","title":"Blizzcon","text":""},{"location":"category/lifecycle-manager/","title":"Lifecycle Manager","text":""},{"location":"category/it/","title":"IT","text":""},{"location":"category/nsx/","title":"NSX","text":""},{"location":"category/telstra/","title":"Telstra","text":""},{"location":"category/sql-server/","title":"SQL Server","text":""},{"location":"category/emc/","title":"EMC","text":""},{"location":"category/storage/","title":"Storage","text":""},{"location":"category/conference/","title":"Conference","text":""},{"location":"page/2/","title":"Home","text":""},{"location":"page/3/","title":"Home","text":""},{"location":"page/4/","title":"Home","text":""},{"location":"page/5/","title":"Home","text":""},{"location":"page/6/","title":"Home","text":""},{"location":"page/7/","title":"Home","text":""},{"location":"page/8/","title":"Home","text":""},{"location":"archive/2022/page/2/","title":"2022","text":""},{"location":"archive/2020/page/2/","title":"2020","text":""},{"location":"archive/2019/page/2/","title":"2019","text":""},{"location":"category/powershell/page/2/","title":"Powershell","text":""},{"location":"category/vrealize-automation/page/2/","title":"vRealize Automation","text":""},{"location":"category/vrealize-automation/page/3/","title":"vRealize Automation","text":""}]}